MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models
via Dynamic Expert Skipping
Yushi Huang1, Zining Wang2, Zhihang Yuan3*, Ruihao Gong2, Yifu Ding2,
Jinyang Guo2, Xianglong Liu2, Jun Zhang1*
1Hong Kong University of Science and Technology
2Beihang University
3Peking University
Abstract
Mixture-of-Experts (MoE) Multimodal large language mod-
els (MLLMs) excel at vision‚Äìlanguage tasks, but they suf-
fer from high computational inefficiency. To reduce infer-
ence overhead, expert skipping methods have been pro-
posed to deactivate redundant experts based on the cur-
rent input tokens.
However, we find that applying these
methods‚Äîoriginally designed for unimodal large language
models (LLMs)‚Äîto MLLMs results in considerable perfor-
mance degradation. This is primarily because such meth-
ods fail to account for the heterogeneous contributions of
experts across MoE layers and modality-specific behaviors
of tokens within these layers.
Motivated by these find-
ings, we propose MoDES, the first training-free framework
that adaptively skips experts to enable efficient and accu-
rate MoE MLLM inference.
It incorporates a globally-
modulated local gating (GMLG) mechanism that integrates
global layer-wise importance into local routing probabil-
ities to accurately estimate per-token expert importance.
A dual-modality thresholding (DMT) method is then ap-
plied, which processes tokens from each modality sepa-
rately, to derive the skipping schedule. To set the optimal
thresholds, we introduce a frontier search algorithm that
exploits monotonicity properties, cutting convergence time
from several days to a few hours. Extensive experiments
for 3 model series across 13 benchmarks demonstrate that
MoDES far outperforms previous approaches. For instance,
when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-
Instruct, the performance boost is up to 10.67% (97.33%
vs. 86.66%). Furthermore, MoDES significantly enhances
inference speed, improving the prefilling time by 2.16√ó and
the decoding time by 1.26√ó.
1. Introduction
Multimodal large language models (MLLMs) [45, 49] have
become a dominant paradigm for vision-language under-
*Correspondence to: Zhihang Yuan (hahnyuan@gmail.com), Jun
Zhang (eejzhang@ust.hk).
+7.93
+3.01
+1.74
+10.67
+3.90
+2.13
Figure 1. Average performance (%) vs. expert skipping ratios (%)
across different models [25, 48, 55] and methods [6, 21, 41] on
13 benchmarks (as detailed in Sec. 6.1). The left subfigure is for
Kimi-VL-A3B-Instruct [48] and the right subfigure is for Qwen3-
VL-MoE-30B-A3B-Instruct [25].
standing tasks, showing remarkable performance in inte-
grating text, images, and videos.
However, as the scale
of models keeps increasing to handle richer data and more
complex tasks, they face significant computational bottle-
necks during inference.
For instance, Qwen2-VL [54]
with 72B parameters only achieves <10 tokens/s when
processing a 4K-token input on 2√óA100 GPUs. This is
because each token requires computations with all model
parameters. The mixture-of-experts (MoE) [46] architec-
ture has emerged as an effective solution to reduce the
cost of large-scale MLLMs.
By sparsely activating par-
tial parameters (i.e., selected expert networks) for each to-
ken, MoE MLLMs [25, 48] decouple the factor of model
size from computational costs. This design offers substan-
tial computational savings without compromising perfor-
mance [26, 35].
Nevertheless, MoE models typically struggle with sub-
optimal expert utilization [27, 41] due to a fixed number of
activated experts for all tokens, which can incur significant
inference inefficiency [27, 41, 62]. Recent expert skipping
methods [6, 21, 41] thus propose to skip redundant experts
w.r.t. current tokens to accelerate inference. However, ap-
plying these methods to MoE MLLMs leads to a significant
drop in accuracy. For example, as shown in Fig. 1, skipping
83% of the experts in previous methods [6, 21, 41] during
inference results in accuracy drops of over 10%.
To solve the problem, we first make in-depth analyses
and obtain two key insights overlooked before: (i) The con-
1
arXiv:2511.15690v1  [cs.CV]  19 Nov 2025


tributions of experts to the model outputs vary significantly
across layers. Specifically, experts in shallow layers play
far more critical roles than those in deeper layers. However,
prior works [6, 21, 41] only consider intra-layer informa-
tion (e.g., Eq. (1)) to develop skipping schedules. (ii) To-
kens of different modalities (i.e., text and vision) exhibit
distinct behaviors as they pass through experts, and ex-
perts have a larger effect on updating text tokens. Yet prior
works mainly study unimodal LLMs [28] and do not ac-
count for this modality gap in MLLMs. These observations
underscore the need for a modality-specific expert skipping
method that explicitly models layer-specific contributions.
To this end, we introduce MoDES (Multimodal Dynamic
Expert Skipping), the first accurate and efficient expert skip-
ping framework tailored for MoE MLLMs.
In response
to the first insight, we propose a globally-modulated local
gating (GMLG) mechanism, which combines global layer-
specific importance with local routing probabilities to con-
struct expert importance scores. The global importance is
obtained via offline calibration with no inference-time over-
head.
Then, we introduce a dual-modality thresholding
(DMT) method which skips redundant experts whose im-
portance scores for the current token fall below the thresh-
old corresponding to the token‚Äôs modality. This modality-
specific treatment considerably enhances the performance
of expert skipping for MLLMs. To determine the optimal
thresholds, we further propose a frontier search algorithm
on a given search space.
This search method leverages
monotonicity properties of the performance loss and effi-
ciency w.r.t. thresholds, reducing the search time from more
than 2 days to less than 2 hours for models with tens of bil-
lions of parameters without compromising performance.
To demonstrate the effectiveness of our method, we con-
duct extensive experiments on 3 MLLM families across 13
image and video understanding benchmarks.
As shown
in Fig. 1, the results indicate that MoDES consistently
surpasses state-of-the-art (SOTA) methods. Notably, with
extremely high expert skipping ratios (>80%), MoDES
achieves 7.93-10.67% performance enhancements com-
pared with baselines while retaining >95% accuracy of
original models. Moreover, our MoDES yields a signifi-
cantly 2.03√ó speedup in prefilling and a 1.24√ó speedup in
decoding for Qwen3-VL-MoE-30B-A3B-Instruct [25].
2. Related Work
Multimodal large language models. Multimodal Large
language models (MLLMs) [4, 30, 37], which build upon
the success of large language models (LLMs) [1, 3, 10, 33],
have become a dominant paradigm for vision-language
tasks [2, 9, 23, 30, 34, 54].
However, as MLLMs [20,
31, 38] advance to handle higher resolutions and more
video frames, the escalating number of visual tokens cre-
ates a severe computational bottleneck. Current advanced
MLLMs [25, 35, 48, 55] adopt the mixture-of-experts
(MoE) [15] architecture to reduce computational costs by
processing each token with a subset of expert networks. De-
spite this, computation between tokens and multiple acti-
vated experts still incurs substantial overhead [13, 39].
Efficient MoE. Existing works on efficient MoE mod-
els can be categorized into training-aware and training-
free approaches.
Training-aware methods enhance rout-
ing balance and expert utilization during training [6, 18,
62], but they necessitate costly retraining and extensive
data access.
In contrast, training-free techniques enable
lightweight efficiency enhancement without modifying the
training pipeline, including quantization for parameter com-
pression [14, 28] and pruning for structural sparsity [29,
59]. Owing to the modular and sparse nature of MoE, a new
line of research‚Äîexpert skipping‚Äîhas emerged, which dy-
namically bypasses redundant experts [6, 21, 41] to speed
up inference. Among these studies, Lu et. al [41] utilize
dynamic expert skipping based on expert routing probabil-
ities. MC-MoE [21] further integrates an attention-aware
expert protection approach during skipping and combines
mixed-precision quantization for expert compression. Ad-
ditionally, DiEP [6] introduces a differentiable expert prun-
ing framework with adaptive expert skipping, which jointly
considers routing probabilities and expert similarity. How-
ever, these skipping methods are primarily developed for
text-only LLMs [26], which limits their scalability to com-
plex multimodal architectures.
In contrast, our training-
free expert skipping framework focuses on advanced MoE
MLLMs, achieving efficient inference without sacrificing
cross-modal understanding.
3. Preliminaries
Architecture of MLLM. A typical MLLM [5, 8, 53] com-
prises three core components: A visual encoder, a projector,
and an LLM backbone. The visual encoder first extracts
visual tokens from an image or video. The projector then
aligns these tokens with the LLM‚Äôs text embedding space.
Finally, the LLM backbone, a stack of transformer lay-
ers [51] composed of self-attention and feed-forward net-
works (FFNs), processes the combined visual and text to-
kens to generate responses.
Mixture-of-Experts (MoE). The advanced MLLMs [48,
64] employ Mixture-of-Experts (MoE) [21] layers as their
FFNs of the LLM backbones. This structure can be viewed
as a conditional computation module composed of multiple
parallel experts. Formally, let the l-th MoE layer contain M
experts, i.e., {Expert(l)
1 , . . . , Expert(l)
M }, each of which
is implemented as a multi-layer perception (MLP). Given
an input token representation x(l) ‚ààRd (d denotes hidden
dimension), a lightweight router predicts a set of routing
logits r(l) = {r(l)
1 , . . . , r(l)
M }. These logits are then normal-
2


ized into routing probabilities through a softmax operation:
œÄ(l)
m =
exp(r(l)
m )
PM
ÀÜm=1 exp(r(l)
ÀÜm )
,
(1)
where œÄ(l)
m reflects the contribution of Expert(l)
m . To en-
sure sparse activation, only a subset of experts is executed.
Let S(l) denote the indices of the top-k experts with the
largest routing probabilities. The output y(l) of the MoE
layer is obtained through a weighted aggregation:
y(l+1) =
X
m‚ààS(l)
œÄ(l)
m ¬∑ Expert(l)
m (x(l)).
(2)
This formulation allows the model to scale the number of
parameters independently of the active computation cost.
4. Motivation
Existing studies [6, 21, 41] have found that not every se-
lected expert provides essential contributions for tokens.
They thus propose to skip the computation of unimpor-
tant experts to improve inference efficiency.
However,
they focus on text-only LLMs [26].
In this study, we
have identified that directly adapting these methods to MoE
MLLMs [25, 48] overlooks two key factors: Global contri-
bution (Sec. 4.1) and modality gap (Sec. 4.2). Both factors
significantly affect the performance and efficiency of expert
skipping in MLLMs.
4.1. Global Contribution Disregard
Recent skipping strategies [6, 21, 41] rely on the local rout-
ing probabilities (Eq. (1)) to determine the skipping sched-
ule of the l-th layer, reflecting only input-dependent gat-
ing within a single layer. Such layer-agnostic rules ignore
the global contribution (i.e., impact on final outputs) im-
balance of experts across different layers. Empirically, as
shown in Fig. 2, we observe that when reducing the value
of k for expert routing, shallower layers incur much severe
performance drops than those of deeper layers. This may
result from that, relative to the error of deeper layers, er-
rors introduced in shallow layers are amplified by subse-
quent layers [22], leading to a significant error explosion.
Accordingly, the aforementioned layer-independent expert
skipping strategies [6, 21, 41] risk excessive skipping at
shallow layers, which are critical to final outputs, and vice
versa for deep layers.
Insight (i): The observation yields a core design princi-
ple: With higher global contributions, experts in shallow-
critical layers should be preserved; while experts in deeper,
less influential ones can be skipped more aggressively.
4.2. Modality Gap Matters
Focusing on expert skipping for MLLMs, we further exam-
ine the properties of modality-specific tokens with respect
(a) ChartQA [43]
0.0
2.5
5.0
Top-k Experts
0
20
40
60
80
Score
Layer 1-10
Layer 9-18
Layer 17-26
(b) MME [16]
0.0
2.5
5.0
Top-k Experts
0
500
1000
1500
2000
Score
Layer 1-10
Layer 9-18
Layer 17-26
(c) VideoMMMU [19]
0.0
2.5
5.0
Top-k Experts
20
40
Score
Layer 1-10
Layer 9-18
Layer 17-26
Figure 2.
Performance on image (i.e., (a)-(b)) and video (i.e.,
(c)) understanding tasks across various numbers of top-k routed
experts applied to different layer ranges for Kimi-VL-A3B-
Instruct [48].
The model has 64 routed experts for each FFN
within the 1-st to the 26-th layers, and sets k = 6 by default.
to the FFN layers. We first visualize the FFN input repre-
sentations via t-SNE in Fig. 3 (Left), which reveals a consis-
tent distributional gap between text and vision tokens across
layers.
To quantify the effect of this modality disparity,
we compute the cosine similarity between token represen-
tations before and after the FFNs. As shown in Fig. 3 (Mid-
dle), FFNs induce a smaller effect on vision tokens (i.e.,
higher similarity for tokens pre- vs. post-FFN), whereas text
tokens undergo substantially larger updates. By tracking the
angles between tokens and FFN weights in Fig. 3 (Right),
we attribute this phenomenon to their geometry: Vision to-
kens are more orthogonal to FFN weights (angles‚Üí90‚ó¶),
which alleviates the magnitude of their updates.
100
0
100
100
50
0
50
100
Text
Vision
0
10
20
Layer Index
0.7
0.8
0.9
1.0
Cosine Similarity
Text
Vision
0
10
20
Layer Index
89.92
89.94
89.96
89.98
90.00
Angle ( )
Vision
Text
Figure 3. (Left) t-SNE [50] visualization of pre-FFN text/vision
tokens across all layers. (Middle) Cosine similarity between pre-
FFN and post-FFN text/vision tokens across layers. (Right) Angle
between text/vision tokens and weights across different FFN lay-
ers. Here, GQA [24] dataset is used as the model inputs, and the
model is employed the same as that in Fig. 2.
Insight (ii): In a word, tokens from different modali-
ties differ, and the magnitudes of updates by FFNs for to-
kens also vary across modalities. Intuitively, when decid-
ing whether to skip the experts w.r.t. the current token, we
should account for these modality-specific differences. In
the following, a modality-aware skipping policy is proposed
for multimodal expert routing.
5. MoDES
Based on the above analyses, we propose MoDES (Multi-
modal Dynamic Expert Skipping), an efficient training-free
framework composed of two key components, as illustrated
in Fig. 4: (i) A globally-modulated local gating (GMLG)
(Sec. 5.1) mechanism that integrates a global and layer-level
calibration with local routing probabilities to compute re-
3


vision 
token
Self-Attention
text 
token
Layer 1
Layer 2
‚Ä¶
 ‚Ä¶
 ‚Ä¶
Layer ùêø
ùíü!"
 ‚Ä¶
E
E
E
E
E
Linear
Softmax
Router
ùúã#
(%)
calib set
(a) Globally-Modulated Local Gating
(b) Dual-Modality Thresholding
min
!t‚ààùìë, !v‚ààùìë    ùëìùúèt, ùúèv     s.t.   ùëîùúèt, ùúèv  ‚â•ùúå
Non-optimal
Frontier Area
Optimal
Frontier Search
ùúè&
ùúè'
Performance
Efficiency
Monotonicity properties
ùí™ùëÅùê∑'  ‚Üíùí™ùëÅùê∑
ùúèt, ùúèv : Optimal ùúè( , ùúè)
layer ùëô
layer ùëô+ 1
‚â•ùúèt
ùë†#
(%)
‚â•ùúèv
ùë†#
(%)
or
or
ùõº% ‚ãÖùúã#
(%)= ùë†#
(%)
prob*
prob*
%
Layer ùëô
Attn
Experts
Layer ùëô
Attn
Experts
Layer ùëô+ 1
‚Ä¶
Figure 4. Overview of MoDES. At inference, use a text token (e.g., ‚ñ†above) at the l-th FFN layer as an example. (a) We compute
importance scores s(l)
i
(i ‚àà{2, 4, M}) by combining the offline-calibrated globally-modulated factor Œ±(l) with the local routing probability
œÄ(l)
i . These scores evaluate the top-k (k = 3) routed experts for token ‚ñ†. (b) We then apply a modality-specific threshold‚ÄîœÑt for text
and œÑv for vision‚Äîfound by an efficient and effective frontier search. Experts with scores below the threshold are skipped. This method
significantly reduces computation while preserving performance for MoE MLLMs. ‚ÄúE‚Äù and ‚Äúcalib set‚Äù denote the expert and C (Eq. (4)).
fined importance scores for top-k experts; and (ii) a dual-
modality thresholding (DMT) (Sec. 5.2) method that deter-
mines modality-specific skipping boundaries based on these
importance scores. An efficiency‚Äìeffectiveness search strat-
egy is further introduced to optimize the threshold configu-
ration under a given computational budget.
5.1. Globally-Modulated Local Gating
In light of Insight (i) in Sec. 4.1, we present a globally-
modulated local gating (GMLG) mechanism, which com-
bines the global contributions of experts with local routing
behaviors to estimate expert importance for given tokens.
During inference, experts in S(l) (Eq. (2)) with importance
scores lower than the thresholds (defined in Sec. 5.2) will
be skipped. Specifically, for Expert(l)
i
(i ‚ààS(l)) with an
input token x(l), the importance score is defined as:
s(l)
i
= Œ±(l) ¬∑ œÄ(l)
i ,
(3)
where œÄ(l)
i
is the local routing probability (Eq. (1)) that
Expert(l)
i
will be activated for x(l).
The globally-
modulated factor Œ±(l) reflects the impact of experts in the
layer on the final prediction, which is obtained by offline
calibration. This s(l)
i
accounts for both global and local con-
tributions, yielding an accurate importance estimation.
To obtain Œ±(l), we calculate the Kullback-Leibler (KL)
divergence between the output distribution of the original
model and that of a counterpart where experts in the l-th
layer are skipped:
Œ±(l) = 1
N
N
X
j=1
DKL

probj || prob(l)
j

,
(4)
where N is the size of data (i.e., C = {c1, . . . , cN}) used for
this calibration. probj and prob(l)
j
are the output proba-
bilities for the j-th example of C from the original and mod-
ified models, respectively. This process quantifies the sensi-
tivity of the model‚Äôs output to the removal of experts in cer-
tain layers, and Œ±(l) serves as a global importance weight re-
flecting their relative contributions. With the pre-computed
Œ±(l), the final importance score s(l)
i
can be obtained without
additional overhead during inference.
5.2. Dual-Modality Thresholding
Building on Insight (ii) in Sec. 4.2, we introduce a dual-
modality thresholding (DMT) method to adaptively de-
termine modality-specific expert skipping thresholds for
MLLMs. We define two thresholds: œÑt for text tokens and œÑv
for visual tokens, which control the degree of expert skip-
ping for each modality. This design considers the distinct
behavior of tokens from different modalities, thereby allow-
ing a tailored and effective skipping strategy.
To be specific, based on the importance scores (Eq. (3))
for the l-th layer, experts that should be skipped for the
given token x(l) are:
{Expert(l)
i
| s(l)
i
< œÑt ¬∑ It + œÑv ¬∑ Iv},
(5)
where It and Iv are text and vision token indicator functions
for x(l), respectively.
To find the optimal œÑt and œÑv that balance computational
efficiency with model performance, we propose a frontier
search algorithm that effectively and efficiently determines
these thresholds under an efficiency constraint. We first for-
mulate the problem in the following.
4


Algorithm 1 Frontier search for optimal thresholds.
func FRONTIERSEARCH(B, œÅ)
Require:
B ‚Äî Candidate set of thresholds {œÑ (1), . . . , œÑ (D)}
œÅ ‚Äî Target skipping ratio
1: frontier ‚Üê‚àÖ
2: p ‚ÜêD
3: for q = 1 to D do
4:
while p ‚â•1 and g(œÑ (q), œÑ (p)) ‚â•œÅ do
5:
p ‚Üêp ‚àí1
6:
end while
7:
p(q) ‚Üêp + 1
8:
if p(q) ‚â§D then
9:
Compute and save f(œÑ (q), œÑ (p(q)))
10:
frontier ‚Üêfrontier ‚à™{(q, p(q))}
11:
end if
12: end for
13: (q‚àó, p‚àó) ‚Üêarg min(q,p(q))‚ààfrontier f(œÑ (q), œÑ (p(q)))
14: return (œÑ (q‚àó), œÑ (p‚àó))
Problem definition. For an MoE MLLM, the goal is to
find the thresholds œÑt and œÑv that minimize the difference
between the outputs of the original model and the expert-
skipping one, while satisfying a pre-defined target skipping
ratio œÅ ‚àà(0, 1). Hence, the problem can be expressed as:
min
œÑt‚ààB,œÑv‚ààB f(œÑt, œÑv)
s.t.
g(œÑt, œÑv) ‚â•œÅ,
(6)
where B = {œÑ (1), . . . , œÑ (D)} is the search grid set with D
candidates that satisfies œÑ (1) < œÑ (2) < . . . < œÑ (D). f(œÑt, œÑv)
is the average KL divergence between the output distribu-
tions of the original model and the modified version, where
experts are skipped according to Eq. (5). g(œÑt, œÑv) is the
fraction of experts that are skipped for the modified model.
Frontier search. We start with a monotonicity assumption:
Assumption 1. Holding other variables fixed, f is non-
decreasing in its respective arguments: If q1 ‚â§q2, then
f(œÑ (q1), œÑ (p)) ‚â§f(œÑ (q2), œÑ (p)); and if p1 ‚â§p2, then
f(œÑ (q), œÑ (p1)) ‚â§f(œÑ (q), œÑ (p2)).
Intuitively, higher thresholds will skip more experts and
degrade accuracy; hence, the assumption is reasonable. Ob-
viously, g is also non-decreasing in its respective arguments
without any assumption. Given these monotonicity proper-
ties, we can search for a frontier set {(q, p(q))} with a
time complexity of O(ND) 1 through Lines 1-12 in Alg. 1.
Here, p(q) for a given q is defined as:
p(q) = min
n
p ‚àà{1, . . . , D} | g(œÑ (p), œÑ (q)) ‚â•œÅ
o
.
(7)
1We compute f and g on data C (with N samples), which is also used
in Eq. (4).
We provide detailed proofs for the correctness of the search
algorithm and its time complexity in the Appendix.
Fi-
nally, as demonstrated in Alg. 1, the optimal thresholds
(œÑ (q‚àó), œÑ (p‚àó)), which lie in frontier (proofs can also be
found in the Appendix), are obtained through Lines 13‚Äì14.
Since all values of f(œÑ (q), œÑ (p(q))) are already computed by
Line 9, this step takes less than a second.
Overall, our frontier search algorithm achieves a time
complexity of O(ND). In comparison, a naive solution in-
volves an exhaustive search of all (œÑt, œÑv) pairs in B √ó B,
leading to a time complexity of O(ND2). In practice, our
method cuts the search time by a remarkable ‚àº45√ó (as de-
tailed in Sec. 6.3).
6. Experiments
6.1. Setups
Models and datasets.
We choose 3 series of MoE
MLLMs to evaluate MoDES: Kimi-VL [48], Qwen3-
VL-MoE
[25],
and
InternVL-3.5
[55].
We
use
8
zero-shot
evaluation
tasks
for
image
understand-
ing:
TextVQAval [47],
ChartQA [43],
MMStar [7],
MMBenchdev, en [40], MMVet [61], MME [16], Real-
WorldQA [58], and COCO2017-Capval [36] (COCO).
For video understanding tasks,
we adopt 5 bench-
marks: MVBench [32], EgoSchema [42], VideoMME [17]
(VMME), LongVideoBenchval,v [57] (LVB), and VideoM-
MMU [19] (VMMMU). lmms-eval [63] is utilized to
perform the above evaluation. For MMBench and MMVet,
we use DeepSeek-V3.1 [12] to rate the generated texts.
Baselines.
As there is no expert skipping baselines for
MLLMs and previous methods for LLMs only consider
models with top-2 routing in practice, we re-implement and
adjust them to top-k (k > 2) settings for MLLMs: For
the l-th layer, NAEE [41] originally skips the top-2 ex-
pert if œÄ(l)
top-2 < Œ≤(l) ¬∑ œÄ(l)
top-1, where œÄ(l)
top-1 and œÄ(l)
top-2 denotes
the top-1 and top-2 routing probabilities (Eq. (1)). Œ≤(l) is
a hyperparameter. Here, we adapt this strategy, referring
to the Appendix of NAEE, to a more general top-k sce-
nario.
Specifically, top-i to top-k experts are skipped if
Pk
u=i œÄ(l)
top-u < Œ≤(l) ¬∑ Pk
v=1 œÄ(l)
top-v. We also apply similar
adjustments for MC-MoE [21] and DiEP [6], which build
on top of NAEE. To be noted, without a specific claim, we
adopt only the expert skipping component of these works to
enable a fair comparison. Moreover, we also compare our
method with expert skipping guided by directly reducing the
value k of top-k routing.
Implementation.
We employ 1024 samples randomly
picked from the GQA [24] dataset to calibrate Œ±(l) (Eq. (4))
and search optimal (œÑt, œÑv) (Eq. (5)). The search space B
is given by D = 100 grid points sampled in (0, 1). More
implementation details can be found in the Appendix.
5


Table 1. Performance comparisons for Kimi-VL-A3B-Instruct [48] across various expert skipping ratios. We mark the target œÅ (Eq. (6))
and the practical skipping ratio x% (i.e., ‚ÄúSkip x% Experts‚Äù) in the table. For each method, we compute the score proportion relative to
the default setting (i.e., k = 6) across benchmarks, and then compute the average value in the ‚ÄúAvg. (%)‚Äù column. For the COCO dataset,
we report the CIDEr [52] score here. The best and second-best results are highlighted in bold and underlined formats, respectively.
Method
Image Understanding
Video Understanding
Avg.
(%)
TextVQA ChartQA
MMStar MMBench
MMVet
MME
RealWorldQA
COCO
MVBench
EgoSchema
VMME
LVB
VMMMU
k = 6 (Default)
88.70
89.48
49.89
83.16
66.33
2207
65.36
86.70
61.80
78.18
66.59
63.13
49.33
100.00
Skip 50% Experts (œÅ = 0.48)
k = 3
85.41
86.20
51.21
80.67
57.71
2065
63.53
87.56
60.42
75.71
64.30
60.14
44.22
95.93
NAEE [41]
86.14
85.74
50.82
80.58
60.81
2084
64.55
85.33
60.02
75.81
65.16
60.27
45.08
96.44
MC-MoE [21]
86.28
87.94
51.61
81.32
62.54
2138
63.82
86.24
60.39
76.57
66.24
60.62
46.26
97.69
DiEP [6]
87.43
88.32
51.48
80.26
60.41
2159
64.74
87.43
61.06
77.32
65.96
61.04
47.83
98.17
MoDES (Ours)
88.18
89.08
49.65
83.16
65.09
2203
65.62
88.23
61.95
78.41
67.19
62.83
49.00
99.91
Skip 67% Experts (œÅ = 0.65)
k = 2
83.49
85.12
52.10
78.87
53.49
2022
63.79
92.61
59.35
70.80
62.15
57.67
41.44
93.88
NAEE [41]
82.84
85.29
50.74
77.31
56.67
2083
64.54
82.09
59.68
72.29
63.74
58.36
43.68
94.03
MC-MoE [21]
85.07
86.32
51.13
77.65
58.42
2104
63.61
84.23
59.86
74.36
64.22
59.73
45.21
95.45
DiEP [6]
84.21
85.56
50.76
78.94
57.05
2087
64.02
87.54
60.02
72.97
61.07
58.45
44.93
94.81
MoDES (Ours)
85.57
88.24
49.25
82.73
60.78
2204
64.58
85.37
61.65
77.98
66.52
62.90
48.78
98.46
Skip 83% Experts (œÅ = 0.80)
k = 1
77.17
76.68
42.65
54.55
22.98
1647
54.38
77.37
51.10
37.23
50.52
43.83
24.56
71.60
NAEE [41]
75.73
78.41
41.48
69.14
43.41
1827
60.32
72.35
58.41
57.28
53.49
49.68
42.64
82.81
MC-MoE [21]
79.41
80.25
43.57
73.42
50.37
2063
62.54
80.42
54.87
63.56
59.87
54.39
44.02
88.32
DiEP [6]
82.32
78.31
42.47
76.28
47.45
2071
61.34
77.91
59.15
61.27
57.49
52.41
43.81
87.58
MoDES (Ours)
82.38
84.20
46.68
81.44
60.46
2162
64.84
81.33
61.30
76.98
65.48
62.60
47.11
96.25
Table 2. Performance of combination with quantization. MoDES
employs the quantization strategy in MC-MoE [21]:
weight-
only mixed-precision quantization for MoE-based FFNs and 4-bit
weight-only quantization for other layers.
Method
#Bit
ChartQA
MME
MMBench
LVB
VMMMU
Kimi-VL-A3B-Instruct [48]
k = 6 (Default)
16
89.48
2207
83.16
63.13
49.33
Skip 67% Experts (œÅ = 0.65)
MC-MoE [21]
2.5
78.47
2036
68.84
54.46
41.92
MoDES (Ours)
2.5
81.23
2137
76.48
58.10
43.67
MC-MoE [21]
1.5
69.46
1728
62.18
42.87
38.45
MoDES (Ours)
1.5
72.28
1899
68.57
48.14
40.06
Qwen3-VL-MoE-30B-A3B-Instruct [25]
k = 8 (Default)
16
85.08
2500
86.60
55.42
47.11
Skip 75% Experts (œÅ = 0.73)
MC-MoE [21]
2.5
76.36
2084
79.62
51.85
42.06
MoDES (Ours)
2.5
78.24
2281
81.34
53.63
46.28
MC-MoE [21]
1.5
70.42
1968
73.18
46.08
36.94
MoDES (Ours)
1.5
73.42
2113
75.54
47.32
42.01
6.2. Evaluation
Comparison with baselines.
We benchmark MoDES
against baselines on Kimi-VL-A3B-Instruct [48].
As
shown in Tab. 1, prior methods, such as NAEE [41],
MC-MoE [21], and DiEP [6], struggle to balance perfor-
mance and efficiency, especially at high expert-skipping ra-
tios (‚â•67%). Specifically, these baselines incur an aver-
age accuracy drop of more than 11% when skipping 83%
of experts during inference. We argue that these declines
arise because they rely solely on intra-layer routing log-
its (Eq. (1)) to determine the skipping schedule and are
originally designed for unimodal LLMs. By contrast, our
method, which considers both the impact of expert skip-
ping on the final output and the modality gap in MLLMs
(Sec. 4.2), executes only 13% of experts, while preserv-
ing 96.25% of the full model‚Äôs average accuracy. More-
over, even at a lower skipping ratio of 50%, our approach
still surpasses DiEP and MC-MoE by 1.74% and 2.22%,
respectively. These findings validate the superiority of our
method across different skipping ratios compared with ex-
isting SOTA approaches.
In addition, on some bench-
marks (e.g., RealWorldQA [58] and VideoMME [17]), us-
ing MoDES to skip redundant experts not only prevents
degradation but also improves accuracy, suggesting that cer-
tain experts are not merely redundant but may actively in-
terfere with inference.
Combination with quantization. We conduct experiments
to demonstrate the high compatibility of our MoDES with
model quantization. As shown in Tab. 2 (see the perfor-
mance without quantization for expert skipping in Tab. 1
and the Appendix), quantization causes a much smaller per-
formance drop for MoDES than for MC-MoE. For instance,
on Kimi-VL-A3B-Instruct with a ‚àº10.67√ó compression ra-
tio (i.e., 1.5 bits), quantization reduces MoDES‚Äôs perfor-
mance by 17.30%, compared with >20% for MC-MoE.
In addition, 2.5-bit quantization keeps MoDES more than
90% of the original model performance. Remarkably, for
Qwen3-VL-MoE-30B-A3B-Instruct, it retains 94.43% per-
formance, whereas 2.5-bit MC-MoE retains 89.58%. In fu-
ture work, we will explore combining MoDES with other
orthogonal techniques, such as pruning and distillation, to
further reduce the computational demands of MoE MLLMs.
Comparison across backbones.
In Tab. 3, we evaluate
our method across multiple backbones.
On the power-
ful Qwen3-VL-MoE-30B-A3B-Instruct model [25], our ap-
proach retains 97.33% of the original performance at an
6


Table 3. Performance comparisons across different backbones. InternVL series employs Qwen3 [60] and GPT-OSS [44] as LLM backbones
for 30B and 20B models, respectively. The number of experts for each layer of models from upper to lower is 128, 128, and 32.
Method
Image Understanding
Video Understanding
Avg.
(%)
TextVQA ChartQA
MMStar MMBench
MMVet
MME
RealWorldQA
COCO
MVBench
EgoSchema
VMME
LVB
VMMMU
Qwen3-VL-MoE-30B-A3B-Instruct [25]
k = 8 (Default)
83.41
85.08
59.67
86.60
69.68
2500
66.80
80.37
64.67
62.45
54.89
55.42
47.11
100.00
Skip 88% Experts (œÅ = 0.85)
k = 1
60.71
52.16
31.63
54.90
28.07
1590
52.42
45.64
41.51
32.52
39.78
42.41
12.51
60.11
NAEE [41]
72.41
65.83
48.88
73.62
54.52
1984
58.62
60.37
50.24
49.77
44.48
45.59
35.57
80.60
MC-MoE [21]
74.87
71.43
50.74
75.42
61.35
2168
60.41
68.15
56.60
51.84
52.51
47.22
37.41
86.66
DiEP [6]
73.46
70.51
53.28
73.21
58.64
2074
63.41
62.89
57.21
53.61
50.78
46.13
34.79
85.30
MoDES (Ours)
80.97
78.84
58.18
85.57
67.75
2403
64.58
74.66
62.98
62.04
55.26
55.50
46.56
97.33
InternVL-3.5-30B-A3B-HF [55]
k = 8 (Default)
85.76
84.08
62.49
83.81
69.93
2312
64.77
69.30
68.92
60.49
58.07
57.64
45.11
100.00
Skip 88% Experts (œÅ = 0.85)
k = 1
58.49
46.24
42.27
51.74
35.05
1683
51.44
26.01
31.99
34.47
35.26
37.40
24.27
59.63
NAEE [41]
66.24
68.32
50.14
64.37
49.52
1802
55.23
50.64
54.78
50.25
48.69
47.42
37.27
78.88
MC-MoE [21]
70.41
73.49
56.14
64.38
72.41
1972
57.49
60.12
58.97
52.31
49.72
48.31
40.06
86.20
DiEP [6]
69.37
71.84
57.21
63.19
65.32
1838
56.38
55.78
56.26
51.48
48.94
47.26
38.18
83.26
MoDES (Ours)
80.58
82.00
61.20
81.67
67.80
2222
61.73
65.16
68.65
60.79
57.63
54.49
44.33
97.03
InternVL-3.5-GPT-OSS-20B-A4B-Preview-HF [55]
k = 4 (Default)
80.20
90.64
57.64
79.78
69.68
2270
61.63
70.61
67.65
58.79
53.93
54.65
43.79
100.00
Skip 75% Experts (œÅ = 0.73)
k = 1
68.74
79.72
45.77
67.63
48.49
1833
53.20
60.70
56.95
49.40
44.04
44.28
41.66
77.58
NAEE [41]
73.89
82.34
44.89
71.59
54.97
2017
63.46
59.73
51.25
46.21
47.83
45.48
42.08
86.79
MC-MoE [21]
76.49
84.53
46.25
73.68
56.83
2137
61.07
60.42
60.06
50.28
48.37
46.68
42.89
89.91
DiEP [6]
77.31
86.24
48.18
74.26
58.07
2109
60.25
62.08
54.18
49.83
49.42
47.91
42.31
90.07
MoDES (Ours)
77.93
89.60
56.48
78.14
66.33
2206
60.64
68.32
66.60
57.95
53.59
53.68
43.13
97.89
aggressive skipping ratio of 88%. Moreover, across back-
bones, our method outperforms other skipping strategies by
more than 5% points in average accuracy. Taken together,
these results highlight the effectiveness and universality of
our technique in identifying redundant experts for tokens of
different modalities and across different layers. In addition,
we provide comparisons across different skipping ratios for
these models in the Appendix, where our method consis-
tently delivers higher accuracy at matched skipping ratios.
We further exhibit some qualitative visual reasoning exam-
ples in the Appendix to comprehensively demonstrate the
superiority of our method.
6.3. Efficiency Discussion
5.1
33.6
114.2
116.3
15h
2.4d
2.9d
7.5d
18min
1.3h
1.5h
4.0h
46.5√ó
45.7√ó
45.2√ó
45.2√ó
Figure 5. (Left) Œ±(l) calibration time. (Right) Search time of fron-
tier search (blue) vs. naive search (yellow). The bars/markers
from left to right are for Kimi-VL-A3B-Instruct [48], Qwen3-VL-
MoE-30B-A3B-Instruct [25], InternVL-3.5-30B-A3B-HF [55],
and InternVL-3.5-GPT-OSS-20B-A4B-Preview-HF [55].
Calibration and search efficiency.
As illustrated in
Fig. 5, we evaluate the calibration and search times
of MoDES for MoE MLLMs with ‚â•20B parameters
on 8√óH200 GPUs.
It is important to note that since
InternVL-3.5-GPT-OSS-20B-A4B-Preview-HF [55] in the
transformers [56] library supports only naive attention
computation, its time consumption is significantly higher
compared to the same-sized Kimi-VL-A3B-Instruct, which
uses flash-attention2 [11]. As observed from the
other models, MoDES processes 20-30B MoE MLLMs
(i.e., calibration + search) in 20 minutes to under 4 hours,
demonstrating high efficiency.
Furthermore, compared
to naive search with O(ND2) complexity, our frontier
search with O(ND) significantly reduces the search time
by ‚àº45√ó. In terms of performance, we benchmarked naive
search for Kimi-VL-A3B-Instruct [48] with an 83% expert
skipping ratio and found nearly identical average perfor-
mance with frontier search (96.24% vs. 96.25%). This re-
sult helps confirm the correctness of our Alg. 1.
2.04√ó
1.99√ó
2.16√ó
1.26√ó
1.19√ó
1.19√ó
1.92√ó
2.03√ó
2.07√ó
1.24√ó
1.23√ó
1.21√ó
Figure 6. Inference speed for (Upper) Kimi-VL-A3B-Instruct [48]
and (Lower) Qwen3-VL-MoE-30B-A3B-Instruct [25] on a single
H200 GPU. The expert skipping ratios for the former and the latter
are 83% and 88%, respectively. The batch size for prefilling is 8,
and the sequence length for decoding is 1024.
Inference efficiency. Next, we study the practical infer-
ence speedup.
As shown in Fig. 6, MoDES attains an
7


‚àº2√ó speedup in the prefill phase compared with the origi-
nal model. In the decoding phase, it still delivers a ‚àº1.2√ó
speedup. The smaller ratio during decoding likely arises be-
cause: (i) MoDES primarily reduces computation in MoE
layers, while decoding remains memory-bound; and (ii)
only text tokens are processed during decoding, which leads
to lower expert skipping ratios (Sec. 6.5). In addition, base-
lines like DiEP [6] use offline calibration to select hyperpa-
rameters, so their inference overhead is negligible. Under
the same skipping ratios, their speedup ratios are similar to
ours with <1% difference. Despite this, our method outper-
forms them across benchmarks by a clear margin (Sec. 6.2).
6.4. Ablation Studies
In this section, we employ Kimi-VL-A3B-Instruct [48], and
the settings are the same as those in Sec. 6.1 without specific
claims. More ablations can be found in the Appendix.
Effect of each component. We evaluate each component
of MoDES and use a single threshold œÑ w.r.t. s(l)
i
= œÄ(l)
i
(denoted as ‚ÄúThresholding‚Äù) with a grid search as our base-
line.
As shown in Tab. 4, GMLG, which incorporates
both global and local contributions, significantly enhances
both Thresholding and DMT. Moreover, by applying differ-
ent thresholds for different modalities, DMT outperforms
Thresholding by a large margin. These results underscore
the importance of the two key insights discussed in Sec. 4,
highlighting the substantial contributions of both GMLG
and DMT. Remarkably, performance improvements derived
from GMLG and DMT increase as the skipping ratio grows.
Table 4. Ablation results for each component of MoDES. ‚ÄúThresh-
olding‚Äù means we employ a single threshold œÑ for both modalities
and adopt a grid search for the optimal œÑ. For Thresholding and
DMT, we set s(l)
i
= œÄ(l)
i , instead of using Eq. (3).
Method
ChartQA
MME
MMBench
LVB
VMMMU
k = 6 (Default)
89.48
2207
83.16
63.13
49.33
Skip 67% Experts (œÅ = 0.65)
Thresholding
85.48
2030
77.67
57.97
45.56
Thresholding w/ GMLG
87.64
2172
79.46
60.24
46.48
DMT
87.47
2158
81.07
61.26
46.88
DMT w/ GMLG (Ours)
88.24
2204
82.73
62.90
48.78
Skip 83% Experts (œÅ = 0.80)
Thresholding
76.74
1956
65.48
54.67
40.33
Thresholding w/ GMLG
79.28
2107
75.19
60.02
43.87
DMT
82.94
2081
79.42
61.16
45.08
DMT w/ GMLG (Ours)
84.20
2162
81.44
62.60
47.11
Choice of data.
We also investigate the effect of dif-
ferent datasets with randomly sampled 1024 examples on
MoDES. In Fig. 7, the trends of Œ±(l) across datasets are sim-
ilar, with shallow layers having larger values than deep lay-
ers. This aligns with our insight in Sec. 4.1, where experts
in shallow layers contribute more to the final outputs. Addi-
tionally, the performance is also consistent across datasets,
as shown in Tab. 5. These results indicate that MoDES is
robust and not sensitive to the choice of dataset.
Table 5. Ablation results of using 3 dif-
ferent datasets for both calibration and
frontier search (C&S).
C&S
GQA
COCO
VMMMU
Skip 83% Experts (œÅ = 0.80)
GQA
62.68
62.65
62.63
COCO
81.33
81.72
80.72
VMMMU
47.11
47.67
47.67
ChartQA
84.20
86.56
83.46
MMBench
81.44
79.38
81.87
MME
2162
2138
2136
LVB
62.60
62.30
62.75
0
10
20
Layer Index
0.00
0.05
0.10
(l)
GQA
COCO
VMMMU
Figure 7.
Visualization
results of global contri-
butions Œ±(l)
(Eq. (4))
across layers and various
datasets.
6.5. Visualization Analysis
0
10
20 26
0
20
40
Ratio (%)
Text
Vision
0
10
20 26
0
50
100
Layer Index
0
10 20 30 4047
0
20
40
Ratio (%)
Text
Vision
0
10 20 30 4047
0
50
100
Layer Index
Figure 8. Visualization of expert skipping ratios (%) across modal-
ities and layers on 13 benchmarks (Sec. 6.1). The left subfigure
is for Kimi-VL-A3B-Instruct [48] and the right subfigure is for
Qwen3-VL-MoE-30B-A3B-Instruct [25].
The overall skipping
ratios for the former and the latter are 83% and 88%, respectively.
In this section, we visualize the expert skipping ratios of
MoDES across modalities and layers to interpret the effec-
tiveness of our approach. As shown in Fig. 8, our method
skips substantially more experts in shallow layers than in
deeper layers, which is consistent with the key insight dis-
cussed in Sec. 4.1. In addition, it skips far more experts
for vision tokens than for text tokens, indicating greater re-
dundancy among experts for vision tokens. We corroborate
this observation with experiments in the Appendix. These
results suggest that a uniform, modality-agnostic skipping
schedule is inappropriate. This finding also reinforces the
second insight in Sec. 4.2 and helps explain how our method
preserves the model‚Äôs strong performance.
7. Conclusions
In this work, we proposed MoDES, a novel framework for
expert skipping in MoE multimodal large language mod-
els (MLLMs). First, we identified two key insights: The
imbalance of expert contributions across layers and the dis-
tinct behaviors between modalities in FFNs. Based on these
findings, we introduced a globally-modulated local gat-
ing (GMLG) mechanism and a dual-modality thresholding
(DMT) method, which allow the model to adaptively skip
experts based on layer-specific importance and modality-
specific characteristics. Additionally, we developed an ef-
ficient frontier search algorithm, which greatly improves
search efficiency for threshold optimization. Extensive ex-
periments on large-scale multimodal benchmarks demon-
strate that MoDES provides significant computational sav-
ings without sacrificing performance.
8


References
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-
mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
Gpt-4 technical report.
arXiv preprint arXiv:2303.08774,
2023. 2
[2] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Es-
sam Sleiman, Deyao Zhu, Jian Ding, and Mohamed El-
hoseiny. Minigpt4-video: Advancing multimodal llms for
video understanding with interleaved visual-textual tokens.
arXiv preprint arXiv:2404.03413, 2024. 2
[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2
[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A versatile vision-language model for un-
derstanding, localization, text reading, and beyond. arXiv
preprint arXiv:2308.12966, 2023. 2
[5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun
Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhao-
hai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren
Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen
Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Jun-
yang Lin.
Qwen2.5-vl technical report.
arXiv preprint
arXiv:2502.13923, 2025. 2
[6] Sikai Bai, Haoxi Li, Jie Zhang, Zicong Hong, and Song
Guo.
Diep:
Adaptive mixture-of-experts compression
through differentiable expert pruning.
arXiv preprint
arXiv:2509.16105, 2025. 1, 2, 3, 5, 6, 7, 8, 13, 15
[7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang
Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,
Dahua Lin, and Feng Zhao. Are we on the right way for
evaluating large vision-language models?, 2024. 5
[8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhang-
wei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian,
Zhaoyang Liu, et al. Expanding performance boundaries of
open-source multimodal models with model, data, and test-
time scaling. arXiv preprint arXiv:2412.05271, 2024. 2
[9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,
Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,
Lewei Lu, et al. Internvl: Scaling up vision foundation mod-
els and aligning for generic visual-linguistic tasks. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 24185‚Äì24198, 2024. 2
[10] Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,
Yonghao Zhuang, Joseph E Gonzalez, et al.
Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt
quality.
See https://vicuna. lmsys. org (accessed 14 April
2023), 2(3):6, 2023. 2
[11] Tri Dao. Flashattention-2: Faster attention with better paral-
lelism and work partitioning, 2023. 7
[12] DeepSeek-AI. Deepseek-v3 technical report, 2024. 5
[13] Akash Dhasade, Anne-Marie Kermarrec, Erick Lavoie, Jo-
han Pouwelse, Rishi Sharma, and Martijn de Vos. Practical
federated learning without a server. In Proceedings of the
5th Workshop on Machine Learning and Systems, page 1‚Äì11.
ACM, 2025. 2
[14] Haojie Duanmu, Xiuhong Li, Zhihang Yuan, Size Zheng,
Jiangfei Duan, Xingcheng Zhang, and Dahua Lin. Mxmoe:
Mixed-precision quantization for moe with accuracy and
performance co-design. arXiv preprint arXiv:2505.05799,
2025. 2
[15] William Fedus, Barret Zoph, and Noam Shazeer.
Switch
transformers: Scaling to trillion parameter models with sim-
ple and efficient sparsity. Journal of Machine Learning Re-
search, 23(120):1‚Äì39, 2022. 2
[16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui
Yang, Xiawu Zheng, et al. Mme: a comprehensive evalu-
ation benchmark for multimodal large language models. corr
abs/2306.13394 (2023), 2023. 3, 5, 16
[17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai
Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang
Shen, Mengdan Zhang, et al.
Video-mme: The first-ever
comprehensive evaluation benchmark of multi-modal llms in
video analysis. In Proceedings of the Computer Vision and
Pattern Recognition Conference, pages 24108‚Äì24118, 2025.
5, 6
[18] Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng
Tu, and Tao Lin.
Dynamic mixture of experts: An auto-
tuning approach for efficient transformer models.
arXiv
preprint arXiv:2405.14297, 2024. 2
[19] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan
Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu:
Evaluating knowledge acquisition from multi-discipline pro-
fessional videos, 2025. 3, 5, 16
[20] Wenbo Hu, Zi-Yi Dou, Liunian Li, Amita Kamath, Nanyun
Peng, and Kai-Wei Chang. Matryoshka query transformer
for large vision-language models. Advances in Neural Infor-
mation Processing Systems, 37:50168‚Äì50188, 2024. 2
[21] Wei Huang, Yue Liao, Jianhui Liu, Ruifei He, Haoru Tan,
Shiming Zhang, Hongsheng Li, Si Liu, and Xiaojuan Qi.
Mixture compressor for mixture-of-experts llms gains more.
arXiv preprint arXiv:2410.06270, 2024. 1, 2, 3, 5, 6, 7, 13,
15
[22] Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Fei Chao,
and Rongrong Ji. Determining layer-wise sparsity for large
language models through a theoretical perspective, 2025. 3
[23] Zhengchao Huang, Bin Xia, Zicheng Lin, Zhun Mou, Wen-
ming Yang, and Jiaya Jia. Ffaa: Multimodal large language
model based explainable open-world face forgery analysis
assistant. arXiv preprint arXiv:2408.10072, 2024. 2
[24] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering.
Conference on Computer Vision and
Pattern Recognition (CVPR), 2019. 3, 5, 15
[25] Hugging Face. Qwen3-vl-moe, 2025. 1, 2, 3, 5, 6, 7, 8, 13,
14, 15
9


[26] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,
Arthur Mensch, Blanche Savary, Chris Bamford, Deven-
dra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
Florian Bressand, Gianna Lengyel, Guillaume Bour, Guil-
laume Lample, L¬¥elio Renard Lavaud, Lucile Saulnier,
Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao, Th¬¥eophile
Gervet, Thibaut Lavril, Thomas Wang, Timoth¬¥ee Lacroix,
and William El Sayed. Mixtral of experts, 2024. 1, 2, 3
[27] Peng
Jin,
Bo
Zhu,
Li
Yuan,
and
Shuicheng
Yan.
Moe++: Accelerating mixture-of-experts methods with zero-
computation experts, 2024. 1
[28] Young Jin Kim, Raffy Fahim, and Hany Hassan Awadalla.
Mixture of quantized experts (moqe): Complementary ef-
fect of low-bit quantization and robustness. arXiv preprint
arXiv:2310.02410, 2023. 2
[29] Jaeseong Lee, Aurick Qiao, Daniel F Campos, Zhewei Yao,
Yuxiong He, et al.
Stun:
Structured-then-unstructured
pruning for scalable moe pruning.
arXiv preprint
arXiv:2409.06211, 2024. 2
[30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,
Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Zi-
wei Liu, et al. Llava-onevision: Easy visual task transfer.
arXiv preprint arXiv:2408.03326, 2024. 2
[31] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,
Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Zi-
wei Liu, et al. Llava-onevision: Easy visual task transfer.
arXiv preprint arXiv:2408.03326, 2024. 2
[32] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,
Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin
Wang, and Yu Qiao.
Mvbench: A comprehensive multi-
modal video understanding benchmark, 2024. 5
[33] Yanwei Li, Chengyao Wang, and Jiaya Jia.
Llama-vid:
An image is worth 2 tokens in large language models. In
European Conference on Computer Vision, pages 323‚Äì340.
Springer, 2024. 2
[34] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng
Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya
Jia. Mini-gemini: Mining the potential of multi-modality
vision language models. arXiv preprint arXiv:2403.18814,
2024. 2
[35] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng
Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning,
et al.
Moe-llava:
Mixture of experts for large vision-
language models. arXiv preprint arXiv:2401.15947, 2024.
1, 2
[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir
Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva
Ramanan, C. Lawrence Zitnick, and Piotr Doll¬¥ar. Microsoft
coco: Common objects in context, 2015. 5
[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. Advances in neural information
processing systems, 36:34892‚Äì34916, 2023. 2
[38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 26296‚Äì26306, 2024. 2
[39] Xinyi Liu, Yujie Wang, Fangcheng Fu, Xupeng Miao, Shen-
han Zhu, Xiaonan Nie, and Bin CUI. Netmoe: Accelerating
moe training through dynamic sample placement.
In The
Thirteenth International Conference on Learning Represen-
tations, 2025. 2
[40] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your
multi-modal model an all-around player?, 2024. 5
[41] Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang,
Bo Zhang, Junchi Yan, and Hongsheng Li.
Not all ex-
perts are equal: Efficient expert pruning and skipping for
mixture-of-experts large language models.
arXiv preprint
arXiv:2402.14800, 2024. 1, 2, 3, 5, 6, 7, 13
[42] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra
Malik. Egoschema: A diagnostic benchmark for very long-
form video language understanding, 2023. 5
[43] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Ena-
mul Hoque. ChartQA: A benchmark for question answer-
ing about charts with visual and logical reasoning. In Find-
ings of the Association for Computational Linguistics: ACL
2022, pages 2263‚Äì2279, Dublin, Ireland, 2022. Association
for Computational Linguistics. 3, 5, 16
[44] OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. 7
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748‚Äì8763. PmLR, 2021. 1
[46] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-
geously large neural networks: The sparsely-gated mixture-
of-experts layer, 2017. 1
[47] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,
Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards
vqa models that can read. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
8317‚Äì8326, 2019. 5
[48] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen
Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chen-
zhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv
preprint arXiv:2504.07491, 2025. 1, 2, 3, 5, 6, 7, 8, 15, 16
[49] V Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo
Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi,
Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan
Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang
Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi,
Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing
Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jin-
hao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi
Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu,
Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong,
Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu,
Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang
Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiao-
han Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yan-
10


ling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yi-
fan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu,
Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yut-
ing Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou,
Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin
Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang.
Glm-4.5v and glm-4.1v-thinking: Towards versatile multi-
modal reasoning with scalable reinforcement learning, 2025.
1
[50] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of Machine Learning Research, 9
(86):2579‚Äì2605, 2008. 3
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 2
[52] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi
Parikh. Cider: Consensus-based image description evalua-
tion, 2015. 6
[53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,
Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui
Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Jun-
yang Lin. Qwen2-vl: Enhancing vision-language model‚Äôs
perception of the world at any resolution.
arXiv preprint
arXiv:2409.12191, 2024. 2
[54] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,
Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, et al. Qwen2-vl: Enhancing vision-language model‚Äôs
perception of the world at any resolution.
arXiv preprint
arXiv:2409.12191, 2024. 1, 2
[55] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long
Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong
Ye, Jie Shao, et al.
Internvl3. 5: Advancing open-source
multimodal models in versatility, reasoning, and efficiency.
arXiv preprint arXiv:2508.18265, 2025. 1, 2, 5, 7, 13, 15
[56] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim
Rault, R¬¥emi Louf, Morgan Funtowicz, Joe Davison, Sam
Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush.
Trans-
formers: State-of-the-art natural language processing.
In
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations,
pages 38‚Äì45, Online, 2020. Association for Computational
Linguistics. 7
[57] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li.
Longvideobench: A benchmark for long-context interleaved
video-language understanding, 2024. 5
[58] x.ai. Grok-1.5 vision preview, 2024. 5, 6
[59] Yanyue Xie, Zhi Zhang, Ding Zhou, Cong Xie, Ziang Song,
Xin Liu, Yanzhi Wang, Xue Lin, and An Xu. Moe-pruner:
Pruning mixture-of-experts large language model using the
hints from its router. arXiv preprint arXiv:2410.12013, 2024.
2
[60] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen
Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv
preprint arXiv:2505.09388, 2025. 7
[61] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
Mm-vet: Evaluating large multimodal models for integrated
capabilities. arXiv preprint arXiv:2308.02490, 2023. 5
[62] Tongtian Yue, Longteng Guo, Jie Cheng, Xuange Gao, Hua
Huang, and Jing Liu. Ada-k routing: Boosting the efficiency
of moe-based llms. In The Thirteenth International Confer-
ence on Learning Representations, 2024. 1, 2
[63] Kaichen
Zhang,
Bo
Li,
Peiyuan
Zhang,
Fanyi
Pu,
Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan
Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-
eval: Reality check on the evaluation of large multimodal
models, 2024. 5
[64] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shen-
glong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie
Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao,
Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin
Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng,
Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Con-
ghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun
He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao,
Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye
Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou
Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai
Wang. Internvl3: Exploring advanced training and test-time
recipes for open-source multimodal models, 2025. 2
11


Appendix
This document supplements the main paper as follows:
‚Ä¢ Sec. A provides detailed proofs for the proposed frontier
search;
‚Ä¢ Sec. B details additional experimental setups;
‚Ä¢ Sec. C provides additional comparisons with baselines
across different expert skipping ratios and MLLMs;
‚Ä¢ Sec. D presents visual question answering examples
across methods;
‚Ä¢ Sec. E reports ablations on the number of grid points in
frontier search;
‚Ä¢ Sec. F shows ablations on the number of samples used for
calibration and search;
‚Ä¢ Sec. G analyzes expert redundancy w.r.t. tokens across
modalities.
A. Proofs
In this section, we first provide complete proofs of the
correctness and time complexity for our frontier search
(Prop. 1). We then prove that the optimal thresholds lie on
the frontier (Prop. 2).
Lemma 1 (Monotone feasibility in p). For fixed q, define
Œ¶q(p) :=

g(œÑ (q), œÑ (p)) ‚â•œÅ

.
(I)
If g is non-decreasing in its second argument, then Œ¶q(p)
is monotone in p. Hence, if a feasible p exists, the smallest
feasible index
p(q) := min{ p : Œ¶q(p) }
(II)
is well-defined.
Proof. If p1 ‚â§p2 and Œ¶q(p1) holds, then by monotonicity
of g in its second argument,
g(œÑ (q), œÑ (p2)) ‚â•g(œÑ (q), œÑ (p1)) ‚â•œÅ,
(III)
so Œ¶q(p2) holds. Therefore, the feasible set is a suffix in p,
and the minimum exists when the set is non-empty.
Lemma 2 (Monotone shift in q). Assume g is non-
decreasing in its first argument. If q‚Ä≤ ‚â§q and both p(q‚Ä≤)
and p(q) exist, then
p(q) ‚â§p(q‚Ä≤).
(IV)
Proof. For any fixed p and q‚Ä≤ ‚â§q,
g(œÑ (q), œÑ (p)) ‚â•g(œÑ (q‚Ä≤), œÑ (p)).
(V)
Hence
{ p : Œ¶q(p) } ‚äá{ p : Œ¶q‚Ä≤(p) }.
(VI)
Taking minima over these sets gives p(q) ‚â§p(q‚Ä≤).
Lemma 3 (Loop invariant). Let p be the pointer value at
the start of the q-th outer iteration in Alg. 1. If p(q) exists,
then
p ‚â•p(q) ‚àí1.
(VII)
Moreover, after the inner loop for this q, the algorithm stops
at p = p(q) ‚àí1 and records p(q) = p + 1.
Proof. Base case (q = 1): The algorithm sets p ‚ÜêD, and
D ‚â•p(1), so the claim holds.
Inductive step: Assume that the claim holds for q. By
Lem. 1, Œ¶q is monotone in p. The inner loop decreases
p until ¬¨Œ¶q(p) holds for the first time. Thus, it stops at
p = p(q) ‚àí1, and the code sets p(q) ‚Üêp + 1. For the next
iteration, the carried pointer is p ‚Üêp(q) ‚àí1. By Lem. 2,
p(q+1) ‚â§p(q), hence
p = p(q) ‚àí1 ‚â•p(q+1) ‚àí1.
(VIII)
Thus, the invariant holds for q + 1.
Proposition 1 (Correctness and time). Assume g is non-
decreasing in each argument. Then Lines 1-12 of Alg. 1
compute the frontier {(q, p(q))}. If each evaluation of (f, g)
on C costs O(N) time, the total time is O(ND).
Proof. By Lem. 1, each feasible p(q) is well-defined. By
Lem. 3, at the q-th iteration the inner loop stops at p =
p(q) ‚àí1 and records p(q) = p+1, which is the smallest fea-
sible index. If no feasible p exists for some q, then Œ¶q(D)
is false and the guard p(q) ‚â§D excludes this q, as desired.
Therefore, Lines 1-12 are correct.
For the time bound, by Lem. 2, p(q) is non-increasing
in q. Hence, across all outer iterations, the while-guard in-
spects g at most D times when p is decremented and at most
D additional times when the guard fails immediately at the
start of an iteration, so the total number of guard evaluations
of g is at most 2D (i.e., O(D)). Moreover, for each recorded
frontier element (q, p(q)) (at most D in total), we use a sin-
gle forward pass that computes f(œÑ (q), œÑ (p(q))). Each evalu-
ation costs O(N). Therefore, the total time is O(ND).
Implementation note. In practice, we compute f and g
simultaneously and can record their values. This merges
their costs and reduces constant factors, while the asymp-
totic bound remains O(ND).
Lemma 4 (Frontier suffices). Assume f is non-decreasing
in each argument and F = {(q, p) : g(œÑ (q), œÑ (p)) ‚â•œÅ} Ã∏=
‚àÖ. For any fixed feasible q, the pair (q, p(q)) satisfies
f
 œÑ (q), œÑ (p(q))
‚â§f
 œÑ (q), œÑ (p)
for all (q, p) ‚ààF. (IX)
Proof. By definition, p ‚â•p(q) for all feasible (q, p). Since
f is non-decreasing in its second argument,
f
 œÑ (q), œÑ (p(q))
‚â§f
 œÑ (q), œÑ (p)
.
(X)
12


Table I. Performance comparisons for Qwen3-VL-MoE-30B-A3B-Instruct [25] across various expert skipping ratios.
Method
Image Understanding
Video Understanding
Avg.
(%)
TextVQA ChartQA
MMStar MMBench
MMVet
MME
RealWorldQA
COCO
MVBench
EgoSchema
VMME
LVB
VMMMU
k = 8 (Default)
83.41
85.08
59.67
86.60
69.68
2500
66.80
80.37
64.67
62.45
54.89
55.42
47.11
100.00
Skip 63% Experts (œÅ = 0.60)
k = 3
80.81
78.12
66.74
83.33
68.39
2326
45.88
71.70
62.02
57.96
53.48
54.60
50.44
95.20
NAEE [41]
81.20
79.41
55.39
84.18
68.61
2348
59.67
78.09
61.31
58.32
51.08
55.12
48.32
95.61
MC-MoE [21]
82.51
79.37
56.48
86.12
69.37
2438
62.01
76.82
62.61
58.73
54.22
54.13
48.54
97.09
DiEP [6]
82.04
80.23
57.26
85.07
68.42
2405
60.31
75.41
63.15
59.46
53.41
55.08
48.76
96.80
MoDES (Ours)
81.82
82.48
58.61
86.17
69.95
2493
63.92
76.55
64.42
62.39
55.15
55.50
49.89
99.22
Skip 75% Experts (œÅ = 0.73)
k = 2
77.54
69.60
62.38
80.50
61.33
2060
55.56
82.77
60.70
53.79
50.67
54.08
46.00
92.03
NAEE [41]
78.42
77.28
54.64
81.34
65.58
2208
61.75
77.31
60.98
55.24
48.87
54.87
47.12
93.25
MC-MoE [21]
80.13
78.41
57.02
85.32
67.22
2286
61.83
74.49
61.65
57.13
52.64
54.03
47.49
95.11
DiEP [6]
79.64
78.52
56.48
84.91
67.13
2243
60.94
75.53
62.78
57.86
52.38
54.62
48.16
95.21
MoDES (Ours)
81.65
82.44
58.78
86.25
67.61
2469
64.71
75.73
64.45
62.53
54.81
55.57
51.22
99.11
Skip 88% Experts (œÅ = 0.85)
k = 1
60.71
52.16
31.63
54.90
28.07
1590
52.42
45.64
41.51
32.52
39.78
42.41
12.51
60.11
NAEE [41]
72.41
65.83
48.88
73.62
54.52
1984
58.62
60.37
50.24
49.77
44.48
45.59
35.57
80.60
MC-MoE [21]
74.87
71.43
50.74
75.42
61.35
2168
60.41
68.15
56.60
51.84
52.51
47.22
37.41
86.66
DiEP [6]
73.46
70.51
53.28
73.21
58.64
2074
63.41
62.89
57.21
53.61
50.78
46.13
34.79
85.30
MoDES (Ours)
80.97
78.84
58.18
85.57
67.75
2403
64.58
74.66
62.98
62.04
55.26
55.50
46.56
97.33
Table II. Performance comparisons for InternVL-3.5-30B-A3B-HF [55] across various expert skipping ratios.
Method
Image Understanding
Video Understanding
Avg.
(%)
TextVQA ChartQA
MMStar MMBench
MMVet
MME
RealWorldQA
COCO
MVBench
EgoSchema
VMME
LVB
VMMMU
k = 8 (Default)
85.76
84.08
62.49
83.81
69.93
2312
64.77
69.30
68.92
60.49
58.07
57.64
45.11
100.00
Skip 63% Experts (œÅ = 0.60)
k = 3
82.16
81.38
60.30
77.94
68.67
1964
61.34
65.47
65.34
58.83
55.62
55.81
42.07
94.79
NAEE [41]
82.98
83.02
61.18
79.65
67.57
2054
61.47
66.05
66.73
58.46
56.34
55.74
42.81
95.86
MC-MoE [21]
84.36
83.22
61.45
80.89
68.67
2192
62.13
66.87
67.38
59.03
56.79
56.02
43.45
97.25
DiEP [6]
83.68
82.79
61.82
80.22
68.13
2084
62.56
67.17
66.82
58.74
56.25
57.84
43.16
96.82
MoDES (Ours)
84.27
83.15
62.06
81.46
68.41
2289
63.10
68.22
68.64
60.15
57.76
56.12
43.84
98.42
Skip 75% Experts (œÅ = 0.73)
k = 2
64.51
64.25
46.69
71.56
56.42
1821
57.29
58.28
61.42
53.25
51.06
48.87
38.63
83.02
NAEE [41]
75.37
76.18
58.82
74.53
61.38
1968
59.47
63.31
64.46
54.83
55.45
52.79
41.08
90.76
MC-MoE [21]
77.41
78.24
57.65
75.58
66.41
2037
60.28
64.24
65.18
56.14
53.65
53.08
41.74
92.30
DiEP [6]
76.84
79.12
58.42
76.14
65.27
2021
58.74
63.10
64.89
55.83
54.12
54.22
40.23
91.80
MoDES (Ours)
82.13
82.54
61.46
81.88
67.92
2258
62.48
67.89
68.83
60.32
57.54
55.85
44.16
97.90
Skip 88% Experts (œÅ = 0.85)
k = 1
58.49
46.24
42.27
51.74
35.05
1683
51.44
26.01
31.99
34.47
35.26
37.40
24.27
59.63
NAEE [41]
66.24
68.32
50.14
64.37
49.52
1802
55.23
50.64
54.78
50.25
48.69
47.42
37.27
78.88
MC-MoE [21]
70.41
73.49
56.14
64.38
65.41
1972
57.49
60.12
58.97
52.31
49.72
48.31
40.06
86.20
DiEP [6]
69.37
71.84
57.21
63.19
65.32
1838
56.38
55.78
56.26
51.48
48.94
47.26
38.18
83.26
MoDES (Ours)
80.58
82.00
61.20
81.67
67.80
2222
61.73
65.16
68.65
60.79
57.63
54.49
44.33
97.03
Proposition 2 (Optimality on the frontier). Under the as-
sumptions of Lem. 4, any optimal solution of
min
(q,p)‚àà{1,...,D}2 f(œÑ (q), œÑ (p))
s.t.
g(œÑ (q), œÑ (p)) ‚â•œÅ (XI)
lies on the frontier {(q, p(q))}.
Proof. By Lem. 4, for each feasible q, the best feasible
choice in p is p(q). Therefore, an optimal pair can be chosen
from

(q, p(q)) : p(q) exists
	
,
(XII)
which is exactly the frontier. This is what Lines 13‚Äì14 min-
imize over, using the f-values already stored when each
(q, p(q)) was inserted into the frontier.
B. More Setups
Baselines.
As noted in Sec. 6.1, baselines such as
NAEE [41], MC-MoE [21], and DiEP [6] are not directly
compatible with MoE MLLMs when (k > 2). We therefore
describe more about our adaptations here. For the hyperpa-
rameter Œ≤(l), we perform a layer-wise search from the first
to the last layer on the same dataset as our method. When
tuning the l-th layer, we constrain the cumulative skipping
ratio from the 1-st layer through the l-th layer to be the tar-
get ratio. We apply the same procedure to the other base-
lines. All remaining settings follow the original papers.
Implementation. In practice, we normalize Œ±(l) across lay-
ers as g
Œ±(l) =
Œ±(l)
PL
l‚Ä≤=1 Œ±(l‚Ä≤) . During inference, we compute
s(l)
i
= g
Œ±(l) ¬∑ œÄ(l)
i
for a given token x(l). Since 0 < œÄ(l)
i
< 1
13


Question: Based on data presented in the video, what was the approximate difference in value between the US bond market and the US stock market as of 2020?
Qwen3-VL-MoE-30B-A3B-Instruct : The video states that the bond market was valued at $50.1 trillion and the stock market at $40.7 trillion. The difference is calculated as 
$50.1 trillion - $40.7 trillion, which equals $9.4 trillion.
MC-MoE:  The US bond market value was $50.1 trillion, while the US stock market value was $40.7 trillion. The difference in value between the two is $5.1 trillion.
DiEP:  $50.1 trillion dollars.
MoDES (Ours): The US bond market was valued at $50.1 trillion, while the stock market was valued at $40.7 trillion, resulting in a difference of approximately $9.4 trillion.
Question: Subtract all brown blocks. Subtract all large blue rubber things. Which objects are left? Use a sentence to answer the 
question.
Qwen3-VL-MoE-30B-A3B-Instruct : After subtracting the brown block and the large blue rubber sphere, the objects left are a green 
cube, a yellow cube, a purple cube, a gray cylinder, a small purple cylinder, a red cube, and a large purple cylinder.
MC-MoE:  The objects left are: a gray cylinder, a yellow cube, a purple cube, and a purple cylinder.
DiEP:  The objects left are a gray cylinder and a blue sphere.
MoDES (Ours): After subtracting the brown block and the large blue rubber sphere, the remaining objects are a green cube, a small 
purple cylinder, a small red cube, a large purple cylinder, a large yellow cube, a large purple cube, and a gray cylinder.
Question: How much time has been spent on the answer?
Qwen3-VL-MoE-30B-A3B-Instruct: Based on the image provided, we can determine the time spent on the current question by 
looking at the progress bar and the timer displayed.
- The progress bar shows the elapsed time and the total duration of the audio.
- The timer on the left side of the progress bar reads `0:26`.
- The timer on the right side of the progress bar shows the total duration, which is `-9:43`. The negative sign indicates the time 
remaining, so the total duration is 9 minutes and 43 seconds.
The number `0:26` represents the amount of time that has passed since the audio started playing.
Therefore, **26 seconds** have been spent on the answer.
MC-MoE: The image provided shows a screen from a mobile application, likely a quiz or learning app, where the user is asked to 
choose the correct answer to a question. The question is "Why is cod becoming less popular?" with three options: "Because it's not 
very healthy," "Because people don't like the taste," and "Because there are fewer fish in the sea."
The time spent on the answer is not explicitly shown in the image. However, the time spent on the answer is the difference between 
the time spent on the current question and the time spent on the previous question. The time spent on the previous question is 0:26, 
and the time spent on the current question is 9:43. Therefore, the time spent on the answer is 9:43 - 0:26 = 9:17.
The time spent on the answer is 9:17.
DiEP: The given question is "Why is cod becoming less popular?" and the options are "Because it's not very healthy", "Because 
people don't like the taste", and "Because there are fewer fish in the sea". The correct answer is "Because there are fewer fish in the 
sea". The time spent on this answer is 9:43.
MoDES (Ours): Based on the image provided, we can determine the time spent on the answer by examining the progress bar.
- The progress bar shows the total duration of the audio clip, which is `9:43`.
- The current position of the playhead on the bar indicates how much time has passed.
- The time displayed to the left of the playhead is `0:26`.
This means that 26 seconds have elapsed since the audio began playing.
Therefore, the time spent on the answer is **26 seconds**.
Figure I. Visual understanding examples from Qwen3-VL-MoE-A3B-Instruct [25]. We employ an 88% expert skipping ratio for all
methods, and color the text to show the correct or the wrong responses.
(i ‚ààS(l)), s(l)
i
‚àà(0, 1). Thus, we choose D = 100 grids
in (0, 1) as B to search for optimal thresholds. In detail,
we apply a rectified sigmoid function to 100 grids falling
into [0, 1] with equal intervals. For inference speed mea-
surement, we implement efficient CUDA kernels for MoE
layers. To efficiently execute the computations for the acti-
vated experts, we employ a Grouped General Matrix Multi-
plication (Group GEMM) approach. Group GEMM enables
the concurrent execution of all required matrix multiplica-
tions within a single, unified kernel launch. Each expert‚Äôs
computation is treated as an independent sub-task within the
group. The performance of this kernel is highly dependent
on the workload distribution. Therefore, to achieve maxi-
mum efficiency, we perform an offline profiling step where
we conduct a grid search to identify the optimal kernel tile
sizes for various representative activation patterns. This en-
sures high computational throughput across the diverse and
dynamic workloads characteristic of MoDES computation.
All performance experiments are conducted on 8√óH200
GPUs, and efficiency experiments are performed on a single
14


Question: Is the actress in this picture named Anne Hathaway, and which movie is this picture from?
Kimi-VL-A3B-Instruct: Yes, the actress in this picture is named Anne Hathaway, and this picture is from the movie "The Devil 
Wears Prada ." 
MC-MoE: Yes, the actress in the picture is Anne Hathaway, and this is from the movie "Rachel Getting Married."
DiEP: The actress's name is Anne Hathaway, and the movie from is Gattino.
MoDES (Ours): Yes, the actress in the picture is Anne Hathaway, and this is from the movie "The Devil Wears Prada."
Question: Please carefully observe the image and come up with a caption for the image.
Kimi-VL-A3B-Instruct: The image depicts a bunch of ripe bananas resting on a wooden surface, with a computer keyboard and a 
cup visible in the background, suggesting a casual or home office setting.
MC-MoE: The image features a bunch of bananas prominently displayed on a wooden table. The background is blurred, with a cup of 
coffee placed on the table, and a newspaper also visible. The arrangement of the objects suggests a casual, yet organized setup.
DiEP: The image depicts a stack of bananas, which are yellow and slightly green at the bottom, and a bunch of green bananas at the 
top. There are also some bananas in the middle of the stack, but the middle bananas are green as well.
MoDES (Ours): The image depicts a stack of bananas placed on a wooden surface, with a computer keyboard and a cup visible in the 
background. The bananas are yellow with some brown spots, indicating ripeness. The setting appears to be a workspace or desk area.
Question: How many pens are shown in the video?       Kimi-VL-A3B-Instruct: Three pans.       MC-MoE: Two pans.       DiEP: Two pans.       MoDES (Ours): Three pans.
Figure II. Visual understanding examples from Kimi-VL-A3B-Instruct [48]. We employ an 83% expert skipping ratio for all methods.
H200 GPU.
C. More Comparison with Baselines
We provide additional results for the Qwen3-VL-MoE-30B-
A3B-Instruct [25] and InternVL-3.5-30B-A3B-HF [55] in
Tabs. I and II, respectively. The observations from these re-
sults align with the phenomena identified in Kimi-VL-A3B-
Instruct [48]. Across different expert skipping ratios, our
method consistently outperforms the baselines, with espe-
cially large gains at high skipping levels (‚â•75%).
D. Visual Understanding Visualization
In this section, we present a case study comparing our pro-
posed MoDES with previous SOTA methods [6, 21] for
LLMs. As shown in Figs. I and II, MoDES consistently
generates text that far outperforms the baselines.
E. Ablation for N
Table III. Ablation results for N.
N
ChartQA
MME
MMBench
LVB
VMMMU
Skip 67% Experts (œÅ = 0.65)
2048
88.32
2201
82.79
62.92
48.89
1024 (Ours)
88.24
2204
82.73
62.90
48.78
512
87.44
2122
81.27
61.95
47.68
256
85.56
2085
79.68
60.63
45.11
Skip 83% Experts (œÅ = 0.80)
2048
84.84
2186
81.45
62.63
46.67
1024 (Ours)
84.20
2162
81.44
62.60
47.11
512
84.12
2118
80.27
61.88
46.85
256
83.35
2016
77.48
59.84
43.69
We apply MoDES to Kimi-VL-A3B-Instruct [48] using
different numbers of data samples from GQA [24] and show
the results in Tab. III. The results indicate a clear trend:
With more calibration samples, models using expert skip-
ping perform better. Yet the accuracy gains become smaller
as the sample count grows. Moreover, doubling the sam-
ples increases both calibration and search time by ‚àº2√ó. To
balance accuracy and cost, we use 1024 samples in this pa-
per. This choice provides most of the achievable gains while
keeping computation reasonable (Sec. 6.3).
F. Ablation for D
Table IV. Ablation results for D.
D
ChartQA
MME
MMBench
LVB
VMMMU
Skip 67% Experts (œÅ = 0.65)
200
88.16
2219
82.78
62.94
48.76
100 (Ours)
88.24
2204
82.73
62.90
48.78
50
87.85
2178
81.76
62.21
47.89
Skip 83% Experts (œÅ = 0.80)
200
84.78
2178
81.61
62.59
47.00
100 (Ours)
84.20
2162
81.44
62.60
47.11
50
83.96
2143
80.68
62.47
47.15
We ablate the number of grid points D in the search
space B. As shown in Tab. IV, larger D brings diminishing
accuracy gains, so using a very fine grid (e.g., D > 100)
is unnecessary. The time cost also grows roughly linearly
with D. Based on this trade-off, we set D = 100 in this
work.
G. Expert Redundancy across Modalities
In this section, we analyze expert redundancy across modal-
ities. As shown in Fig. III, reducing k for vision tokens
causes task performance to drop more slowly than for text
tokens. This indicates greater redundancy among experts
15


(a) ChartQA [43]
0.0
2.5
5.0
Top-k Experts
0
20
40
60
80
Score
Text
Vision
(b) MME [16]
0.0
2.5
5.0
Top-k Experts
0
500
1000
1500
2000
Score
Text
Vision
(c) VideoMMMU [19]
0.0
2.5
5.0
Top-k Experts
20
40
Score
Text
Vision
Figure III. Task performance across various numbers of top-k
routed experts applied to tokens of different modalities for Kimi-
VL-A3B-Instruct [48].
for vision tokens, allowing more aggressive skipping than
for text tokens. It also motivates modality-aware strategies
for expert skipping.
16
