Instruction-Based Coordination of Heterogeneous
Processing Units for Acceleration of DNN Inference
Anastasios Petropoulos and Theodore Antonakopoulos
Dept. of Electrical and Computer Engineering, University of Patras, Patras, Greece
a.petropoulos@ece.upatras.gr, antonako@upatras.gr
Abstractâ€”This paper presents an instruction-based coordina-
tion architecture for Field-Programmable Gate Array (FPGA)-
based systems with multiple high-performance Processing Units
(PUs) for accelerating Deep Neural Network (DNN) inference.
This architecture enables programmable multi-PU synchroniza-
tion through instruction controller units coupled with peer-to-
peer instruction synchronization units, utilizing instruction types
organized into load, compute, and store functional groups. A com-
pilation framework is presented that transforms DNN models into
executable instruction programs, enabling flexible partitioning
of DNN models into topologically contiguous subgraphs mapped
to available PUs. Multiple deployment strategies are supported,
enabling pipeline parallelism among PUs and batch-level paral-
lelism across different PU subsets, with runtime switching among
them without FPGA reconfiguration. The proposed approach
enables design space exploration, supporting dynamic trade-offs
between single-batch and multi-batch performance. Experimental
results on ResNet-50 demonstrate notable compute efficiency, up
to 98%, and throughput efficiency gains, up to 2.7Ã—, over prior
works across different configurations.
Index
Termsâ€”Deep
Neural
Networks
(DNNs),
Field-
Programmable
Gate
Array
(FPGA),
hardware
accelerator,
heterogeneous architecture.
I. INTRODUCTION AND RELATED WORK
The advancement of Deep Neural Networks (DNNs) has
driven the demand for specialized hardware acceleration,
with Field-Programmable Gate Arrays (FPGAs) considered
as promising accelerators due to their reconfigurability and
parallel processing capabilities for DNN inference [1]. Numer-
ous automated design frameworks have emerged for DNN-to-
FPGA acceleration [2]â€“[11], typically following two architec-
tural strategies: unified Processing Unit (PU) architectures and
heterogeneous PU architectures, as described in [7], [12].
Unified PU architectures employ generic accelerators that
execute DNN layers sequentially. These designs typically
utilize systolic array (SA)-based PUs with DSP48E2 units
[13], operating at high clock frequencies, to achieve high
throughput [6], [14]â€“[16]. Recent unified accelerators target di-
verse models, such as architectures that support both attention
and convolutional (Conv) operations [17] or versatile designs
that handle Convolutional Neural Networks (CNNs), Graph
Neural Networks (GNNs), and Vision Transformers (ViTs)
[18]. Flexible architectures have also emerged for arbitrary-
kernel convolutions [19] and the exploitation of dynamic
parallelism in each DNN layer [20]. Although multiple PU in-
stantiations enable batch-level parallelism [6], [16], [21], these
approaches process layers within individual PUs, leaving inter-
PU pipeline opportunities unexploited. Thus, the sequential
nature inherently limits single-batch performance, requiring
large batch sizes to achieve high throughput.
Heterogeneous architectures take a different approach, ex-
ploiting dedicated PUs for subsets of DNN layers [22]â€“[25]
or even for each layer [26], enabling pipelining across entire
models. Some designs also focus on low-latency inference
through optimized heterogeneous accelerators [27]. In addi-
tion, hybrid approaches have emerged that combine elements
of both strategies to balance flexibility and performance. In
this context, segment-grained pipeline architectures share PUs
across DNN model segments [12]. In contrast, others fuse
layer groups into pipeline stages [7] or adopt heterogeneous
PUs for initial layers and uniform PUs for the remaining layers
[8]. SA-based implementations are also commonly found in
heterogeneous architectures [24], [25], [28].
Instruction Set Architectures (ISAs) provide runtime pro-
grammability for different DNN models without FPGA re-
configuration [5], [6], [10], [21], [29], [30]. However, existing
instruction-based approaches focus on single-PU control, and
extending these concepts to multi-PU coordination presents
challenges in distributed execution and inter-PU synchroniza-
tion. Multi-PU approaches typically operate either as batch-
level parallelism with independent PU processing of complete
models, or as pipeline execution among PUs for single batches.
These execution schemes are generally fixed at design time,
limiting flexibility under dynamic workload conditions.
This work introduces an instruction-based coordination ar-
chitecture that enables the synchronization of multiple PUs
without requiring FPGA reconfiguration. Our approach em-
ploys an Instruction Controller Unit (ICU) within each PU
and peer-to-peer Instruction Synchronization Units (ISUs)
that manage inter-PU synchronization through hardware-based
request-acknowledgment mechanisms. The key innovation lies
in expressing coordination patterns within instruction se-
quences that execute in each PU. Also, via instruction updates,
the architecture can enable runtime switching between pipeline
parallelism for single-batch acceleration and hybrid paral-
lelism, which combines pipeline and batch-level strategies.
In addition, we developed a comprehensive DNN compila-
tion framework that transforms DNN models into instruction
programs optimized for multi-PU execution. In particular,
it encompasses model processing with hardware-aware fu-
sion, node-to-PU partitioning with weight transfer scheduling,
pipeline memory management, and instruction generation.
This compilation approach, along with the proposed archi-
Â© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any
copyrighted component of this work in other works.
arXiv:2511.15505v1  [cs.AR]  19 Nov 2025


tecture, enables systematic Design Space Exploration (DSE)
across diverse deployment strategies for selecting deployment
configurations, allowing runtime adaptation based on applica-
tion requirements and constraints.
Our contributions include: 1) an instruction-based coor-
dination architecture enabling programmable multi-PU syn-
chronization; 2) a compilation framework transforming DNN
models into instruction sequences and supporting flexible
parallelism strategies; 3) a DSE methodology for the system
architecture for single-batch and multi-batch configurations;
and 4) experimental validation on the ResNet-50 across differ-
ent deployment configurations, demonstrating notable compute
and throughput efficiency gains over prior works.
The remainder of this paper is organized as follows. Section
II presents the hardware foundation, covering the baseline
heterogeneous PU architecture and the custom ISA. Section
III details the multi-PU coordination architecture, including
the ICU and ISU components. Section IV describes the DNN
compilation framework, while Section V presents the imple-
mentation details, the DSE methodology, and the performance
evaluation for ResNet-50.
II. HARDWARE FOUNDATION
In this section, the hardware foundation for multi-PU co-
ordination is presented, covering the baseline heterogeneous
design and the custom ISA that enables programmable coor-
dination and execution among PUs.
A. Baseline Architecture
This work builds upon a heterogeneous multi-PU archi-
tecture designed for FPGA-based DNN acceleration [16],
providing the hardware baseline enhanced in this work with
instruction-based coordination mechanisms for programmable
inter-PU synchronization.
In the baseline architecture, five PU1x and five PU2x units
are instantiated across the Super Logic Regions (SLRs) of
the FPGA. PU1x and PU2x units are configured as 64Ã—4 and
64Ã—8 SAs, respectively, with the latter exhibiting twice the
computational performance. Both PU types support Conv and
Fully Connected (FC) layers via General Matrix-Matrix Mul-
tiplication (GEMM) operations. Fig. 1 illustrates the multi-PU
system organization, showing distributed PUs across different
FPGA regions abstractly, with ISUs forming the coordination
infrastructure introduced in this work for orchestrating the
dataflow across PUs during DNN inference.
The PU, as shown in Fig. 1 inset, comprises three func-
tional blocks. The pre-processing block contains two AXI
DataMover (ADM) modules that interface with the High-
Bandwidth Memory (HBM) and utilize Block RAM (BRAM)-
based ping-pong activation buffers for continuous data stream-
ing. The main block features the SA for GEMM opera-
tions, utilizing UltraRAMs (URAMs) to store weights and
biases at twice the system clock (sys clk) frequency, denoted
as dsp clk. The post-processing block comprises activation
functions, vector units that support residual additions, and a
wave reorder buffer that manages out-of-order systolic waves,
enabling about 98% performance efficiency on ResNet-50.
Peer-to-peer 
ISUs
HBM
SLR1
SLR0
Systolic Array (SA)
Unit
Weight 
URAMs
Vector 
Unit
Inputs Ping/Pong
W
Inp
RA
Out
cmd/sts
Inp/Out
AXI DataMovers
W/RA
AXI
AXI
GEMM
ğ‘Šğ‘Ã—ğ‘€ ğ´ğ‘€Ã—ğ‘ƒâ†’ ğ‘‚ğ‘Ã—ğ‘ƒ 
SA: ğ‘…Ã— ğ¶
ğ‘…: output-row parallelism (over ğ‘ dim.)
ğ¶: dot-product parallelism (over ğ‘€ dim.)
Fig. 1. System architecture of multiple PUs. Inset: PU architecture [16].
B. Instruction Set Architecture
The ICU architecture employs a custom ISA, designed to
support PU operations and to coordinate dataflow execution
across multiple PUs. Instructions are organized into six distinct
types that collectively enable configurable control of memory
transfers, computations, and inter-PU synchronization. This
instruction-based approach enables runtime reconfiguration
without hardware changes, supporting different DNN models
and PUs configurations by updating only the PU instructions.
The instruction set
covers three ICU groups (Load,
Compute, Store), each optimized for specific dataflow func-
tions, decoupling of memory accesses from computational
tasks, and enabling prefetching and overlapping of data trans-
fers with SA operations. The Load (LD) group handles input
activation data management, including linear, Image to Col-
umn (IM2COL) transform, and stride-patterned data move-
ment capabilities, along with acknowledgment transmission
and request awaiting for synchronization. The Compute (CP)
group focuses on GEMM operations, weights management,
and residual shortcut activation data transfers, supporting both
linear and stride-patterned transfers. The Store (ST) group
manages output activation data transfers and complements the
LD groupâ€™s synchronization capabilities by providing trans-
mission request and acknowledgment awaiting functions.
Each ICU group has its dedicated dual-port BRAM for stor-
ing instruction programs, with program rounds representing
complete execution cycles that iterate through instructions in
sequential order until reaching the end of the program, i.e., an
instruction with the PRG_END field set. All instructions use
64-bit instruction length and include operation code (OPCD)
and PRG_END fields for consistent decoding. In addition,
they are classified into two categories, based on their runtime
behavior: static instructions remain unchanged during pro-
gram execution, while dynamic instructions update their state
through write-back mechanisms that modify the respective
ICU BRAM parameters.
In Table I(a), the core instruction types are detailed and
described below, with each type serving a specific role in
PU operations. Table I(b) presents the dynamic state update
algorithms, while Table I(c) shows the basic instructions
within each ICU group.
â€¢ ProgCtrl: It manages program flow and looping behavior
for all ICU groups. The rounds parameter (NR) controls the
instruction program behavior: zero enables infinite loops until


TABLE I
ISA OVERVIEW
(a) Instruction Types
TYPE1
ICU GROUPS
DESCRIPTION; KEY FIELDS2
LD
CP
ST
ProgCtrl
PRG PRM
âœ“
âœ“
âœ“
Control program loops; NR, ICU_BA
Config
* PRM
âœ“
âœ“
âœ“
Prepare PU before ADM; stride_pattern,
IM2COL, URAM_addr
DataMove
* ADM
âœ“
âœ“
âœ“
ADM
transfers
control;
CUR_BA, LEN.
CUR_BA latched for successor AddrCyc.
AddrCyc
CYCLE ADDR
âœ“
âœ“
âœ“
Cyclic addressing; BA, AOFFS, NC, IC
Sync
SEND/WAIT REQ/ACK
âœ“
âœ“
Inter-ICU
coordination;
DST/SRC_PID,
BID, BASE_BID, NC, IC
Compute
GEMM
âœ“
PU operations; ReLu, Rounds, Scales,
Add_enable
(b) Key Instruction Type Algorithms & Write-Back
TYPE
PSEUDOCODE & WRITE-BACK3
ProgCtrl if (NR == 0): âˆ-loop: run_until(PRG_END); jump(ICU_BA)
else: repeat(NR): run_until(PRG_END); jump(ICU_BA)
AddrCyc if (IC == 0): IC, CUR_BA = NC, BA
else: IC, CUR_BA = IC-1, CUR_BA+AOFFS
Updates: predecessor CUR_BA, current IC
Sync
if (NC == 0): BID = BID // bypass
elif (IC == 0): BID, IC = BASE_BID, NC
else: BID, IC = BID+1, IC-1
Updates: current BID, IC
1 ProgCtrl, Config, Compute: static instructions; Others: dynamic ICU BRAM write-back.
2 All instructions include OPCD, PRG_END fields.
3 Updates: Write-back to ICU BRAM. IC init: NC when loaded offline.
4 â†’: mandatory sequence. âˆ—: optional successor to any *_ADM.
(c) Basic Instructions in each ICU Group
ICU
BASIC INSTRUCTIONS4
LD
LINEAR_ADM, (IM2COL_PRMâ†’IM2COL_ADM), (STRIDE_PRMâ†’STRIDE_ADM), SEND_ACK, WAIT_REQ, CYCLE_ADDRâˆ—, PRG_PRM
CP
(URAM_PRMâ†’WEIGHTS_ADM), (RES_ADD_STRIDE_PRMâ†’RES_ADD_STRIDE_ADM), RES_ADD_ADM, CYCLE_ADDRâˆ—, GEMM, PRG_PRM
ST
LINEAR_ADM, (STRIDE_PRMâ†’STRIDE_ADM), SEND_REQ, WAIT_ACK, CYCLE_ADDRâˆ—, PRG_PRM
reset, otherwise, it executes the specified rounds with the
instruction pointer jumping to the designated base address
(ICU_BA) at the end of each round. This static instruction type
is essential for multiple inference rounds in DNN applications.
â€¢ Config: It prepares PUs by setting parameters for stride
memory access patterns, IM2COL operations, and URAM
addressing. These static instructions establish the operational
context for subsequent DataMove instructions.
â€¢ DataMove: It controls ADM operations for memory trans-
fers between HBM and the PU on-chip buffers. It specifies
the current base address (CUR_BA) and transfer length (LEN),
with CUR_BA latched for successor AddrCyc instructions to
enable dynamic address management.
â€¢ AddrCyc: It implements cyclic addressing for efficient mem-
ory utilization during inference. It operates on base address
(BA), offset (AOFFS), cycles (NC), and iteration counter (IC),
with the algorithm [see Table I(b)] managing address progres-
sion and counter updates across program rounds.
â€¢ Sync: It coordinates buffer synchronization between co-
operating PUs via request-acknowledgment messages. These
instructions are exclusive to LD and ST groups, and include
PU identifiers (PID), buffer identifiers (BID), and cyclic
parameters. The algorithm [see Table I(b)] provides bypass,
reset, and cyclic increment operations.
â€¢ Compute: It controls SA and vector operations in the
PUs for the CP group, and configures activation functions,
quantization scaling factors, and residual addition.
This instruction-level approach eliminates external control
logic, enabling coordination sequences to be expressed as
instruction programs that execute within each ICU. As the PU
designs advance, the ISA can accommodate new instruction
types, allowing the expansion of computational capabilities
without requiring fundamental architectural changes.
III. MULTI-PU COORDINATION ARCHITECTURE
Building upon the ISA described in Section II-B, here we
detail how the coordination architecture enables programmable
multi-PU execution. The approach uses an ICU in each PU for
local instruction execution and coordination logic, and peer-
to-peer ISUs that manage inter-PU flow control via hardware-
based control tokens (REQ/ACK messages). Each ICU inde-
pendently manages the progression of its instruction pointers
and states, with the distributed token-based coordination elim-
inating the need for centralized control.
Fig. 2 illustrates the system architecture, showing the PUs
organization using the hardware baseline as described in
Section II-A, detailed ICU and ISU architectures, and the
measured control token latencies. During system initialization,
instruction programs are loaded offline via a cascaded configu-
ration link (CfgLink) that employs AXI4-Lite (AXIL) control
registers bridged to a daisy-chained AXI4-Stream (AXIS)
interface. This configuration link includes embedded switches
that route instruction data, first by PID and then by ICU group,
enabling the host to write instruction programs to designated
ICU groups in each PU. Once loaded, instruction programs
execute upon receiving a start signal from the host.
A. Instruction Synchronization Network
The ISUs network implements a distributed switch fabric
that routes single-beat control tokens between PUs using
AXIS channels, as shown in Fig. 2(b). Each ISU contains
an AXIS switch with register slices and dedicated master-
slave interfaces. These comprise slave interfaces for local ISU
injection (S0) and bidirectional control token forwarding (S1,
S2), along with master interfaces for local ISU delivery (M0)
and directional routing (M1, M2). The switch employs fixed
routing tables and utilizes one-transfer round-robin arbitration
to resolve contention when multiple tokens compete for the
same port.
Control tokens carry synchronization information in a com-
pact AXIS format with TDATA fields encoding the BID,
source PU ID (SRC_PID), and REQ/ACK type. In contrast,
TDEST fields specify the destination PU (DST_PID) for
routing decisions. The static routing tables enable determin-


LD instrs
REQ LUTRAM
BID, SRC_PID
REQ
ACK
To ISULink
From    ISULink
From    CfgLink
S0
M0
M1
S2
M2
S1
S0 â†’ M0, M1, M2 inject REQ/ACK
S1 â†’ M0, M1 route left-going tokens
S2 â†’ M0, M2 route right-going tokens
AXI Stream Interfaces
*tdata: <BID & SRC_PID & REQ/ACK>
*tdest: <DST_PID>
PCIe/AXIL
HBM Memory
SLR crossing registers
SLR1
SLR0
ACK LUTRAM
BID, SRC_PID
addr. <BID & SRC_PID>
SET
ISU
CfgLink
PU AXI 
channels
PU AXI channels
crossing SLR
PU1x
PCIe AXI 
channel
AXIL to AXIS bridge
To/From PU
Memory
SLR crossing registers
start/stop
Decoders
CP instrs
ST instrs
Gen. IM2COL
Addr/Len
read/clear
PU-to-PU REQ/ACK
(c)
ICU
(d)
(a)
[0]
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
(b)
PU2x
0
1
2
3
4
5
6
7
8
9
Destination (DST)
0
1
2
3
4
5
6
7
8
9
Source (SRC)
2
13
15
17
20
33
36
38
41
43
13
2
13
15
18
31
34
36
39
41
15
13
2
13
15
28
31
33
36
38
18
15
13
2
13
26
29
31
34
36
20
18
15
13
2
23
26
28
31
33
33
31
28
26
23
2
13
15
18
20
36
34
31
29
26
13
2
13
15
18
38
36
33
31
28
15
13
2
13
15
41
39
36
34
31
18
15
13
2
13
43
41
38
36
33
20
18
15
13
2
5
10
15
20
25
30
35
40
Latency (cycles)
Fig. 2. Instruction-based coordination architecture: (a) System architecture with PUs distribution. (b) ISU. (c) Control token PU-to-PU latencies. (d) ICU.
istic latency characteristics captured in the measured latency
matrix, as shown in Fig. 2(c). This matrix is derived under
nominal conditions with a single token in transit at the sys clk
rate, measuring the cycle count from token transmission to
destination delivery. The results reveal the architectural impact
of the SLR boundary crossing, which adds a 13-cycle penalty
to cross-SLR paths. In contrast, same-SLR hops incur 2 â€“ 3
cycles, and same-PU delivery bypasses the switch fabric,
resulting in a 2-cycle latency. Consequently, given that PU
execution operates in the hundreds of microseconds timescale
for most DNN models, while control tokens complete in sub-
microseconds, potential contention effects are negligible for
the intended coordination requirements.
B. Instruction Controller Unit
Each PU contains a dedicated ICU that executes instruction
programs from three independent dual-port BRAMs corre-
sponding to the ICU groups, as depicted in Fig. 2(d). The
ICU incorporates separate instruction decoders for each group,
enabling parallel execution of LD, CP, ST operations within a
PU. This execution scheme enables the decoupling of memory
accesses from computational operations, allowing overlapped
pipelining and simplifying instruction generation.
Critical to the coordination mechanism are the REQ and
ACK LUTRAMs that store the synchronization states for inter-
PU flow control. These LUTRAMs are addressed using the
BID and SRC_PID fields, enabling the ICU to track multiple
outstanding synchronization messages. The BID parameter en-
ables pipeline coordination by distinguishing between multiple
in-flight data transfers in balanced and unbalanced pipeline
configurations. Its cyclic behavior ensures buffer management
without race conditions in dependency chains that span mul-
tiple PUs, as demonstrated in Section III-C.
The REQ/ACK messages are issued via complemen-
tary instruction pairs specified in the Sync instruction type
[see Table I(a)]. The SEND_REQ/ACK instructions transmit
synchronization messages to destination PUs, whereas the
WAIT_REQ/ACK instructions monitor the related LUTRAMs
for control tokens from source PUs. When a token is re-
ceived through the ISULink interface (M0 connection from
local ISU), the ICU extracts the REQ/ACK type from the
token data and updates the corresponding LUTRAM address.
Subsequently, WAIT instructions act as barriers by polling the
LUTRAMs until synchronization is satisfied, after which the
associated entries are cleared and execution proceeds.
Outgoing synchronization messages are issued by SEND in-
structions and forwarded through a multiplexer to the ISULink
interface (S0 connection to local ISU). A small FIFO buffer at
this interface enables the ICU decoder FSMs to proceed after
issuing instructions, without blocking until the ISU network
becomes available. This maintains instruction execution flow
and avoids stalls in the local instruction pipeline.
C. Two-PU Pipeline Example
To demonstrate these coordination capabilities, we present
a two-PU pipeline example, mapping two Conv layers (trans-
formed to GEMM) to PU0 and PU1, respectively, creating a
producer-consumer dependency chain, where PU0 generates
intermediate activations consumed by PU1. Fig. 3 illustrates
the instruction execution, buffer management, and synchro-
nization across balanced and unbalanced pipeline scenarios.
The memory hierarchy employs three tensor buffer categories:
A-regions store input activations, B-regions provide interme-
diate activations storage with ping-pong buffering (BID=0,
BID=1), and C-regions store outputs. Both A and C-regions
use cyclic access patterns across n HBM regions in each,
whereas B-regions enable overlapped execution while main-
taining data dependencies.
In Fig. 3 (top part), the instruction programs are shown,
where PU0â€™s ST group uses WAIT_ACK/SEND_REQ in-
structions, while PU1â€™s LD group issues the reciprocal
WAIT_REQ/SEND_ACK handshakes. Pipeline initialization
relies on ACK bypass in PU1â€™s LD instructions at addresses
{1, 2}, pre-authorizing PU0 to use both B0, B1 buffers
by setting the appropriate ACK LUTRAM addresses before
execution. Also, CYCLE_ADDR instructions manage the cyclic
patterns: n cycles for A/C-regions versus ping-pong alternation
for B0/B1 intermediate buffers (NC=1 creates a two-region
cycle), reflecting different buffer management requirements.
In Fig. 3 (bottom part), balanced pipeline operation emerges
when both PUs achieve comparable throughput (Case-1). Fol-
lowing the ACK bypass warm-up, steady-state execution pro-
ceeds as PU1 processes round N-1 using buffer BID=X while
PU0 executes round N, writing to buffer BID=1-X, thereby


[0]: PRG_PRM(NR=0, ICU_BA=1)
[1]: IM2COL_PRM(params)
[2]: IM2COL_ADM(CUR_BA=A0, LEN_A)
[3]: CYCLE_ADDR(BA=A0, AOFFS=LEN_A, NC=n, IC=n, PRG_END=1)
[0]: PRG_PRM(NR=0, ICU_BA=1)
[1]: GEMM(params, PRG_END=1)
[0]: PRG_PRM(NR=0, ICU_BA=1)
[1]: WAIT_ACK(SRC_PID=1, BID=0, BASE_BID=0, NC=1, IC=1)
[2]: LINEAR_ADM(CUR_BA=B0, LEN_B)
[3]: CYCLE_ADDR(BA=B0, AOFFS=LEN_B, NC=1, IC=1)
[4]: SEND_REQ(DST_PID=1, BID=0, BASE_BID=0, NC=1, IC=1, PRG_END=1)
PU0 (PID=0) instructions
[0]: PRG_PRM(NR=0, ICU_BA=3)
[1]: SEND_ACK(DST_PID=0, BID=0, NC=0) //bypass
[2]: SEND_ACK(DST_PID=0, BID=1, NC=0) //bypass
[3]: WAIT_REQ(SRC_PID=0, BID=0, BASE_BID=0, NC=1, IC=1)
[4]: IM2COL_PRM(params)
[5]: IM2COL_ADM(CUR_BA=B0, LEN_A)
[6]: CYCLE_ADDR(BA=B0, AOFFS=LEN_B, NC=1, IC=1)
[7]: SEND_ACK(DST_PID=0, BID=0, BASE_BID=0, NC=1, IC=1, PRG_END=1)
[0]: PRG_PRM(NR=0, ICU_BA=1)
[1]: GEMM(params, PRG_END=1)
[0]: PRG_PRM(NR=0, ICU_BA=1)
[1]: LINEAR_ADM(CUR_BA=C0, LEN_C)
[2]: CYCLE_ADDR(BA=C0, AOFFS=LEN_C, NC=n, IC=n, PRG_END=1)
PU1 (PID=1) instructions
A0/B0
A1/B1
A2/B0
A3/B1
B0/C0
B1/C1
B0/C2
B1/C3
A0/B0
A1/B1
A2/B0
A3/B1
B0/C0
B1/C1
B0/C2
A0/B0
A1/B1
A2/B0
A3/B1
B0/C0
B1/C1
B0/C2
B1/C3
(1)
(2)
(1)
(2)
(1)
(2)
(1)
(1)
(2) (1)
(2)
(1)
(2)
(1)
(2)
(1)
(2)
(1)
(2)
(1): REQ(BID=0) 
(1): ACK(BID=0)
(2): REQ(BID=1) 
(2): ACK(BID=1)
LD Group
CP Group
ST Group
PU 
dependence
HBM
FPGA
Host
A0
A1
An
C0
C1
Cn
B0
B1
Sync 
instructions 
dependence
Case-1
Case-2
Case-3
LEN_A
LEN_C
PCIe
PU0
PU1
PU0
PU1
PU0
PU1
(1)
LEN_B
Fig. 3. Two-PU pipeline coordination example: instructions/timing overview.
eliminating memory conflicts. As for the intra-PU pipelines,
they operate with LDâ†’CPâ†’ST dependencies during warm-
up, transitioning to parallel execution as PU activation buffers
allow overlapped memory and computation operations.
On the contrary, when PU1 operates at half the throughput
of PU0 (Case-2), bottlenecks manifest as extended ACK wait
intervals for PU0. After initial bypass rounds, PU0â€™s ST
group blocks await ACK messages from PU1, filling on-
chip output buffers and triggering STâ†’CPâ†’LD back-pressure
propagation that throttles PU0 to match PU1â€™s rate. Case-3
reverses this dynamic, with PU1 awaiting extended periods for
REQ messages, which indicate data availability. In this case,
ACK messages are unnecessary, as PU0â€™s reduced throughput
prevents memory contention. Notably, instruction uniformity
is maintained across the cases, regardless of performance
characteristics.
Cyclic buffering in the A and C-regions enables concurrent
PCIe transfers with PU execution. The host writes new input
batches to available A-regions while reading the results from
C-regions as the PUs execute. Intermediate B buffers require
only two regions, since the producer-consumer access patterns
between PU0 and PU1 guarantee buffer consistency. That
scales to deeper pipelines requiring additional BID values
proportional to the pipeline depth. Complex topologies with
multiple consumers (e.g., PU0 feeding both PU1 and PU2)
increase the BID cyclic depth proportionally, when all PUs
achieve comparable throughput, while expanding REQ/ACK
instructions for synchronization. The required buffer depth for
each tensor depends on the pipeline configuration, as detailed
in Section IV-C.
Profiler
Nodes 
Execution times
Gen.
Instructions
Time
PID
PU-0
* N: layer/node, j>i, Nk: last node
PU-k
PU-n
N1
N2 N3
Ni
Nk
Nj
min. worst makespan
min. worst makespan
HBM
FPGA
Host
Conv(ReLu)
Element-wise Add
FusedConvAdd(Relu)
1
2
3
1
2
N1
N2
N3
N4
N5
ğ¹ğ‘œğ‘Ÿ ğ‘’ğ‘ğ‘â„ ğ‘‡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ (ğ‘‡):
ğ‘ƒğ‘‡= ğ‘ƒ1, â€¦ , ğ‘ƒğ‘›âˆ¶ğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘ 
ğ¶ğ‘‡= ğ¶1, â€¦ , ğ¶ğ‘šâˆ¶ğ‘ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘’ğ‘Ÿğ‘ 
ğ›¿= ğ‘šğ‘ğ‘¥
ğ‘ ğ‘âˆ’ğ‘ ğ‘
âˆ¶ğ‘âˆˆğ‘ƒğ‘‡, ğ‘âˆˆğ¶ğ‘‡
ğ›½ğ‘‡= ğ›¿+ 1
PID stages
s(0)
s(1)
s(2)
s(3)
PID color assignments
T1
T2
T2
T4
T5
T6
N2
N1
N3
N4
N5
T3
T4
RD{T1}, WR{T2}
RD{T2}, WR{T3}
RD{T4}, WR{T5}
RD{T4,T5}, WR{T6}
RD{T2,T3}, WR{T4}
Time
steady-state pipeline
Different HBM channels for Tensor 
Interval Conflicts:
1. same op. RD/WR temporal overlap
2. Cross-PU fork scenarios 
T1
T6
Ch-0
Ch-1
Ch-2
Ch-3
Ch-4
Ch-5
Ch-31
T2 T2 T3 T4 T5
(a)
(b)
(c)
(d)
(e)
(f)
# PUs, PU types, PU 
supported operations
ONNX
Quant. DNN 
Parser
Extract Node info
2
Node 
DAG
Fuse Nodes
1
Weight Transfer 
Schedule
2
Node Partition
& Mapping
1
PU
DAG
Find tensors num. 
buffers ğ›½ğ‘‡
1
Tensor
Interval
Liveness
Analysis
2
Fig. 4. DNN Compilation Framework: (a) DNN model preparation. (b) Node
fusion and parser. (c) Node profiler. (d) DNN graph partitioning to PUs
and weight transfer scheduling. (e) Tensors buffer optimization and liveness
analysis. (f) Instruction generation.
IV. DNN COMPILATION FRAMEWORK
The instruction-based coordination architecture necessitates
a compilation framework that transforms DNN models into
executable instruction programs. The proposed multi-phase
process, as illustrated in Fig. 4, from model processing through
optimization to instruction generation, is described below.
A. DNN Model Processing and Profiling
The proposed framework starts with quantized DNN models
in ONNX format [31], utilizing 8-bit quantization with power-
of-two scaling factors for compatibility with the PU architec-
ture, as shown in Fig. 4(a). Also, it incorporates system details,
i.e., available PU types and supported operations. Using the
tile-based execution model [16], the nodes are partitioned into
computational tiles matching the first SA dimension of each
mapped PU, enabling node-to-PU assignments and tile-level
weight management, as described in Section IV-B.
Node fusion adapts DNN graphs to exploit the PU hardware
capabilities, while preserving computational correctness, gen-
erating the node Directed Acyclic Graph (DAG) [Fig. 4(b1)].
As shown in its inset, Conv layers with subsequent element-
wise addition fuse into a FusedConvAdd(ReLU) node, as PUs
support residual shortcut additions in their dataflow, while
the other Conv layer remains unchanged. In addition, the
activation functions are integrated with preceding operations.
These fusion opportunities can be extended as the PU architec-
ture evolves to support additional hardware-aware optimization
patterns and new operations.


The parser extracts node information from the transformed
DAG, including weights, biases, quantization scales, dimen-
sions, and operation types [Fig. 4(b2)]. Specifically, it orga-
nizes dependency information into structured representations,
capturing producer-consumer relationships, and assigns tensor
identifiers to all data paths for the next framework phases.
To guide partitioning decisions, execution time profiling
is applied under conflict-free conditions, with weights pre-
loaded in URAMs, eliminating the memory transfer overhead
[Fig. 4(c)]. The measured execution times represent complete
node processing, from activation fetching from HBM through
computation to output storage. These provide the required
performance metrics while abstracting weight loading com-
plexities, which are handled separately in the next phase.
B. Partitioning and Scheduling
This phase addresses two challenges: assigning nodes to
PUs and scheduling weight transfers. The first utilizes dynamic
programming to partition the Node DAG topological order
into contiguous subgraphs mapped to PUs, considering PU1x
and PU2x heterogeneity and employing the profiled execution
times to minimize the maximum completion time across PUs,
as shown in Fig. 4(d1).
Weight transfer scheduling addresses the constraint that
node tiles within assigned subgraphs often require more
weight data than the available URAMs capacity on each
PU [Fig. 4(d2)]. Inspired by SMOF [32], which fragments
weights at design-time and streams chunks at runtime under
bandwidth constraints, our approach analyzes tile loading and
execution times for timing-based scheduling. Correspondingly,
the tile weights are split into chunks, with part of them
allocated offline in URAMs, while the rest of them are loaded
dynamically from HBM during execution. Dynamic chunks
from subsequent tiles are scheduled for loading during the
current tileâ€™s execution to conceal transfer latency within the
execution time.
A greedy deficit-based strategy prioritizes tiles with the
highest deficit (chunks causing execution stalls after account-
ing for overlap hiding opportunities) for offline allocation.
The method iteratively allocates chunks to the most deficit-
prone tiles, using a priority-based selection, until the capacity
constraint is satisfied. In particular, the memory capacity
constraint ensures that both statically allocated chunks and
the worst-case concurrent dynamic chunks (i.e., the maximum
simultaneous memory requirements of two adjacent tiles) do
not exceed the total URAMs capacity. Regarding the dynamic
chunks, these are evicted after their tile execution completes,
releasing storage capacity for subsequent tiles. Although the
optimization challenges in this subsection could be considered
jointly, the current implementation treats them separately.
C. Pipeline Memory Optimization
For effective pipeline execution, memory management is
required to prevent data hazards and to allocate the HBM
channels for each DNN tensor. Buffer requirement analysis, as
shown in Fig. 4(e1), determines the minimum buffers needed
for each tensor to support steady-state pipeline execution with-
out read-after-write and write-after-read dependencies, where
consumer and producer PUs must coordinate their accesses. In
Fig. 4(e1) inset, the stage-distance method computes the buffer
requirements (Î²(T)) for each tensor based on the pipeline
topology, where stages represent the execution levels in the PU
dependency graph. For each tensor, the framework has iden-
tified producer-consumer PUs and mapped them to pipeline
stages to calculate the maximum stage distance between all
producer-consumer pairs. The buffer requirement equals this
distance, plus an additional buffer, which enables pipeline
operation by allowing producers to write new data while
consumers read previously loaded data, thereby preventing
pipeline stalls. Longer dependency chains across multiple
stages require correspondingly more buffers to prevent access
conflicts and maintain synchronization.
Complementing the buffer analysis, tensor liveness analysis
identifies HBM channel-sharing opportunities among tensors
with non-overlapping access patterns, as detailed in Fig. 4(e2).
For that purpose, the pipeline steady-state execution is sim-
ulated using the node-to-PU mappings and execution times
to track the temporal intervals when tensors are accessed.
Thus, tensors with conflicting access patterns, particularly
concurrent memory operations of the same type (read-read or
write-write), require separate HBM channels to prevent band-
width bottlenecks [33]. In addition, cross-PU fork scenarios,
where multiple tensors from different PUs are fed to a single
consumer (e.g., FusedConvAddRelu consuming two tensors),
are separately handled. Hence, these tensors are assigned to
different HBM channels, ensuring channel sharing decisions
do not limit the PUs performance through memory contention.
D. Instruction Generation
The final phase utilizes the optimization results to create
executable instruction programs for all the ICUs. Using the
instruction types (see Section II-B), the generator produces
instruction sequences that embed the optimized node-to-PU
assignments and memory management decisions. Specifically,
the cyclic buffering patterns are encoded into BID parameters
within Sync instructions, while cyclic addressing manages
transitions between memory buffers for each tensor. The
DNN input/output tensors require special handling with cyclic
access patterns, coordinated with the PCIe host, following
the same principles as the A and C-regions, as described in
Section III-C. Conversely, intermediate tensors employ cyclic
buffering with BID values rotation for inter-PU coordination.
The resulting programs capture both the intra-PU execution
logic and the inter-PU coordination, enabling pipeline paral-
lelism across PUs.
V. IMPLEMENTATION AND EVALUATION
The proposed architecture was implemented in RTL on
an AMD Alveo U50 card, following the heterogeneous PU
baseline architecture presented in Section II-A (5 PU1x and
5 PU2x). Table II details the hardware resource utilization,
and as shown, the coordination mechanism introduces minimal
resource overhead compared to the baseline implementation.


TABLE II
SYSTEM ARCHITECTURE RESOURCE UTILIZATION ON ALVEO U50.
Modules
LUTs
BRAMs
URAMs
DSPs
PUs
160 K (18.4 %)
200 (14.9 %)
640 (100.0 %)
3860 (64.8 %)
ICUs, ISUs
18.2 K (2.1 %)
30 (2.2 %)
â€“
â€“
Clocks: 300 MHz (sys clk), 600 MHz (dsp clk), 150 MHz (AXIL), 450 MHz (HBM)
A. Performance Analysis
The proposed architecture enables flexible deployment
strategies from single-batch pipeline execution to multi-batch
parallel processing. Here, we evaluate our framework through
a DSE methodology and demonstrate how the coordination of
multiple PUs unlocks the hybrid parallelism opportunities that
combine pipeline and batch-level execution strategies across
diverse execution scenarios.
Resource Pool
(ğ‘› PU1X, ğ‘š PU2X)
Multi-batch enumeration
ğµğ‘ğ‘¡ğ‘â„ğ‘’ğ‘ :  ğ‘˜= 2, â€¦ , ğ‘›+ ğ‘š
unordered ğ‘ ğ‘ ğ‘’ğ‘¡ğ‘  ğ‘œğ‘“ğ‘, ğ‘âˆˆğ¶âˆ¶ğ‘†ğ‘˜
with Ïƒğ‘–=1
ğ‘˜
ğ‘ğ‘–â‰¤ğ‘›, Ïƒğ‘–=1
ğ‘˜
ğ‘ğ‘–â‰¤ğ‘š
Search space: Ú‚ğ‘˜=2
ğ‘›+ğ‘šğ‘†ğ‘˜
Pareto Analysis & 
User Requirements
(ğµğ‘ğ‘¡ğ‘â„ğ‘’ğ‘  ğ‘˜, ğ¿ğ‘šğ‘ğ‘¥, ğ‘‡ğ‘šğ‘–ğ‘›)
Selected 
configuration
ğ¿ğ‘ ğ‘¦ğ‘ 
ğ‘‡ğ‘ ğ‘¦ğ‘ 
ğ‘‡ğ‘‚ğ‘ƒğ‘†ğ‘ ğ‘¦ğ‘ 
Single-batch enumeration
ğ¶=
ğ‘, ğ‘ğ‘â‰¤ğ‘›, ğ‘â‰¤ğ‘š, ğ‘+ ğ‘â‰¥1
ğ¿ğ‘, ğ‘, ğ‘‡ğ‘, ğ‘, ğ‘‡ğ‘‚ğ‘ƒğ‘†(ğ‘, ğ‘)
Framework
Section IV
Fig. 5. Three-step DSE methodology.
The DSE methodology, illustrated in Fig. 5, provides a
systematic approach to performance analysis of DNN models
on our architecture, based on the application requirements.
In particular, it is a three-step process, which is initiated by
enumerating all feasible single-batch configurations, where
each configuration (a,b) specifies the assignment of PU1x and
PU2x units respectively, for DNN pipeline execution. In our
setup, this enumeration yields 35 distinct single-batch config-
urations, each generated via the DNN compilation framework
(Section IV), and their resulting performance characteristics
are cached. The second step constructs multi-batch schedules
by composing all the unordered combinations of single-batch
configurations within the constraint of available PU resources.
This composition enables hybrid parallelism, where each batch
is processed by a subset of PUs exhibiting internal pipeline
parallelism. In essence, different batches are processed by
separate PU subsets that exhibit batch-level parallelism across
them. Each multi-batch schedule is characterized by its aggre-
gated throughput, system latency (determined by the slowest
configuration), and the cumulative DSP48E2 Tera Operations
per Second (TOPS) across all assigned PUs. The final step
applies Pareto analysis to identify configurations optimized
for different metrics while accommodating application-specific
constraints, such as maximum acceptable latency, minimum
required throughput, and target batch processing requirements.
We selected ResNet-50 [34] as our benchmark model, due
to its diverse layer composition, which is recognized as a
standard benchmark for assessing acceleratorsâ€™ performance
by effectively representing various operational characteristics
of DNN layers. In Fig. 6(a), the single-batch configurations
results are presented, showing the throughput-latency trade-
offs, where the throughput is measured in Frames per Second
(FPS). The resulting design space demonstrates diversity in
14
16
18
20
22
24
Latency (ms)
0
100
200
300
400
500
600
Throughput (FPS)
0/1
0/2
0/3
0/4
0/5
1/0
1/1
1/2
1/3
1/4
1/5
2/0
2/1
2/2
2/3
2/4
2/5
3/0
3/1
3/2
3/3
3/4
3/5
4/0
4/1
4/2
4/3
4/4
4/5
5/0
5/1
5/2
5/3
5/4
5/5
Format: PU1X/PU2X units
Same PU2X trend
Throughput-Latency Pareto Frontier
0.3
0.6
0.9
1.2
1.5
1.8
2.2
2.5
2.8
3.1
3.4
3.7
4.0
4.3
4.6
PU(s) Resources (TOPS)
91
93
95
97
99
Pipeline Balance Efciency (PBE) (%)
(a)
(b)
14
16
18
20
22
24
Latency (ms)
0
100
200
300
400
500
600
Throughput (FPS)
Multi-Batch Design Space
Single-Batch Design Space
Throughput-Latency Pareto Frontier
A
B
C
A. (B=1): T=528.4 FPS, L=17.3 ms, PBE=90.9%
B. (B=5): T=583.0 FPS, L=17.1 ms, PBE=99.0%
C. (B=10): T=584.9 FPS, L=25.4 ms
1
2
3
4
5
Number of Batches (B)
Fig. 6. Throughput-latency trade-offs: (a) Single-batch. (b) Multi-batch.
performance characteristics, with throughput ranging from
configurations utilizing minimal PU resources (marker-coded
according to their TOPS utilization), suitable for resource-
constrained scenarios, to maximum resources deployment
achieving peak performance. The pipeline balance efficiency
(PBE, color-coded), calculated consistently with the balance
factor equations used in [24], reflects the effectiveness of the
node-to-PU assignment algorithm. Single-PU configurations
naturally achieve optimal PBE since pipeline coordination
is not required. In contrast, multi-PU configurations exhibit
varying levels of this metric, depending on the distribution of
DNN nodes across PUs and the heterogeneity of the PUs. The
throughput-latency Pareto frontier is also shown, establishing
the optimal performance boundary and guiding configuration
selection based on application requirements.
Multi-batch configuration analysis, shown in Fig. 6(b),
extends the design space to concurrent processing of multiple
input streams/batches. In conjunction with the Pareto frontier
(applied with a small tolerance), the results reveal how hy-
brid parallelism strategies can outperform both pure pipeline
and pure batch-level approaches, validating the flexibility
of our architecture. Also, we highlight three design points
that illustrate our architectureâ€™s versatility across different
deployment scenarios. Design point A (DP-A) represents the
highest single-batch throughput achieved by utilizing pipeline
parallelism across all PUs, resulting in 90.9% PBE. In con-
trast, DP-B achieves maximum system throughput through
hybrid parallelism across five concurrent batches, resulting in
a 99% system-level PBE, which demonstrates a key insight.
In particular, the flexibility to assign different single-batch
configurations to individual pipelines enables better matching
of the PU capabilities to DNN layers computational require-
ments. Despite DP-C using twice as many batches as DP-
B, it achieves the same throughput performance, representing
maximum batch-level parallelism with one PU per batch, and


TABLE III
PERFORMANCE COMPARISON OF FPGA-BASED ACCELERATOR DESIGNS ON RESNET-50 WITH INT8 ARITHMETIC.
ARCHITECTURE
FPGA
Device
Year
Freq.
(MHz)
Used DSPs
(Util. %)
Batch
Size
Latency
(ms)
FPS
GOPS
CE (%)
GOPS/DSP
FPS/TOPS
Power
(W)
FPS/W
GOPS/W
Peak
TOPS
DPU [21], [35]
XCU501
2021
600
3,406 (57.2 %)
6
â€“
572.7
4,409.8
59.8
1.29
77.7
â€“
â€“
â€“
7.373
ShortcutFuse3 [36]
XCKU1151
2022
200
2,240 (40.6 %)
1
11.69
85.5
1,006.0
56.1
0.45
47.7
â€“
â€“
â€“
1.792
Full-Stack [37]
Arria 10 GX11502
2022
200
1,473 (97.0 %)
1
5.07
197.3
1,519.0
92.7
1.03
120.4
19.1
10.3
79.5
1.638
Rotated [15]
XCKU15P1
2022
500
1,280 (65.0 %)
1
4.13
242.1
1,874.0
73.2
1.46
94.6
â€“
â€“
â€“
2.560
xDNN [6]
XCU2501
2022
800
7,548 (61.4 %)
4
3.12
1,281.0
9,863.7
50.2
1.31
65.2
128.9
9.9
76.5
19.661
Unified Acc. [17]
XCVU37P1
2023
200
1,024 (11.3 %)
1
13.12
76.2
590.0
72.0
0.58
93.0
17.6
4.3
33.5
0.819
Amoeba [19]
Arria 10 SoC2
2024
200
522 (30.0 %)
1
28.03
35.7
286.2
69.9
0.55
87.2
8.2
4.4
35.0
0.410
PipeFuser [7]
XCU2001
2024
220
3,560 (52.0 %)
1
â€“
â€“
2,106.0
â€“
0.59
â€“
â€“
â€“
â€“
â€“
DCP [20]
Stratix 10 GX6502
2025
200
1,024 (88.9 %)
1
9.60
103.9
800.0
97.7
0.78
126.9
9.0
11.5
88.9
0.819
DP-C
XCU501
2025
600
3,860 (64.8 %)
10
25.40
584.9
4,515.4
98.0
1.17
126.9
46.0
12.7
98.2
4.608
DP-B
XCU501
2025
600
3,860 (64.8 %)
5
17.10
583.0
4,500.7
97.7
1.16
126.5
46.0
12.7
97.8
4.608
DP-A
XCU501
2025
600
3,860 (64.8 %)
1
17.30
528.4
4,079.2
88.5
1.06
114.7
46.0
11.5
88.7
4.608
The gray-colored values are inferred to the best of our understanding from the respective works; Peak TOPS are the DSP TOPS, where for xDNN [6], the value uses only the â€big
blockâ€ SAs (6,144 DSPs); FPS/TOPS uses the Peak TOPS. 1 AMD/Xilinx devices. 2 Intel devices. 3 Input Size: 256Ã—256, while the other designs 224Ã—224.
matching the hardware baseline configuration [16]. Compared
to DP-A, it provides a marginal 1.1Ã— throughput improvement
with 1.5Ã— latency penalty, demonstrating the significance of
DP-Bâ€™s hybrid parallelism strategy, which balances throughput
gains with latency and batch constraints.
Table III presents a comprehensive comparison of our archi-
tecture against a broad range of prior FPGA accelerators that
optimize DNN inference from different aspects, on ResNet-50
with INT8 arithmetic, encompassing designs for both AMD
and Intel platforms. Although different accelerators utilize
diverse FPGA devices and hardware resources, we employ
normalized metrics to enable the fairest possible comparison.
The subsequent analysis focuses on the DP-B configuration, as
it exhibits better performance compared to DP-A and DP-C.
DP-B demonstrates competitive performance across multi-
ple metrics, achieving 1.0Ã— â€“ 2.7Ã— improvement in FPS/TOPS
compared
to
prior
works,
reflecting
each
architectureâ€™s
throughput efficiency in utilizing the DSP resources for the
selected target. Energy efficiency analysis reveals that DP-
B achieves gains of 1.0Ã— â€“ 2.9Ã— over prior designs. Notably,
the reported power consumption of 46 W for both DP-A and
DP-B configurations is derived from the hardware baseline
average power reported in [16], assuming comparable power
consumption due to the same underlying architectural foun-
dation. The performance density in GOPS/DSP ranges from
0.8Ã— to 2.6Ã— across comparisons. In addition, the compute
efficiency (CE), calculated as the ratio of measured GOPS to
the available DSP GOPS in each architecture, consistent with
the calculations in the compared works, demonstrates improve-
ments of 1.0Ã— â€“ 1.9Ã—, with DP-B reaching 97.7%. However,
system latency presents a trade-off, with DP-B exhibiting
0.6Ã— â€“ 5.5Ã— performance relative to other designs, i.e., up
to 5.5Ã— slower than the fastest implementation, since our
architecture is not optimized for low-latency inference. The
latter reflects our architectural focus on inter-PU coordination
flexibility over intra-layer parallelism optimization, resulting
in higher latencies compared to low-latency designs.
B. Discussion
Although our evaluation demonstrated performance im-
provement over prior works and runtime adaptability, limi-
tation areas can be identified. First, the current approach as-
signs topologically contiguous DNN layers to PUs, potentially
achieving a lower PBE compared to methods that explore non-
contiguous subgraph mappings [23]. Also, the layer-by-layer
schedule could benefit from tile-triggered pipelines [12], [27],
where each layer is split into spatial tiles, and the downstream
PU starts as soon as the upstream PU has enough activation
data. This fine-grained producerâ€“consumer overlap allows ad-
jacent layers to be launched earlier, thereby reducing latency.
Our frameworkâ€™s multiple phases and the ISA design are
orthogonal to these limitations and could enable future integra-
tion of such enhancements without fundamental architectural
changes. Also, the system architecture can support architec-
tural evolution through PU substitution, where specialized
PUs targeting specific layers can replace current PUs while
maintaining identical HBM interfaces and coordination mecha-
nisms. In this context, our framework could also accommodate
heterogeneous PU configurations, loaded via FPGA reconfig-
uration, according to specific DNN model requirements.
While ResNet-50 provided detailed insights, the results
cannot be generalized to most DNN models, which may ex-
hibit different layer characteristics. The 10-PU design reflects
current hardware capabilities rather than inherent coordination
constraints. Therefore, our instruction-based coordination can
be a viable approach for FPGA acceleration architectures,
since programmable inter-PU synchronization enables deploy-
ment flexibility while maintaining competitive performance
across diverse execution scenarios.
VI. CONCLUSION
This work introduced an instruction-based coordination
architecture that enables runtime reconfiguration between
pipeline and hybrid parallelism strategies on the FPGA ac-
celerator, without requiring hardware reprogramming. Our ap-
proach combines an ICU per PU with peer-to-peer ISUs, sup-
porting dynamic switching via instruction updates. To enable
this coordination, we developed a compilation framework that
transforms DNN models into optimized instruction sequences.
Experimental validation on ResNet-50 demonstrates CE of
88.5% â€“ 98.0% with higher throughput efficiency (1.0Ã— â€“ 2.7Ã—
in FPS/TOPS) compared to prior works, while providing de-
ployment flexibility that enables the same architecture to adapt
from single-batch pipelining to hybrid parallelism processing.


ACKNOWLEDGMENT
This work has been performed in the framework of the EU
project â€NeuroSoC: A multiprocessor system on chip with in-
memory neural processing unitâ€, HORIZON-101070634 [38].
REFERENCES
[1] A. Nechi, L. Groth, S. Mulhem, F. Merchant, R. Buchty, and
M. Berekovic, â€œFPGA-based Deep Learning Inference Accelerators:
Where Are We Standing?â€ ACM Trans. Reconfigurable Technol. Syst.,
vol. 16, no. 4, pp. 1â€“32, Oct. 2023.
[2] S. Zeng, G. Dai, H. Sun, J. Liu, S. Li, G. Ge, K. Zhong, K. Guo,
Y. Wang, and H. Yang, â€œA Unified FPGA Virtualization Framework
for General-Purpose Deep Neural Networks in the Cloud,â€ ACM Trans.
Reconfigurable Technol. Syst., vol. 15, no. 3, pp. 1â€“31, Dec. 2021.
[3] Y. Ma, Y. Cao, S. Vrudhula, and J.-s. Seo, â€œAutomatic Compilation
of Diverse CNNs Onto High-Performance FPGA Accelerators,â€ IEEE
Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 39, no. 2, pp.
424â€“437, Feb. 2020.
[4] X. Zhang, J. Wang, C. Zhu, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen,
â€œDNNBuilder: an Automated Tool for Building High-Performance DNN
Hardware Accelerators for FPGAs,â€ in Proc. IEEE/ACM Int. Conf.
Comput.-Aided Des. (ICCAD), Nov. 2018, pp. 1â€“8.
[5] H. Ye, X. Zhang, Z. Huang, G. Chen, and D. Chen, â€œHybridDNN: A
Framework for High-Performance Hybrid DNN Accelerator Design and
Implementation,â€ in Proc. 57th ACM/IEEE Des. Automat. Conf. (DAC),
Jul. 2020, pp. 1â€“6.
[6] P. Dâ€™Alberto, V. Wu, A. Ng, R. Nimaiyar, E. Delaye, and A. Sirasao,
â€œxDNN: Inference for Deep Convolutional Neural Networks,â€ ACM
Trans. Reconfigurable Technol. Syst., vol. 15, no. 2, pp. 1â€“29, Jan. 2022.
[7] X. Zhou, S. Li, H. Lu, and K. Wang, â€œPipeFuser: Building Flexible
Pipeline Architecture for DNN Accelerators via Layer Fusion,â€ in Proc.
29th Asia and South Pacific Des. Automat. Conf. (ASP-DAC), Jan. 2024,
pp. 921â€“926.
[8] X. Zhang, H. Ye, J. Wang, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen,
â€œDNNExplorer: A Framework for Modeling and Exploring a Novel
Paradigm of FPGA-based DNN Accelerator,â€ in Proc. IEEE/ACM Int.
Conf. Comput.-Aided Des. (ICCAD), Dec. 2020, pp. 1â€“9.
[9] S. Basalama, A. Sohrabizadeh, J. Wang, L. Guo, and J. Cong, â€œFlex-
CNN: An End-to-end Framework for Composing CNN Accelerators on
FPGA,â€ ACM Trans. Reconfigurable Technol. Syst., vol. 16, no. 2, pp.
1â€“32, Mar. 2023.
[10] Y. Xing et al., â€œDNNVM: End-to-End Compiler Leveraging Heteroge-
neous Optimizations on FPGA-Based CNN Accelerators,â€ IEEE Trans.
Comput.-Aided Design Integr. Circuits Syst., vol. 39, no. 10, pp. 2668â€“
2681, Oct. 2020.
[11] J. Li, W. Wang, and W.-J. Li, â€œHardware Computation Graph for DNN
Accelerator Design Automation Without Inter-PU Templates,â€ IEEE
Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 44, no. 11, pp.
4276â€“4289, Nov. 2025.
[12] X. Cai, Y. Wang, X. Ma, Y. Han, and L. Zhang, â€œDeepBurning-SEG:
Generating DNN Accelerators of Segment-Grained Pipeline Architec-
ture,â€ in Proc. 55th IEEE/ACM Int. Symp. Microarchit. (MICRO), Oct.
2022, pp. 1396â€“1413.
[13] AMD, Inc., Santa Clara, CA, USA, â€œUltraScale Architecture DSP Slice
(UG579),â€ 2021. [Online]. Available: https://docs.amd.com/v/u/en-US/
ug579-ultrascale-dsp
[14] R. Shi et al., â€œFTDL: A Tailored FPGA-Overlay for Deep Learning with
High Scalability,â€ in Proc. 57th ACM/IEEE Des. Automat. Conf. (DAC),
Jul. 2020, pp. 1â€“6.
[15] X. Fan, G. Xie, Z. Huang, W. Cao, and L. Wang, â€œAcceleration of
Rotated Object Detection on FPGA,â€ IEEE Trans. Circuits Syst. II,
vol. 69, no. 4, pp. 2296â€“2300, Apr. 2022.
[16] A. Petropoulos and T. Antonakopoulos, â€œA Scalable FPGA Architecture
With Adaptive Memory Utilization for GEMM-Based Operations,â€ IEEE
Trans. VLSI Syst., vol. 33, no. 8, pp. 2334â€“2338, Aug. 2025.
[17] T. Li, F. Zhang, X. Fan, J. Shen, W. Guo, and W. Cao, â€œUnified Accel-
erator for Attention and Convolution in Inference Based on FPGA,â€ in
Proc. IEEE Int. Symp. Circuits Syst. (ISCAS), May 2023, pp. 1â€“5.
[18] B. Zhang, R. Kannan, C. Busart, and V. K. Prasanna, â€œVisionAGILE:
A Versatile Domain-Specific Accelerator for Computer Vision Tasks,â€
IEEE Trans. Parallel Distrib. Syst., vol. 35, no. 12, pp. 2405â€“2422, Dec.
2024.
[19] X. Wu, M. Wang, J. Lin, and Z. Wang, â€œAmoeba: An Efficient and
Flexible FPGA-Based Accelerator for Arbitrary-Kernel CNNs,â€ IEEE
Trans. VLSI Syst., vol. 32, no. 6, pp. 1086â€“1099, Jun. 2024.
[20] K. Dai, Z. Xie, and S. Liu, â€œDCP-CNN: Efficient Acceleration of CNNs
With Dynamic Computing Parallelism on FPGA,â€ IEEE Trans. Comput.-
Aided Design Integr. Circuits Syst., vol. 44, no. 2, pp. 540â€“553, Feb.
2025.
[21] AMD, Inc., Santa Clara, CA, USA, â€œDPUCAHX8H Performance
(PG367),â€ 2024. [Online]. Available: https://docs.amd.com/r/en-US/
pg367-dpucahx8h/Performance
[22] Q. Xiao, Y. Liang, L. Lu, S. Yan, and Y.-W. Tai, â€œExploring hetero-
geneous algorithms for accelerating deep convolutional neural networks
on FPGAs,â€ in Proc. 54th ACM/IEEE Des. Automat. Conf. (DAC), Jun.
2017, pp. 1â€“6.
[23] Y. Shen, M. Ferdman, and P. Milder, â€œMaximizing CNN Accelerator
Efficiency Through Resource Partitioning,â€ in Proc. ACM/IEEE 44th
Annu. Int. Symp. Comput. Archit. (ISCA), Jun. 2017, pp. 535â€“547.
[24] E. Wu, X. Zhang, D. Berman, I. Cho, and J. Thendean, â€œCompute-
Efficient Neural-Network Acceleration,â€ in Proc. ACM/SIGDA Int.
Symp. Field-Program. Gate Arrays (FPGA), Feb. 2019, pp. 191â€“200.
[25] L. C. Chan, G. Malik, and N. Kapre, â€œPartitioning FPGA-Optimized
Systolic Arrays for Fun and Profit,â€ in Proc. Int. Conf. Field-Program.
Technol. (ICFPT), Dec. 2019, pp. 144â€“152.
[26] M. Doumet, M. Stan, M. Hall, and V. Betz, â€œH2PIPE: High Throughput
CNN Inference on FPGAs with High-Bandwidth Memory,â€ in Proc.
34th Int. Conf. Field Program. Log. Appl. (FPL), Sep. 2024, pp. 69â€“77.
[27] X. Wei, Y. Liang, X. Li, C. H. Yu, P. Zhang, and J. Cong, â€œTGPA:
Tile-Grained Pipeline Architecture for Low Latency CNN Inference,â€ in
Proc. IEEE/ACM Int. Conf. Comput.-Aided Des. (ICCAD), Nov. 2018,
pp. 1â€“8.
[28] A. Samajdar, T. Garg, T. Krishna, and N. Kapre, â€œScaling the Cascades:
Interconnect-Aware FPGA Implementation of Machine Learning Prob-
lems,â€ in Proc. 29th Int. Conf. Field-Program. Log. Appl. (FPL), Sep.
2019, pp. 342â€“349.
[29] M. S. Abdelfattah et al., â€œDLA: Compiler and FPGA Overlay for
Neural Network Inference Acceleration,â€ in Proc. 28th Int. Conf. Field-
Program. Log. Appl. (FPL), Aug. 2018, pp. 411â€“418.
[30] Y. Yu, C. Wu, T. Zhao, K. Wang, and L. He, â€œOPU: An FPGA-Based
Overlay Processor for Convolutional Neural Networks,â€ IEEE Trans.
VLSI Syst., vol. 28, no. 1, pp. 35â€“47, Jan. 2020.
[31] ONNX Community, â€œOpen Neural Network Exchange (ONNX),â€ 2024.
[Online]. Available: https://github.com/onnx/onnx/releases/tag/v1.16.0
[32] P. Toupas, Z. Yu, C.-S. Bouganis, and D. Tzovaras, â€œSMOF: Streaming
Modern CNNs on FPGAs with Smart Off-Chip Eviction,â€ in Proc. IEEE
32nd Annu. Int. Symp. Field-Program. Custom Comput. Mach. (FCCM),
May 2024, pp. 185â€“196.
[33] H. Huang et al., â€œShuhai: A Tool for Benchmarking High Bandwidth
Memory on FPGAs,â€ IEEE Trans. Comput., vol. 71, no. 5, pp. 1133â€“
1144, May 2022.
[34] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep Residual Learning for
Image Recognition,â€ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
(CVPR), Jun. 2016, pp. 770â€“778.
[35] AMD, Inc., Santa Clara, CA, USA, â€œVitis AI (UG1354),â€ 2021. [Online].
Available:
https://docs.amd.com/r/1.4.1-English/ug1354-xilinx-ai-sdk/
Alveo-U50/U50LV-Data-Accelerator-Card
[36] D. T. Nguyen, H. Je, T. N. Nguyen, S. Ryu, K. Lee, and H.-J. Lee,
â€œShortcutFusion: From Tensorflow to FPGA-Based Accelerator With
a Reuse-Aware Memory Allocation for Shortcut Data,â€ IEEE Trans.
Circuits Syst. I, vol. 69, no. 6, pp. 2477â€“2489, Jun. 2022.
[37] S. Liu, H. Fan, M. Ferianc, X. Niu, H. Shi, and W. Luk, â€œToward Full-
Stack Acceleration of Deep Convolutional Neural Networks on FPGAs,â€
IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 8, pp. 3974â€“3987,
Aug. 2022.
[38] NeuroSoC. [Online]. Available: https://neurosoc.eu/
