1
Joint Semantic-Channel Coding and Modulation for
Token Communications
Jingkai Ying, Graduate Student Member, IEEE, Zhijin Qin, Senior Member, IEEE, Yulong Feng,
Liejun Wang, and Xiaoming Tao, Senior Member, IEEE
Abstractâ€”In recent years, the Transformer architecture has
achieved outstanding performance across a wide range of tasks
and modalities. Token is the unified input and output repre-
sentation in Transformer-based models, which has become a
fundamental information unit. In this work, we consider the
problem of token communication, studying how to transmit
tokens efficiently and reliably. Point cloud, a prevailing three-
dimensional format which exhibits a more complex spatial
structure compared to image or video, is chosen to be the
information source. We utilize the set abstraction method to
obtain point tokens. Subsequently, to get a more informative and
transmission-friendly representation based on tokens, we propose
a joint semantic-channel and modulation (JSCCM) scheme for
the token encoder, mapping point tokens to standard digital
constellation points (modulated tokens). Specifically, the JSCCM
consists of two parallel Point Transformer-based encoders and a
differential modulator which combines the Gumel-softmax and
soft quantization methods. Besides, the rate allocator and channel
adapter are developed, facilitating adaptive generation of high-
quality modulated tokens conditioned on both semantic informa-
tion and channel conditions. Extensive simulations demonstrate
that the proposed method outperforms both joint semantic-
channel coding and traditional separate coding, achieving over
1dB gain in reconstruction and more than 6Ã— compression ratio
in modulated symbols.
Index Termsâ€”Token communication, joint semantic-channel
coding, digital modulation, adaptive transmission, point cloud.
I. INTRODUCTION
Semantic communication, which aims to transmit the mean-
ing of information, has recently drawn increasing attention
[2], [3]. Leveraging powerful deep learning (DL) models, se-
mantic communication achieves impressive compression ratio
and robust performance under extreme channel conditions.
However, the absence of unified semantic representations and
significant architectural gaps across modalities remain major
obstacles to its advancement. Recent revolutionary progress in
the field of artificial intelligence (AI) has made it possible to
address these challenges. The AI community has witnessed
that Transformers are gaining prominence as the models of
Parts of this paper have been presented at the IEEE International Conference
on Communications, Montreal, Canada, June 2025 [1].
Jingkai Ying, Zhijin Qin, and Xiaoming Tao are with the Department
of Electronic Engineering, Tsinghua University, Beijing 100084, China,
and with the State Key Laboratory of Space Network and Communica-
tions, Beijing, 100084, China (e-mail: yjk23@mails.tsinghua.edu.cn; qinzhi-
jin@tsinghua.edu.cn; taoxm@tsinghua.edu.cn).
Yulong Feng is with the State Key Laboratory of Mobile Network and
Mobile Multimedia Technology, Shenzhen 518055, China, and with the ZTE
Corporation, Shenzhen, 518055, China (e-mail: feng.yulong1@zte.com.cn).
Liejun Wang is with the School of Computer Science and Technology,
Xinjiang University, Â¨UrÂ¨umqi 830046, China (e-mail: wljxju@xju.edu.cn).
Text
Text
Tokenizer
Audio
Image
/Video
Audio
Tokenizer
Visual
Tokenizer
Point
Tokenizer
Point
Cloud
â€¦
â€¦
Tokens
Wireless
Channel
Token
Encoder
Token
Decoder
Task-oriented
De-tokenizer
â€¦
Reconstruction
Generation
Understanding
Translation
Fig. 1: Schematic of token communications [9].
choice across diverse modalities. After the initial success in
natural language processing (NLP) [4], Transformers, with
their exceptional ability to capture complex relationships, were
further applied to other domains such as audio [5], vision
[6], three-dimensional (3D) data [7], and the multimodal
[8] community, where they have also achieved remarkable
success. Moreover, the parallel computation efficiency and un-
precedented scalability of Transformers led to the emergence
of large language models (LLMs) and multimodal large lan-
guage models (MLLMs), profoundly shaping the development
paradigm of AI and revolutionizing the way information is
processed. These exciting advances provide a new view on
semantic communication system design, inspiring us to focus
on token transmission based on the Transformers architecture.
Token, as the unified representation for the input and output
of Transformers, is emerging as a new fundamental unit
of information. Data from various modalities is converted
into tokens for processing [10]â€“[12]. Token/s and Token/J
are regarded as key performance metrics for inference speed
and energy efficiency, respectively [13]. Moreover, Google
introduced the Agent2Agent protocol, further demonstrating
the importance of tokens as fundamental units of informa-
tion [14]. Currently, extensive efforts are being devoted to
token processing to unleash intelligent capacity within the
AI community. As important, ensuring efficient and reliable
token transmission is also required to establish Integrated
AI and Communication [15]. A conceptual diagram of token
communication is shown in Fig. 1. In this framework, source
information from different modalities is converted into tokens
through tokenizers. The token encoder, based on the Trans-
formers architecture, refines and compresses data based on
tokens. The token decoder is used to recover tokens and pass
them into the de-tokenizers designed according to tasks at the
arXiv:2511.15699v1  [eess.SP]  19 Nov 2025


2
receiver.
Recently, there have been some preliminary explorations
of token communication, focusing on image transmission
[9], [16]. These pioneering works employ advanced tokeniz-
ers to transform images into the token domain and utilize
Transformer-based models to predict tokens at the decoder
side, which could achieve remarkable improvements in both
bandwidth efficiency and semantic fidelity. Furthermore, the
utilization of cross-modal information [9] and the exploitation
of semantic orthogonality [16] reveal new optimization spaces
within token communications.
Rather than focusing on image transmission, we apply token
communication to point cloud transmission1. Point cloud is an
important representation of the 3D world, which represents a
3D object through a set of (x, y, z) coordinates, referred to
as geometry. Nowadays, point clouds have found increasing
applications in fields such as immersive media, autonomous
driving, and robotics [17]. However, the transmission of point
clouds results in a significant increase in data volume, posing
substantial challenges to existing communication systems.
Dense point cloud sequences can reach a data rate of Gbps
level. Therefore, specialized research on point cloud transmis-
sion is of paramount importance.
Specifically, we develop a token communication system
for point clouds in this paper. To obtain more informative
and robust token representations instead of directly using the
bitwise form of indices, we design a joint semantic-channel
coding and modulation (JSCCM) scheme for the token encoder
to generate modulated tokens (constellation points). The joint
semantic-channel coding (JSCC) encoder fully exploited the
capacity of the Point Transformer [18]. And the modulator
combines the advantage of probabilistic sampling-based meth-
ods [19], where the JSCC outputs are interpretable, with the
benefit of distance-weighted soft quantization [20], which can
explore more available modulation locations. Based on the
proposed JSCCM, a rate allocator and a channel adapter are
introduced to generate modulated tokens adaptively according
to the semantics of point tokens and channel conditions. By
integrating the rate allocator and channel adapter into the
JSCCM framework, the developed system is no longer fixed-
rate and can achieve high-quality transmission under varying
channel conditions using a single model.
The main contributions of this paper can be summarized as
follows:
â€¢ An end-to-end token communication system for point
cloud geometry transmission is developed, which trans-
mits informative and robust modulated tokens over chan-
nels. To this end, the JSCCM scheme for the token
encoder is devised, which employs two parallel Point
Transformers and a differential modulator to map point
tokens onto a finite set of digital constellation points.
â€¢ Recognizing the defect of fix-rate transmission, which
assigns the same number of constellation points to point
clouds with different semantics, the rate allocator is
1Although our model is designed for point cloud, the paradigm of token
representing, processing, and transmission can be shared across other modal-
ities. The proposed framework becomes applicable to other modalities when
equipped with corresponding tokenizers and Transformer models.
designed. Herein, a masking strategy based on differential
cutoff position selection is proposed.
â€¢ To enable the proposed model to adapt to varying channel
environments, the channel adapter is introduced. The
JSCC outputs representing probabilities of constellation
points and channel conditions are concatenated to gener-
ate refined JSCC outputs.
The remainder of this paper is structured as follows. Section
II reviews the related work. Section III introduces the general
architecture of the proposed token communication system,
identifying the channel models considered and performance
metrics considered in this system. The implementation meth-
ods of JSCCM, rate allocator, and channel adapter are detailed
in Section IV. Simulation results are provided in Section V.
Section VI concludes this paper and provides an outlook for
future work.
Notations: For a vector x, âˆ¥xâˆ¥denotes its Euclidean norm.
For a set X, |X| denotes the number of elements in X. For
a complex number z, |z| denotes its norm and zâˆ—denotes its
complex conjugate. CN(Âµ, Î£) represents the complex Gaus-
sian distribution with mean vector Âµ and covariance matrix
Î£. E[ Â· ] represents the expectation operation. âŠ™represents
the Hadamard product. ( Â· )T and ( Â· )H represent transpose
and conjugate transpose operations respectively. The spaces of
m Ã— n real and complex matrices are expressed as RmÃ—n and
CmÃ—n respectively.
II. RELATED WORK
This section reviews the related work on tokenizer design,
point cloud semantic communications, and modulation in
semantic communications.
A. Tokenizer Design for various modalities
The concept of tokenizer first emerged in the field of NLP,
where it was used to segment text based on spaces, frequency
statistics, or other methods. With the widespread adoption
of Transformers across various modalities, the concept of
the tokenizer has been extended. For instance, dividing an
image into patches followed by a linear projection [6] is
regarded as a form of tokenization. Besides, researchers utilize
encoderâ€“decoder models for reconstruction and take outputs
of encoders as tokens for downstream tasks [10]â€“[12]. Essen-
tially, a tokenizer aims to produce vector-form tokens that are
suitable for further processing by DL models. Depending on
whether the outputted tokens are finite or not, tokenizers can be
categorized as discrete tokenizers [4], [10], [11] or continuous
tokenizers [6], [21].
B. Point Cloud Semantic Communications
Currently, motivated by the success of semantic communi-
cation in other modalities [22]â€“[24], some researchers have ex-
plored DL-enabled semantic communication methods for point
cloud transmission systems. In [25], Han et al. utilized Point-
BERT [10] as the backbone of the semantic encoder for point
cloud classification. Moreover, inspired by the transformer and
upsampling-based point cloud compression methods [18], Bian


3
et al. constructed a Point Transformer [26] based semantic
communication system [27]. To extract semantic features at
different levels for better reconstruction, in [28], Xie et al.
employed convolutional networks to obtain image features
from projections, while also utilizing PointNet++ [29] to
extract features from point patches.
C. Modulation in Semantic Communications
While the aforementioned researches have preliminarily
demonstrated the remarkable performance of semantic com-
munication systems for point cloud transmission, they pri-
marily focused on the design of JSCC structure. It remains
challenging to apply these designs to existing digital com-
munication systems. Because current DL-based JSCC utilized
in semantic communication systems generates floating-point
numbers as outputs. In most settings, every two floating-
point numbers are paired to form a constellation point, repre-
senting the in-phase and quadrature components, respectively
[25], [27]. This approach is hard to implement with existing
hardware and is incompatible with existing communication
protocols.
To tackle this challenge, it is necessary to map the out-
puts of JSCC to a finite set of channel symbols, enabling
semantic communication systems to be compatible with digital
communication systems. Methods of quantizing JSCC outputs
into bits have been studied in [30], [31], but these methods
encounter difficulties in finding an optimal mapping from
information source to channel symbols after the introduction of
digital modulation due to its non-differentiable nature. Some
attempts have been made to find the optimal mapping from
JSCC outputs to finite channel symbols in image transmission.
In [20], to solve the non-differentiable problem, the constel-
lation points were generated by weighting standard digital
constellation points in backpropagation, with the weighting
coefficients obtained through a softmax operation on the
distances between the JSCC outputs and the standard digital
constellation points. And a more intuitive method based on
straight-through estimator (STE) was used in [32]. Further-
more, by replacing quantization with adding uniform noise in
training, a digital semantic communication system for image
transmission was developed [33]. In [19], Bo et al. adopted a
reparameterization method, Gumbel-Softmax [34], to generate
constellation points from a distribution outputted by JSCC.
III. FRAMEWORK OF THE TOKEN COMMUNICATION
SYSTEM FOR POINT CLOUDS
In this section, we provide an overview of the proposed
token communication system for point clouds. First, the inputs,
outputs, functions of each system model, and their connection
relationship are introduced. Subsequently, the evaluation met-
rics are presented.
A. Formulation of System Models
The overall framework of the proposed point cloud ge-
ometry transmission system is illustrated in Fig. 2. At the
transmitter, a point tokenizer converts point patches into point
tokens represented by vectors, and a token encoder further
refines point tokens and transforms tokens into a modulated
form for transmission. The token encoder comprises two
parallel JSCC encoders for producing the probabilities of
constellation point positions, which further guide the mod-
ulator to generate modulated tokens (constellation points).
This parallel design naturally forms a main branch and an
auxiliary branch, enabling the generation of multi-level point
cloud features. Additionally, the token encoder can embed
rate allocator and channel adapter to achieve corresponding
adaptive capabilities. At the receiver, the received constellation
points are demodulated on the in-phase and quadrature paths
separately. The semantic features obtained from demodulation
are then fed into the JSCC decoder and subsequently passed
through the de-tokenizer for the final reconstruction task.
When Rayleigh fading and rate adaptive transmission are
considered, the corresponding equalization and constellation
points padding procedure will be conducted at the receiver.
To be specific, since only the geometry information is
considered, a point cloud can be expressed as X = {xi},
i = 1, Â· Â· Â· , N, where N is the number of points in the point
cloud, and xi âˆˆR3 represents each pointâ€™s three-dimensional
coordinates. The raw point cloud is first processed by the
point tokenizer, which reduces the number of points in the
point cloud and enhances feature dimensions, resulting in
Xpt = {xâ€²
i}, i = 1, Â· Â· Â· , N â€², where N â€² denotes the number of
point tokens and xâ€²
i âˆˆRCâ€² is the embedding of the token. Câ€²
is the feature dimension of the point token. Subsequently, Xpt
is fed into two parallel DL-based JSCC encoders, namely the
main JSCC encoder and the auxiliary JSCC encoder, to obtain
more informative point cloud semantic features. If channel
adaptation is required, two independent channel adapters are
incorporated into the main branch and auxiliary branch to
adjust the outputs of the JSCC encoders based on the channel
condition Î³. The output of the main branch Ymain can be
obtained by
Ymain = Cmain (Smain (Xpt; Î±main) ; Î²main, Î³) ,
(1)
where Smain ( Â· ; Î±main), Cmain ( Â· ; Î²main, Î³) denotes the main
JSCC encoder and channel adapter with trainable parame-
ters Î±main, Î²main correspondingly. Î³ is the channel condi-
tions. In the same way, for the auxiliary branch, Yauxi. =
Cauxi. (Sauxi. (Xpt; Î±auxi.) ; Î²auxi., Î³). Then Ymain, Yauxi. are con-
catenated as Y and sent to the modulator to generate modu-
lated tokens Z âˆˆCNmod.. If the number of transmitted constel-
lation points needs to be dynamically adjusted based on the
semantic features of point clouds, then before transmission, the
rate allocator will generate a mask based on JSCC outputs to
discard a part of the modulated tokens. After the rate allocation
operation, the resulting constellation points are represented
as Ëœ
Z âˆˆCNsend (Nsend â‰¤Nmod.). The above process can be
formulated as
Ëœ
Z = R (M (Y ; Î¾) ; Y , Î¶) ,
(2)
where Y = concat (Ymain, Yauxi.). M ( Â· ; Î¾) and R ( Â· ; Y , Î¶)
are modulator and rate allocator with parameters Î¾ and Î¶. It
should be noted that Î¶ is trainable, while Î¾ represents the
available standard digital constellation points that are manually


4
3
5
7
-3
7
-1
Original
Point Cloud
Main
JSCC
Encoder
Point
Tokenizer
Auxiliary
JSCC
Encoder
ğ‘¿
ğ‘¿!"
Point
Token
â€¦
Channel
Adapter
Channel
Adapter
Main Branch
Auxiliary Branch
Modulator
ğ’€
ğ’€#$%&
ğ’€$'(%.
ğ›¾
ğ›¾
ğ’
Rate
Allocator
ğ’%
ğ’€
-7
1
-3
5
-1
3
â€¦
Modulated
Token
Power
Normalizer
Token Encoder
Wireless
Channel
ğ’&
ğ’'
Equalizer
Zero
Padder
ğ’'*+.
ğ’'!$,.
In-phase
Demodulator
Quadrature
Demodulator
ğ’€'
JSCC
Decoder
ğ‘¿'!"
Reconstructed
Point Cloud
Point
De-tokenizer
Token Decoder
ğ‘¿'
Fig. 2: The overall framework of the proposed token communication system for point cloud geometry transmission. Blocks
with dashed borders indicate that they are optional. The rate allocator and channel adapter are required when relevant adaptive
characteristics are needed. If Rayleigh fading channels are considered, the equalizer will be used.
set in advance as a token codebook. The output of the power
normalizer is denoted as Â¯Z âˆˆCNsend with targeted power. Â¯Z
will be transmitted over the wireless channel.
We consider both the additive white Gaussian noise
(AWGN) channel and the Rayleigh fading channel. At the
receiver, the received constellation points are denoted as Ë†Z.
As for the AWGN scenario, Ë†Z can be represented as
Ë†Z = Â¯Z + n,
(3)
where n is the Gaussian noise with noise power Pnoise, and
n âˆ¼CN(0, PnoiseI). I is a Nsend Ã— Nsend identity matrix. The
received signal power can be calculated as
Psignal = E

âˆ¥Â¯
Zâˆ¥2
2

Nsend
.
(4)
Correspondingly, the signal-to-noise ratio (SNR) in decibels
for the AWGN channel can be calculated as
SNR = 10 log10
Psignal
Pnoise
.
(5)
As for the Rayleigh fading scenario, Ë†Z can be obtained as
Ë†Z = h Â· Â¯Z + n,
(6)
where h is the channel gain between the transmitter and the
receiver. The definition of n is the same as in the AWGN
scenario. The received signal power should be calculated as
Psignal = E

âˆ¥h Â· Â¯
Zâˆ¥2
2

Nsend
.
(7)
The SNR in decibels for the Rayleigh fading case is also
calculated following (5). For both types of channels, SNRs
are fed into the channel adaptation module as the channel
condition Î³.
The receiver will perform zero forcing (ZF) equalization in
the Rayleigh fading scenario as
Ë†Zeq. = hâˆ—
|h|2 Â· Ë†Z,
(8)
where Ë†Zeq. is the output of the equalizer. And when the rate al-
locator is included at the transmitter, the receiver will conduct
zero padding. By padding zeros to restore the symbol number
to Nmod., the dimensions of Ë†Zpad. can match the subsequent
demodulation models. Subsequently, Ë†Zpad. is divided into the
in-phase and quadrature paths for demodulation individually,
completing the transformation from the constellation points
Ë†Zpad. to the semantic features Ë†Y . Ë†Y is further processed in
the JSCC decoder to get a downsampled point cloud Ë†
Xpt. The
above process can be represented as
Ë†
Xpt = Sâˆ’1 
Mâˆ’1 
Râˆ’1 
Ë†Z

; Îº

; Î·

,
(9)
where Râˆ’1 ( Â· ) represents the padding process, which does
not require a mask from the transmitter. Mâˆ’1 ( Â· ; Îº) repre-
sents demodulator with trainable parameters Îº and Sâˆ’1 ( Â· ; Î·)
represents JSCC decoder with trainable parameters Î·. Ë†
Xpt is
upsampled in the point de-tokenizer to get the reconstructed
point cloud Ë†
X ultimately.
B. Performance Metrics
Two commonly used metrics for measuring the geometric
fidelity of point clouds, namely D1 and D2 [35], are adopted
to evaluate the end-to-end performance of the proposed com-
munication system. To be specific, the D1 metric based the
original point cloud X and the reconstructed point cloud Ë†
X
are defined as
D1 = max
n
e1,Xâ†’Ë†
X, e1, Ë†
Xâ†’X
o
,
(10)
where e1,Xâ†’Ë†
X and e1, Ë†
Xâ†’X can be calculated as (11).
e1,Xâ†’Ë†
X and e1, Ë†
Xâ†’X represent the average point-to-point


5
distance square between two point clouds. This point-to-point
distance is defined as the minimum Euclidean distance from
a specific point in one point cloud to points in the other point
cloud.
e1,Xâ†’Ë†
X =
1
|X|
X
xâˆˆX
min
Ë†xâˆˆË†
X
âˆ¥x âˆ’Ë†xâˆ¥2
2,
e1, Ë†
Xâ†’X =
1
| Ë†
X|
X
Ë†xâˆˆË†
X
min
xâˆˆX âˆ¥Ë†x âˆ’xâˆ¥2
2.
(11)
The peak signal-to-noise ratio (PSNR) version in decibels is
utilized more frequently. D1 PSNR can be calculated as
D1 PSNR = 10 log10
3P 2
D1 .
(12)
where P is the peak value of the point cloud coordinates.
D2 is defined similarly as
D2 = max
n
e2,Xâ†’Ë†
X, e2, Ë†
Xâ†’X
o
,
(13)
where e2,Xâ†’Ë†
X and e2, Ë†
Xâ†’X are calculated based on the
average point-to-plane distance. This point-to-plane distance
is the shortest Euclidean distance from a specific point in one
point cloud to the planes where points in the other point cloud
lie. Taking e2,Xâ†’Ë†
X as an example, it can be calculated as
e2,Xâ†’Ë†
X =
1
|X|
X
xâˆˆX
cos2 Î¸xË†x min
Ë†xâˆˆË†
X
âˆ¥x âˆ’Ë†xâˆ¥2
2,
(14)
in which Î¸xË†x is the angle between vector xâˆ’Ë†x and the normal
vector of Ë†x. e2, Ë†
Xâ†’X can be calculated in the same way by
swapping the position of X and Ë†
X and x and Ë†x. And the
PSNR value of D2 is defined by replacing D1 with D2 in
(12). It should be highlighted that the calculation of D1 is
simpler because it does not include normal vectors, while D2
more accurately reflects the perceptual characteristics of the
human visual system [17].
IV. MODEL DESIGN
This section presents the point tokenizer first. Then it
focuses on the proposed JSCCM scheme for the token encoder,
including two parallel JSCC encoders and the differentiable
modulator. Besides, two adaptive modules, namely the rate
allocator and channel adapter, are introduced. Finally, the
demodulator, JSCC decoder, and point de-tokenizer are pre-
sented.
A. Point Tokenizer
We adopt Set Abstraction, first introduced in PointNet++
[29], to perform point tokenization. Set Abstraction effectively
aggregates the features in a point patch, resulting in subsam-
pled points with higher feature dimensions. Each of these
subsampled points is treated as a point token, with its features
serving as the embedding of the point token. In this way, each
point token represents the structural features of a local region
in the point cloud. When processed by the subsequent Point
Transformer [26], the geometric relationships among the point
tokens are further captured, resulting in tokens with enhanced
representation capabilities.
Set Abstraction consists of three key layers: Sampling
Layer, Grouping Layer, and PointNet Layer. In the Sampling
Layer, the farthest point sampling (FPS) algorithm is applied.
It starts by randomly selecting a point xc1 from the original
point cloud X = {x1, . . . , xN} as a centroid. Then, the point
xc2 that is farthest from Xcentroid = {xc1} is selected from
the remaining points X \ {xc1} and added to the centroid set
Xcentroid. This process is iterated, where each time, the point
xcj farthest from the centroid set Xcentroid =

xc1, . . . , xcjâˆ’1
	
is selected from the remaining points X \

xc1, . . . , xcjâˆ’1
	
.
The ultimate output of the Sampling Layer is Xcentroid =

xc1, . . . , xcNâ€²
	
. It is worth noting that the distance from
a point to a point set is defined as the minimum Euclidean
distance between this point and all points in the set. In the
Grouping Layer, the ball query algorithm is conducted to find
the K neighboring points 2 of each centroid xcj within a
specified radius. In the local region corresponding to each
centroid point xcj, the coordinates and features of the K
neighboring points are grouped. Since only point geometry is
considered, features in the Set Abstraction for point tokenizer
are simply coordinates. In the PointNet Layer, the coordinates
of the neighboring points are transformed into a local frame
relative to the corresponding centroid points. Subsequently, as
in PointNet [36], a 1 Ã— 1 convolution with shared parameters
across different local regions is utilized to adjust the dimension
of grouped features. Finally, by performing max pooling
within each local region, the output of Set Abstraction is
obtained.
(N, 3)
FPS
âˆ’âˆ’â†’(N â€², 3)
|
{z
}
Sampling Layer
ball query
âˆ’âˆ’âˆ’âˆ’âˆ’â†’(N â€², K, C)
group
âˆ’âˆ’âˆ’â†’(N â€², K, C + 3)
|
{z
}
Grouping Layer
diff.
âˆ’âˆ’â†’(N â€², K, C + 3)
1Ã—1 conv.
âˆ’âˆ’âˆ’âˆ’âˆ’â†’(N â€², K, Câ€²)
pool.
âˆ’âˆ’â†’(N â€², Câ€²)
|
{z
}
PointNet Layer
.
(15)
The dimension changes of the input and output can be
represented by (15). In (15), numbers in parentheses represent
dimensions of inputs and outputs. C is the feature dimension
of points. Since we only consider the transmission of point
cloud geometry, the value of C for the point tokenizer is set to
3. Câ€² is the feature dimension of points adjusted by PointNet.
B. JSCCM Scheme for the Token Encoder
The JSCCM scheme is designed for the token encoder to
generate more informative and robust tokens. JSCCM con-
sists of two parallel JSCC encoders implemented with Point
Transformer [26] and a differentiable modulation method that
combines Gumbel-Softmax [19] and soft quantization [20].
As for JSCC encoders, the detailed structure of the two par-
allel JSCC encoders and the illustration of Point Transformer
are shown in Fig. 3. The encoders have two branches: the
main JSCC encoder Smain ( Â· ; Î±main) and the auxiliary JSCC
2In implementation, if there are fewer than K points within the radius,
the point corresponding to the smallest index within the radius is used for
padding. If there are more than K points, the points are sorted in descending
order of their indices, and the top K points are selected.


6
Point Transformer
Block
MLP
Flatten
ğ‘¿!"
ReLU
Linear
ğ‘ğ‘’ğ‘Ÿğ‘šğ‘¢ğ‘¡ğ‘’
ğ‘#$%&, 2 ğ‘€
1,2 ğ‘€Ã—ğ‘#$%&
1,2 ğ‘€Ã—ğ‘#$%&
ğ‘', C'
Point Transformer
Block
ğ‘', C'
Set Abstraction
ğ‘'', ğ‘#$%&
ğ‘'', ğ‘#$%&
ğ’€()**,#$%& ğ‘#$%&, 2 ğ‘€
ğ‘Ÿğ‘’ğ‘ â„ğ‘ğ‘ğ‘’
Point Transformer
Block
Linear
MLP
Flatten
ReLU
ğ‘¿-.&"/0%1
Linear
ğ‘', ğ‘$23%.
ğ‘', ğ‘$23%.
ğ‘ğ‘’ğ‘Ÿğ‘šğ‘¢ğ‘¡ğ‘’
ğ‘$23%., 2 ğ‘€
1,2 ğ‘€Ã—ğ‘$23%.
1,2 ğ‘€Ã—ğ‘$23%.
ğ’€()**,$23%. 
ğ‘$23%., 2 ğ‘€
ğ‘Ÿğ‘’ğ‘ â„ğ‘ğ‘ğ‘’
Main JSCC Encoder
1,2 ğ‘€Ã—ğ‘#$%&
Auxiliary JSCC Encoder
1,2 ğ‘€Ã—ğ‘$23%.
ğ‘¿-.&"/0%5
'
Linearğœ‘
Linearğœ“
features
neighboring
features
Linearğ›¼MLPğ›¿
coordinate
differences
Softmax
MLPğ›¾
Aggregation
(a) Parallel JSCC Encoders
(b) Point Transformer Layer
(c) Point Transformer Block
Linear
Point Transformer
Layer
Linear
Fig. 3: The model architecture of JSCC encoders. The numbers in brackets are dimension information. (a) The diagram of two
parallel JSCC encoders, including a main JSCC encoder and an auxiliary encoder. (b) The diagram of a Point Transformer
layer, which conducts vector attention. (c) The diagram of a Point Transformer Block.
encoder Sauxi. ( Â· ; Î±auxi.), which are used to extract different
levels of geometric features from the point cloud. Both JSCC
encoders take the point tokens xâ€²
i âˆˆXpt obtained from Set
Abstraction as input, and further characterize the relationships
between point tokens through the Point Transformer to en-
hance the representation capability of the tokens.
The Point Transformer no longer uses dot products to
compute attention scores. Instead, it computes the differ-
ence between query and key and then applies a multilayer
perception (MLP) for a nonlinear transformation to obtain
the attention scores, which is a vector attention mechanism.
Additionally, since point clouds inherently contain positional
information, the position encoding of the Point Transformer
is obtained by computing the difference of the point cloud
coordinates, followed by a nonlinear transformation through
an MLP. The first Point Transformer layer in the main branch
can be formulated as
xâ€²â€²
i =
X
xâ€²
jâˆˆËœ
Xj,pt
softmax
 
Î³
 Ï† (xâ€²
i) âˆ’Ïˆ
 xâ€²
j

+ Î´(xi âˆ’xj)

âˆš
Câ€²
!
âŠ™
 Î±
 xâ€²
j

+ Î´(xi âˆ’xj)

,
(16)
where
Ëœ
Xj,pt âŠ†Xpt, and
Ëœ
Xj,pt is obtained by indexing Xpt.
The indices are acquired by conducting k-nearest neighbors
(kNN) on xcj over Xcentroid =

xc1, . . . , xcNâ€²
	
. Ï†, Ïˆ, and
Î± represent linear layers projecting point tokens to get query,
key, and value, respectively. Here xi, xj âˆˆX are the point
coordinates for xâ€²
i and xâ€²
j. Î³ and Î´ are both MLPs, which
consist of two linear layers and one linear rectification function
(ReLU). To control the feature dimensions and alleviate the
vanishing gradient problem, a linear layer is added before and
after the Point Transformer layer, and the input and output are
linked through a residual connection to form a complete Point
Transformer block.
As illustrated in Fig. 3, compared to the auxiliary JSCC
encoder, the main JSCC encoder includes an additional Set
Abstraction operation to further aggregate features and an
additional Point Transformer to further capture relationships
across different local regions. In addition to the core Point
Transformer, both parallel JSCC encoders incorporate MLPs
to adjust the feature dimensions, enabling the outputs to
be compatible with the subsequent modulation module. The
adjacent flatten and linear layers in each encoder are de-
signed to distribute the semantic information of the point
cloud across different constellation points as much as pos-
sible. The final outputs of the two parallel JSCC encoders
are formatted as logits representing the probability of each
constellation point location. They can be represented as
YJSCC, main = Smain (Xpt; Î±main) âˆˆRNmainÃ—2
âˆš
M, YJSCC, auxi. =
Sauxi. (Xpt; Î±auxi.) âˆˆRNauxi.Ã—2
âˆš
M. Nmain, Nauxi. denote the
number of modulated tokens in each branch. M refers to
the number of constellation points in quadrature amplitude
modulation (QAM), which decides the available choices in
the token codebook. Similar to Xcentroid, Xâ€²
centroid represents
N â€²â€² centroid points, obtained by sampling of Set Abstraction
in the main JSCC encoder.
As for the differentiable modulation method, the process
of modulation is demonstrated in Fig. 4. Since the rate
allocator and channel adapter in the proposed communication
system are optional, we skip the channel adapter for now to
present the concepts smoothly.M-QAM, which means there
are M constellation points arranged in a square, is considered
for the modulation scheme. The input of the modulator is
Y âˆˆRNmod.Ã—2
âˆš
M where Nmod. denotes the total number of
modulated tokens generated before rate allocation. Instead of
converting bits into constellation points, the modulator here
generates constellation points based on the semantic features
Y , which represent the probabilities of constellation point


7
ğ’€
ğ‘!"#.
2 ğ‘€
In-phase
Quadrature
Gumbel
R.V.
Softmax
Temperature
 
Token Codebook
[-7,-5,-3,-1,1,3,5,7]
-3.9
-7 -5 -3 -1 1 3 5 7
ğ‘‘%,'
ğ‘‘%,(
ğ‘‘%,)
ğ‘‘%,*
ğ‘‘%,+
ğ‘‘%,,
ğ‘‘%,-
ğ‘‘%,.
Calculate
Distance
ğ’…
Forward Phase
Backward Phase
argmin
Softmax
index
[-7,-5,-3,-1,1,3,5,7]
Non-differentiable
Differentiable
  
-3
-3.7
pair
(-3, 5)
Modulated
Token
5
Generated from
Quadrature Path
[-7,-5,-3,-1,1,3,5,7]
pair
(-3.7, 5.2)
Soft
Modulated
Token
5.2
< Â· >
< Â· >
Fig. 4: An illustrative example of the proposed differentiable
modulation method. 64-QAM is used in this example, which
means M = 64.
locations. After passing through the JSCC encoders and the
modulator, the point tokens to be transmitted are converted
into modulated tokens with fewer symbols, rich semantics,
and suitability for transmission.
We denote yi âˆˆR1Ã—2
âˆš
M as the i-th row of Y , which
contains the positional information of the i-th constellation
point. Furthermore, let yi = (yI,i, yQ,i) which means the first
âˆš
M components of yi represent logits for the in-phase posi-
tion of the constellation point, while the last
âˆš
M components
represent logits for the quadrature position.
Taking the generation of in-phase position of the i-th
constellation point as an example, in the forward phase, we
use the Gumbel-Softmax method to obtain a soft output of the
constellation point position probabilities tI,i, as
tI,i(j) =
exp ([Ï„j + yI,i(j)] /T)
Pâˆš
M
k=1 exp ([Ï„k + yI,i(k)] /T)
,
(17)
where tI,i(j) is the j-th entry of tI,i and yI,i(j) is the j-
th element in yI,i. Ï„j is sampled from Gumbel(0, 1). This
is the Gumbel-Softmax trick [34], which allows a discrete
distribution to be reparameterized for sampling. It transforms
the process of sampling from a discrete distribution into the
process of finding the maximum element (argmax) of a vector
with added randomness. And the softmax operation is used to
approximate the non-differential argmax operation. T is the
temperature hyperparameter, which is utilized to control the
steepness of the distribution of tI,i.
Then we define c = (c1, c2, . . . , câˆš
M) as the token code-
book, consisting of the standard coordinates of an M-QAM
modulation scheme. Based on tI,i and c, the inner product can
be calculated to generate an initial position by
zI,i =< tI,i, c >= tI,i Â· cT.
(18)
Distances between zI,i
and c can be further calcu-
lated asdi = (di,1, di,2, . . . , di,
âˆš
M) = (|c1 âˆ’zI,i|, |c2 âˆ’
zI,i|, . . . , |câˆš
M âˆ’zI,i|). And based on the distances between
the initial position and standard coordinates of M-QAM, the
quantization is performed as
Â¯zI,i =< one-hot
 
argmin
jâˆˆ{1,...,
âˆš
M}
di,j
!
, c >,
= one-hot
 
argmin
jâˆˆ{1,...,
âˆš
M}
di,j
!
Â· cT ,
(19)
where Â¯zI,i is the final output corresponding to the in-phase
coordinate of the i-th constellation point. Similarly, the coordi-
nate of the i-th constellation point zQ,i is generated, followed
by the aforementioned process with yQ,i as inputs. zI,i and
zQ,i will be paired to form a constellation points. Thus far,
the modulator maps the logits yi to the modulated token
zi = (zI,i, zQ,i).
Since the argmin operation in (19) is non-differential, in
the backward phase, a soft quantization method is used and
the soft output corresponding to the in-phase coordinate of the
i-th constellation point ËœzI,i is calculated by
ËœzI,i =
âˆš
M
X
k=1
exp (âˆ’di,k/T)
Pâˆš
M
j=1 exp (âˆ’di,j/T)
Â· ck.
(20)
In implementation, the computations of the forward and back-
ward stages can be integrated as follows
zI,i
output = detach(Â¯zI,i âˆ’ËœzI,i) + ËœzI,i,
(21)
where zI,i
output can represent the output of the i-th constellation
points in In-phase for both forward phase and backward phase.
Because the detach operation has no effect during the forward
phase, so zI,i
output = Â¯zI,i. While during the backward phase, the
detach will detach (Â¯zI,i âˆ’ËœzI,i) from the computation graph.
The gradient will propagate through the differentiable zI,i
output =
ËœzI,i. This logic can be easily implemented using PyTorch.
C. Rate Allocator and Channel Adapter
For the rate allocator, motivated by [37], we employ
Gumbel-Softmax to achieve differentiable rate selection. Nev-
ertheless, unlike the approach in [37], we do not incorporate
channel features into mask generation. Instead, the rate se-
lection is guided solely by Y . This enables a decoupling of
functional modules. The specific structure of the rate allocator
is shown in Fig. 5a. It first reshapes Y so that the max pooling
layer operates along the dimension representing constellation
point position probabilities. The pooled result is then passed
through a Progressive MLP. Each block reduces the output
feature dimension to
1
4 of its input until the target number
of rate levels is reached. Here we set the rate level to 5.
Subsequently, the cutoff position for the mask is determined
using the Gumbel-Max method, where the non-differentiable
argmax will be replaced by softmax during backpropagation,
resulting in a one-hot vector. This one-hot vector is further
transformed into a thermal vector representing the mask,
where 1 indicates that the corresponding constellation point
should be transmitted, and 0 indicates that it should not. It is
worth noting that the rate allocator only adjusts the number
of constellation points originating from the auxiliary branch.


8
ğ’€
MaxPooling
ğ‘Ÿğ‘’ğ‘ â„ğ‘ğ‘ğ‘’
Progressive
MLP
Mask
Generation
1 1 1 0 0
ğ’!"#$
ğ’"%&#.
unmasked
60%
masked
40%
2Ã—ğ‘!"#.,
ğ‘€
Progressive MLP
Mask Generation
1, 2Ã—ğ‘!"#.
Linear
BatchNorm
ReLU
1, ğ‘!"#./2
â€¦
1, 5
1, 5
Gumbel
R.V.
Argmax
0
0
1
0
0
1
1
1
0
0
ğ‘!"#$ 
ğ‘"&'#. 
(a) Rate Allocator
ğ’€!"##,%&'( 
(ğ’€!"##,&*+' )
ğ‘Ÿğ‘’ğ‘ â„ğ‘ğ‘ğ‘’
2Ã—ğ‘%&'(
ğ‘€
(2Ã—ğ‘&*+'.)
SNR
Scale &
Repeat
ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¡ğ‘’
Linear & ReLu =
Linear & ReLu â†“2
Linear & ReLu =
Sigmoid
ğ’€%&'( 
(ğ’€&*+' )
+ 
(b) Channel Adapter
Fig. 5: The model architectures of the rate allocator and
channel adapter.
Because we allocate more modulation symbols and apply an
additional Point Transformer Block to the main branch. To
ensure high reconstruction quality, we retained all symbols
generated by the main branch, which are more informative.
Since this masking strategy only discards constellation points
at the tail, zero padding can be applied at the receiver without
the need to transmit the mask.
For the channel adapter, we fuse the semantic information
of the point cloud with channel conditions using the concate-
nation method. Since the logits output by the designed par-
allel JSCC encoders represent the probability of constellation
point positions, we can directly adjust the logits to influence
constellation point generation with a single module at the
encoder. This approach contrasts with implicit SNR refinement
methods, which typically insert SNR-based feature adjustment
modules after each feature extraction layer in both the encoder
and decoder [32], [33]. The detailed structure of the channel
adapter is demonstrated in Fig. 5b. Scale means the SNR will
be divided by 10 to avoid numerical issues.
D. Token Decoder and Point De-tokenizer
The token decoder and point de-tokenizer are responsible
for converting the modulated tokens received through the
channel back into a point cloud, thereby completing the re-
construction task. We primarily adopt the receiver-side model
proposed in [27] to implement the token decoder and point
de-tokenizer.
Concerning the token decoder, when the transmitter is
equipped with a rate allocator, the token decoder first pads
the received modulated tokens with zeros to get
Ë†Zpad. âˆˆ
RNmod.Ã—2 with a fixed length, enabling further processing
by the demodulator. The demodulation process is divided
into in-phase and quadrature branches, where two separate
1D transposed convolution layers are used to adjust both
the channel dimension and feature dimension. The semantic
features from the two branches are finally merged to obtain
Ë†Y âˆˆRNdemod.Ã— N
16 . In the JSCC decoder, Ë†Y is transformed into
downsampled point coordinates. Based on the downsampled
point coordinates, Ë†Y can be refined by a Point Transformer,
resulting in Ë†Yrefined âˆˆR
N
16 Ã—Ndemod.. Ë†Yrefined can estimate point
coordinates again, obtaining Ë†
Xpt âˆˆR
N
16 Ã—3.
As for the point de-tokenizer, we utilize the upsampling
module in [38]. This method upsamples the point cloud by
adding different offsets to each point. For instance, for the
output of the JSCC decoder ( Ë†
Xpt, Ë†Yrefined), the upsampled point
n
( Ë†
Xk
pt, Ë†Y k
refined)
o
, k âˆˆ{0, Â· Â· Â· , K âˆ’1} is generated by
Ë†
Xk
pt = Ë†
Xpt + r Â· âˆ†k( Ë†Yrefined),
(22)
Ë†Y k
refined = Î¦k( Ë†Yrefined),
(23)
where âˆ†k, Î¦k, k = 0, Â· Â· Â· , K âˆ’1 are MLPs. The last layer of
âˆ†k is tanh. r is used to control the offset range. In this point
de-tokenizer, two upsampling operations are performed, each
with a Ã—4 upsampling ratio, namely K = 4.
V. SIMULATION RESULTS
A. Simulation Settings
1) Dataset:
The publicly accessible dataset ShapeNet-
Core.v2 [39], which contains 55 common objects with about
51000 pre-aligned 3D models, is utilized in the simulations.
ShapeNetCore.v2 is split into training, validation, and test
sets with a ratio of 70%, 10%, and 20%, respectively. In
simulations, the FPS algorithm is used to uniformly sample
2,048 points from the surface of each shape in the dataset to
construct the point clouds, namely N = 2048.
2) Training Details: As for loss functions, when the rate
allocator is not included, the Chamfer distance (CD) metric
is utilized, representing reconstruction performance. CD is
defined as
LCD =
1
|X|
X
xâˆˆX
min
Ë†xâˆˆË†
X
âˆ¥x âˆ’Ë†xâˆ¥2
2 +
1
| Ë†
X|
X
Ë†xâˆˆË†
X
min
xâˆˆX âˆ¥Ë†x âˆ’xâˆ¥2
2,
(24)
which is symmetric, differentiable, and computationally ef-
ficient. If the rate adaptive capacity is considered, another
term LRate
= Nmod/Nsend for controlling the transmitted
constellation points is added to LCD. The loss function can
be calculated as
L = LCD + Î»LRate,
(25)
where Î» is a hyperparameter for balancing the reconstruction
quality and the number of modulated tokens. We set Î» =
2 Ã— 10âˆ’4 in simulations.
The batch size in training is 256. The Adam optimizer is
utilized with an initial learning rate 1 Ã— 10âˆ’3 and weight
decay factor 1 Ã— 10âˆ’4. The learning rate scheduler adjusts
the learning rate every 20 epochs, reducing it to half of its
previous value each time. We train our models with the channel
adaptive ability within a specific SNR range [âˆ’0.5dB, 10.5dB].
During training, the SNR value for each batch in an iteration


9
is uniformly sampled from [âˆ’0.5dB, 10.5dB]. Signal power
normalization is performed with respect to all batches within
the iteration. Psignal is normalized to 1. As for the Rayleigh
fading channel, the channel gain h is created as h âˆ¼CN(0, 1).
Across all experiments, the models under Rayleigh fading
channels are derived by fine-tuning the models trained under
AWGN channels with the same SNR settings.
3) Benchmarks: To demonstrate the effectiveness of our
proposed methods, we choose baseline methods introduced as
follows.
â€¢ â€œModulated SEPTâ€ method: SEPT [27] is a DL-based
analog JSCC method for point clouds. Since this method
produces analog outputs, to ensure a fair comparison, we
apply the differentiable modulation method proposed in
Section IV-B to SEPT [27].
â€¢ â€œModulated PCSTâ€ method: We refer to the point se-
mantic communication method in [28] as PCST (Point
Cloud Semantic Transmission). [28] considered a higher
bitrate range and calculated the bit rate by assuming that
each floating-point number is represented with 16 bits.
For a fair comparison, we employed the differentiable
modulation method proposed in Section IV-B to perform
modulation and control the bit rate to the same range.
We reproduced PCST following [28]: the number of
points per patch was set to 256, the dimension of global
information was 4, and the dimension of local semantic
information was increased from 8 to 16. The MVTorch
[40] library was used to perform projection, with the
number of projection maps set to 4. The number of
symbols for the lossless transmission part was computed
using the Shannon channel capacity formula.
â€¢ â€œSoft Quantizationâ€ method: Soft Quantization is a dig-
ital JSCC technology proposed in [20]. It generates
channel symbols based on the softmax weighted sum of
distances from JSCC outputs to predefined constellation
sets in backpropagation. To ensure that the employed Soft
Quantization in this baseline is fully consistent with ours,
we omitted the temperature coefficient adjustment during
training in [20]. To control the number of transmitted
symbols, our JSCC applies max pooling when using the
Soft Quantization, resulting in features of size Nmod. Ã— 2.
â€¢ â€œGumbel-Softmaxâ€ method: Gumbel-Softmax based dif-
ferential modulation method is proposed in [19]. Gumbel-
Max is used to perform reparameterized sampling so that
constellation points can be generated. Gumbel-Softmax
is adopted during backpropagation to handle the non-
differentiability of the argmax operation.
â€¢ â€œSTEâ€ method: STE is a simple gradient estimation
method that directly sets the output of a non-differentiable
operation equal to its input in the backward phase. In
[32], STE is applied to differentiable modulation. In this
paper, models based on the STE method are finetuned on
models trained with the Soft Quantization method.
â€¢ â€œUniform Noiseâ€ method: This method approximates the
non-differentiable quantization through adding uniform
noise to the quantization inputs in backpropagation [33].
In this paper, models using the Uniform Noise method
are also finetuned on Soft Quantization models.
â€¢ â€œG-PCC + LDPCâ€ method: G-PCC is a point cloud
geometry compression standard proposed by Moving
Picture Experts Group (MPEG). We use the command-
line tool mpeg-pcc-tmc13 3 proposed by MPEG to realize
the G-PCC codec. The parameters of encoding and de-
coding are specified as [41], which meet MPEG common
test conditions. LDPC is chosen for channel coding.
We implement it through Sionna [42], an open-source
library for link-level simulations based on TensorFlow.
The selection of coding rate and modulation order fol-
lows Table 5.1.3.1-1 for the Physical Downlink Shared
Channel (PDSCH) in 3GPP TS 38.214 version 16.2.0 4.
In the simulations, the modulation and coding scheme
(MCS) that yields the best performance is selected.
4) Details and Abbreviation of Our Methods:
All the
temperature hyperparameter T used is set to 1.5. We consider
cases where Nmod. = 50, 100, 150, 200, 250, 300. Except for
Nmod. = 300, where the ratio between Nmain and Nauxi. is set
to 2 : 1, the ratio is 4 : 1 in all other cases. In all proposed
models, N
â€² = 512 and N
â€²â€² = 128. For the MLP in the main
JSCC encoder, the output dimension of each linear layer is
[128, 128, 2
âˆš
M]. While for the MLP in the auxiliary JSCC
encoder, the output dimension is [128, 128, 2
âˆš
M]. For MLPs
Î³ and Î´ in Point Transformer layers, output dimensions are
the same as input feature dimensions.
The proposed token communication system with JSCCM in
the token encoder is named PointTC (Point Token Communi-
cations). The optional rate allocator and channel adapter are
abbreviated as RA and CA, respectively.
B. Performance Analysis on Various SNRs and Rates
Fig. 6 demonstrates the reconstruction performance over
AWGN and Rayleigh fading channels under various SNRs.
Except for G-PCC+LDPC, all methods in this figure use 300
channel symbols with 64-QAM, whereas PointTC+CA+RA
can adaptively adjust the number of modulated tokens to be
transmitted. The simulation results indicate that the proposed
methods outperform Modulated SEPT and the traditional sep-
arated scheme G-PCC+LDPC under all SNRs. The separated
digital methods suffer from a severe cliff effect, while the
proposed method still achieves satisfactory performance at 0
dB despite using standard digital constellation points.
Besides, in Fig. 6, the PointTC trained at 10 dB under
AWGN channels exhibits inferior reconstruction performance
compared to PointTC trained and tested at matched SNRs,
with a performance gap exceeding 1 dB when SNR is 0
dB in terms of D1 PSNR. For D2 PSNR, the performance
gap between the two methods narrows in the high-SNR
regime, as D2 PSNR does not emphasize the precise re-
construction of geometric details. However, a noticeable gap
remains in the low-SNR regime. Moreover, PointTC trained
at matched SNRs requires multiple separate models, whereas
PointTC+CA achieves superior performance using only a
single model by adjusting constellation point positions through
3[Online]. Available: https://github.com/MPEGGroup/mpeg-pcc-tmc13
4[Online]. Available: https://www.etsi.org/deliver/etsi ts/138200 138299/
138214/16.02.00 60/ts 138214v160200p.pdf


10
291.3
Symbols
280.1
Symbols
259.4
Symbols
250.1
Symbols
247.5
Symbols
246.4
Symbols
246.1
Symbols
(a) AWGN
286.4
Symbols
275.7
Symbols
255.9
Symbols
248.2
Symbols
246.6
Symbols
246.0
Symbols
245.6
Symbols
(b) Rayleigh Fading
Fig. 6: The reconstruction performance versus SNRs over AWGN channels and Rayleigh fading channels. The blue numbers
on the D2-PSNR subplots indicate the actual number of modulated tokens transmitted by the PointTC+CA+RA model when
Nmod. = 300 is configured.
a concise channel adapter. With CA, the model can learn
more robust and informative features under varying training
SNRs. Notably, compared to PointTC+CA, PointTC+CA+RA
further improves performance under AWGN channels. Be-
cause using a fixed transmission rate for a single model under
varying channel conditions is suboptimal. PointTC+CA+RA
adopts a strategy of transmitting more symbols under poorer
channel conditions to achieve better end-to-end reconstruc-
tion performance, thereby achieving a better balance between
communication efficiency and reconstruction quality across
different SNRs. As for Rayleigh fading channels, since perfect
channel state information is assumed and equalization is
performed, the model refined on AWGN channels exhibits
similar performance on Rayleigh fading channels. In Fig.
6b, PointTC+CA+RA underperforms PointTC+CA. This may
be because RA operates without explicit channel informa-
tion. Its ability to infer channel conditions must be learned
implicitly through backpropagation, which is more difficult
under Rayleigh fading. Moreover, complex channel conditions
degrade the effectiveness of RAâ€™s probabilistic sampling, im-
peding the learning of an appropriate cutoff position.
Fig. 7 depicts D1 PSNR and D2 PSNR performance under
Rayleigh fading channels when the number of transmitted
symbols varies. We present performance curves under 15
dB and 0 dB. For traditional separation-based methods, bit
errors at 0 dB prevent the G-PCC decoder from successfully
decoding. In contrast, all methods implemented with JSCCM
can still complete decoding at 0 dB. In terms of D1 PSNR,
modulated SEPT outperforms the separation-based approach
even when the test SNR is 0 dB. However, for D2 PSNR,
when the number of transmitted constellation points exceeds
200, the separation-based method begins to surpass modulated
SEPT. In comparison, our proposed PointTC-based methods
demonstrate consistently superior performance across both D1
PSNR and D2 PSNR metrics, achieving a compression ratio
exceeding 6Ã— in terms of transmitted constellation points.
In Fig. 7, the D2 PSNR performance of Modulated PCST
is superior to its D1 PSNR performance. This is because
PCST adopts a strategy that captures point cloud projections
using virtual cameras and then performs feature fusion. The
projection features in PCST are assumed to be transmitted
losslessly, which preserves the global structural information,
leading to better D2 PSNR. However, under the same number
of transmitted symbols, PCST cannot maintain fine geometric


11
Fig. 7: The reconstruction performance versus the number of transmitted constellation points under Rayleigh fading channels.
0
2
4
6
8
10
12
14
16
SNR(dB)
36.5
37.0
37.5
38.0
38.5
39.0
39.5
40.0
D2 PSNR(dB)
PointTC + CA, proposed
PointTC + CA, STE
PointTC + CA, Uniform Noise
PointTC + CA, Gumbel-Softmax
PointTC + CA, Soft Quantization
Fig. 8: The D2 PSNR performance versus SNRs under
Rayleigh fading channels for different modulation methods.
details, resulting in inferior D1 PSNR performance. PCST is
more suitable for cases with a larger number of transmitted
symbols. When sufficient transmission symbols are available,
the proportion of symbols used for the lossless transmission
becomes relatively small, allowing more symbols to be allo-
cated for local features to present geometric details.
C. Performance Analysis on Modulation Methods
In Fig. 8, we compare the proposed JSCCM scheme with
other modulation methods commonly used in digital semantic
communication. For fair comparison, we adopt 64-QAM for all
methods and fix the number of transmitted constellation points
to 300. It can be observed that our modulation approach, which
combines Gumbel-Softmax and soft quantization, consistently
outperforms existing differentiable modulation methods across
all SNRs. Interestingly, the most intuitive method STE,
achieves the best performance among the baselines. Although
the Uniform Noise method theoretically provides a better
approximation of the quantization process, its performance
in our experiments is comparable to that of STE, possibly
requiring more sophisticated training strategies for further
improvement. When compared to the proposed modulation
method, except at 0 dB, either the Gumbel-Softmax-only or the
0
2
4
6
8
10
12
14
16
SNR(dB)
36.0
36.5
37.0
37.5
38.0
38.5
39.0
39.5
40.0
D2 PSNR(dB)
PointTC + CA, 256-QAM
PointTC + CA, 64-QAM
PointTC + CA, 16-QAM
PointTC + CA, 4-QAM
Fig. 9: The D2 PSNR performance versus SNRs under
Rayleigh fading channels of the proposed JSCCM with dif-
ferent modulation orders.
Soft-Quantization-only method shows limited representational
capacity, incurring a PSNR drop of more than 1 dB.
In Fig. 9, we evaluate the performance of the proposed
JSCCM under different modulation orders, with the number
of transmitted constellation points fixed at 300. It is ob-
served that, across all SNRs, higher modulation orders lead
to better point cloud reconstruction performance. From 4-
QAM to 64-QAM, increasing the modulation order results
in a performance gain of approximately 1.5 dB. However,
the improvement becomes marginal as the modulation or-
der increases beyond 64-QAM, indicating that 64-QAM is
approaching the performance limit of the employed JSCC
method. This experiment also suggests that, regardless of
the SNR condition, selecting the model with the highest
modulation order yields the best performance. This contrasts
with the conclusion of traditional adaptive modulation, which
selects lower modulation orders under low SNR conditions.
This difference arises from the fact that traditional adaptive
modulation aims to optimize digital communication systems
with respect to metrics such as bit error rate. Using high-order
modulation under low SNR typically leads to demodulation
errors. In contrast, our proposed method treats the received


12
(b) PointTC, train and test at SNR=0dB
(a) PointTC, train and test at SNR=15dB
(c) PointTC + CA, test at SNR=0dB
Fig. 10: Constellation point distributions generated by different models. The size of the blue circle indicates the probability of
constellation points occurring at that location. The larger the circle, the higher the probability.
0
5
10
15
20
25
30
35
40
45
50
Point Cloud Category
0.0
2.5
5.0
7.5
10.0
12.5
15.0
SNR(dB)
220
230
240
250
260
270
280
290
300
Average number of transmitted symbols
Fig. 11: The average number of transmitted symbols versus
various SNRs and point cloud categories.
constellation points as input features to a neural network at the
receiver side, without converting them into bits. As a result,
higher modulation orders provide more expressive features for
the decoder, leading to better representation capability.
D. Visualization Results
We analyzed the constellation point distributions produced
by the token encoders of different models over 10,000 point
clouds from the test set, with the visualization results shown
in Fig. 10. It can be observed that the models trained at the
test SNR exhibit constellation distributions consistent with
the conclusions of traditional adaptive modulation. Namely,
lower modulation orders are selected under worse channel
conditions. However, the results in Fig. 6 have demonstrated
that models trained at the test SNR perform worse than the
PointTC+CA model. Notably, PointTC+CA does not adopt
a lower modulation order at 0 dB. Instead, it maintains a
relatively high modulation order, providing the token decoder
at the receiver with richer semantic information, which in
turn leads to better performance. Furthermore, regardless of
the model used, the constellation points produced by the
proposed JSCCM method exhibit a distribution resembling
a two-dimensional independent Gaussian. Although no prior
distribution was explicitly imposed, the model achieves a form
of probabilistic shaping gain [43] through end-to-end training.
The number of transmitted symbols for different types of
point clouds under various SNRs is shown in Fig. 11. As
Original
G-PCC + LDPC
PointTC + CA
Modulated SEPT
Fig. 12: Visualization results of reconstruction.
illustrated, there is a substantial variation in the number of
transmitted symbols across different point cloud categories
under the same SNR, indicating that the RA module effec-
tively allocates modulated tokens based on the point cloud
semantics. For the same point cloud category, a clear trend
emerges that as the SNR increases, the number of transmitted
symbols decreases. This demonstrates that both the RA and
CA modules contribute to the token communication systemâ€™s
ability to adapt to both semantic content and channel condi-
tions. Visualization results of the point cloud reconstruction
performance are shown in Fig. 12. We fix the number of
transmitted constellation points to 300 and conduct testing
at an SNR of 15 dB. It can be observed that the proposed
method shows a clear advantage over the baselines in both
reconstruction details and subjective visual quality.
E. Performance Analysis on Practical Scenarios
Considering that the point clouds used in the previous simu-
lations were generated by sampling from common 3D models
in the ShapeNetCore.v2 dataset, we further evaluated our
model on the real-world point cloud dataset SemanticKITTI
[44]. We followed the data processing procedure in [38].
The performance of the PointTC + CA model trained on
ShapeNetCore.v2 is presented in Table I, where a substantial
degradation in both D1 PSNR and D2 PSNR can be observed.
This degradation occurs because the ShapeNetCore.v2 dataset
consists of synthetic point clouds, whereas the SemanticKITTI
dataset contains real point clouds captured by vehicle-mounted


13
TABLE I: Reconstruction performance on SemanticKITTI.
Each entry shows D1 PSNR / D2 PSNR (dB).
0.0 dB
5.0 dB
10.0 dB
15.0 dB
PointTC+CA
15.2 / 22.7
15.2 / 23.0
15.1 / 23.2
15.0 / 23.2
PointTC+CA, finetune
37.4 / 42.8
37.6 / 43.1
37.7 / 43.3
37.7 / 43.3
Fig. 13: The reconstruction performance versus SNRs over
Rayleigh fading channels on SemanticKITTI dataset.
0
2
4
6
8
10
12
14
16
SNR(dB)
36.0
36.5
37.0
37.5
38.0
38.5
39.0
39.5
40.0
D2 PSNR(dB)
PointTC + CA, perfect CSI
PointTC + CA, imperfect CSI (-20dB), w/ finetune
PointTC + CA, imperfect CSI (-20dB), w/o finetune
PointTC + CA, imperfect CSI (-10dB), w/ finetune
PointTC + CA, imperfect CSI (-10dB), w/o finetune
Fig. 14: The D2 PSNR performance versus SNRs under perfect
CSI and imperfect CSI conditions.
Light Detection and Ranging (LiDAR) sensors. To address
this issue, we further finetuned the PointTC + CA model
on the SemanticKITTI dataset to enhance its performance.
The corresponding results are also listed in Table I, and the
comparison between the finetuned model and the baseline is
shown in Fig. 13. It can be observed that, after finetuning, our
model achieves a significant performance improvement and
outperforms baselines.
In the previous simulations, for the Rayleigh fading channel,
we assumed that the ZF equalizer had perfect channel state
information (CSI), which is difficult to achieve in practi-
cal applications. To evaluate the model performance under
imperfect CSI conditions, we added two levels of complex
Gaussian white noise n âˆ¼CN(0, Ïƒ2
n) to the channel gain
h. Ïƒ2
n = 10âˆ’2 (-20 dB) represents a good CSI estimation
while Ïƒ2
n = 10âˆ’1 (-10 dB) represents a poor CSI estimation.
The results are shown in Fig. 14. It can be observed that,
with -20 dB noise, the model experiences only about a 0.3
dB performance loss, which is acceptable. However, with -
10 dB noise, the performance degradation exceeds 1.5 dB.
To improve performance under imperfect CSI, we finetuned
the model under CSI with noise. As shown in Fig. 14, the
TABLE II: Computational and Model Complexity.
Models
Enc time (ms)
Ded time (ms)
FLOPs (G)
Parameters (M)
PointTC + CA
12.8
1.2
10.06
24.45
PointTC + CA + RA
15.0
1.3
10.06
24.47
Modulated SEPT
55.4
1.4
15.38
12.33
Modulated PCST
19.5
0.2
11.06
5.96
GPCC + LDPC
34.2
568
-
-
finetuned model exhibits reduced performance loss under im-
perfect CSI, and the improvement becomes more pronounced
under better channel conditions. These results demonstrate that
our model possesses strong robustness against imperfect CSI
estimation results.
Finally, we conducted experiments to evaluate the compu-
tational and model complexity. The results obtained on an
NVIDIA RTX 4090 GPU are presented in Table II. The results
show that our model achieves the lowest end-to-end codec
latency and requires fewer floating-point operations than the
baselines. With an end-to-end latency of 14 ms, the PointTC
+ CA model can achieve a 71.4 frame per second (FPS) codec
speed. This efficiency is attributed to the fact that, although
the Point Transformer architecture introduces a relatively
large number of parameters, GPUs are well optimized for
Transformer-based structures. In addition, the time-consuming
kNN and FPS operations are implemented using CUDA in
our models, which further reduces the overall codec time.
These results demonstrate the strong potential of our models
for practical applications.
VI. CONCLUSION AND FUTURE WORK
In this paper, we have developed a token wireless com-
munication system for point cloud transmission. To obtain
informative and robust token representations, we design a joint
semantic-channel coding and modulation scheme for the token
encoder, which consists of two parallel Point Transformer-
based JSCC encoders and a differential modulator. The dif-
ferential modulator combines Gumbel-softmax and soft quan-
tization methods to generate high-quality modulated tokens.
Additionally, the rate allocator and channel adapter are based
on the physical meaning of JSCC outputs. Simulation results
demonstrate that our proposed methods exhibit superior per-
formance compared to the existing DL-enabled JSCC method
and traditional separated codec.
In essence, this paper presents a promising framework for
token communication in point clouds. However, many open
problems remain to be explored.
â€¢ In this paper, we only consider point geometry and
static point clouds. Further studies are needed on point
attributes and dynamic point clouds transmission for more
immersive applications.
â€¢ Our models require finetuning for real points. Developing
methods tailored for real-world data with varying point
densities and acquisition noise is necessary.
â€¢ Although this paper proposes the JSCCM framework
for token communication, models are designed for point
clouds. Building a unified token communication system
for multimodal data is an important direction.


14
REFERENCES
[1] J. Ying, Z. Qin, L. Wang, and X. Tao, â€œJoint semantic-channel coding
and modulation for point cloud,â€ in Proc. IEEE Int. Commun. Conf.
(ICC), Montreal, Canada, Jun. 2025, pp. 3984â€“3989.
[2] Z. Qin, L. Liang, Z. Wang, S. Jin, X. Tao, W. Tong, and G. Y. Li, â€œAI
empowered wireless communications: From bits to semantics,â€ Proc.
IEEE, vol. 112, no. 7, pp. 621â€“652, Jul. 2024.
[3] Z. Qin, J. Ying, D. Yang, H. Wang, and X. Tao, â€œComputing networks
enabled semantic communications,â€ IEEE Network, vol. 38, no. 2, pp.
122â€“131, Mar. 2024.
[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Å. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ in Proc. Adv.
Neural Inf. Process. Syst. (NeurIPS), Long Beach, CA, USA, Dec. 2017,
pp. 5998â€“6008.
[5] R. Child, S. Gray, A. Radford, and I. Sutskever, â€œGenerating long
sequences with sparse transformers,â€ arXiv preprint arXiv:1904.10509,
Apr. 2019.
[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,
â€œAn image is worth 16x16 words: Transformers for image recognition
at scale,â€ in Proc. Int. Conf. Learn. Represent. (ICLR), May. 2021.
[7] M. Guo, J. Cai, Z. Liu, T. Mu, R. R. Martin, and S. Hu, â€œPCT: Point
cloud transformer,â€ Comput. Vis. Media, vol. 7, pp. 187â€“199, Jun. 2021.
[8] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al., â€œLearning transferable
visual models from natural language supervision,â€ in Proc. Int. Conf.
Mach. Learn. (ICML), Jul. 2021, pp. 8748â€“8763.
[9] L. Qiao, M. B. Mashhadi, Z. Gao, R. Tafazolli, M. Bennis, and
D. Niyato, â€œToken communications: A unified framework for cross-
modal context-aware semantic communications,â€ IEEE Wireless Com-
mun. Mag., vol. 32, no. 5, pp. 80â€“88, Oct. 2025.
[10] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, â€œPoint-BERT:
Pre-training 3D point cloud transformers with masked point modeling,â€
in Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), New Orleans,
LA, USA, Jul. 2022, pp. 19 313â€“19 322.
[11] C. Ma, Y. Jiang, J. Wu, J. Yang, X. Yu, Z. Yuan, B. Peng, and X. Qi,
â€œUniTok: A unified tokenizer for visual generation and understanding,â€
in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), San Diego, CA, USA,
Dec. 2025.
[12] P. Hansen-Estruch, D. Yan, C.-Y. Chuang, O. Zohar, J. Wang, T. Hou,
T. Xu, S. Vishwanath, P. Vajda, and X. Chen, â€œLearnings from scaling
visual tokenizers for reconstruction and generation,â€ in Proc. Int. Conf.
Mach. Learn. (ICML), Vancouver, BC, Canada, Jul. 2025.
[13] J. Li, J. Xu, S. Huang, Y. Chen, W. Li, J. Liu, Y. Lian, J. Pan,
L. Ding, H. Zhou et al., â€œLarge language model inference acceleration: A
comprehensive hardware perspective,â€ arXiv preprint arXiv:2410.04466,
Oct. 2024.
[14] Google,
â€œAnnouncing
the
Agent2Agent
protocol
(A2A),â€
Apr.
2025.
[Online].
Available:
https://developers.googleblog.com/en/
a2a-a-new-era-of-agent-interoperability/.
[15] ITU-R WP 5D, â€œFramework and overall objectives of the future devel-
opment of IMT for 2030 and beyond,â€ Jun. 2023. [Online]. Available:
https://www.itu.int/md/R19-WP5D-230612-TD-0905/en.
[16] L. Qiao, M. Boloursaz Mashhadi, Z. Gao, and D. GÂ¨undÂ¨uz, â€œToken-
domain multiple access: Exploiting semantic orthogonality for collision
mitigation,â€ in Proc. IEEE Conf. Comput. Commun. Workshops (INFO-
COM WKSHPS), London, UK, May 2025.
[17] G. Valenzise, M. Alain, E. Zerman, and C. Ozcinar, Immersive video
technologies.
Academic Press, 2022.
[18] J. Zhang, G. Liu, D. Ding, and Z. Ma, â€œTransformer and upsampling-
based point cloud compression,â€ in Proc. Adv. Point Cloud Compress.
Process. Anal., Lisboa, Portugal, Oct. 2022, pp. 33â€“39.
[19] Y. Bo, Y. Duan, S. Shao, and M. Tao, â€œJoint coding-modulation for
digital semantic communications via variational autoencoder,â€ IEEE
Trans. Commun., vol. 72, no. 9, pp. 5626â€“5640, Sept. 2024.
[20] T.-Y. Tung, D. B. Kurka, M. Jankowski, and D. GÂ¨undÂ¨uz, â€œDeepJSCC-
Q: Constellation constrained deep joint source-channel coding,â€ IEEE J.
Select. Areas in Inf. Theory, vol. 3, no. 4, pp. 720â€“731, Dec. 2022.
[21] H. Liu, C. Li, Q. Wu, and Y. J. Lee, â€œVisual instruction tuning,â€ in Proc.
Adv. Neural Inf. Process. Syst. (NeurIPS), New Orleans, LA, USA, Dec.
2023, pp. 34 892â€“34 916.
[22] H. Xie, Z. Qin, G. Y. Li, and B.-H. Juang, â€œDeep learning enabled
semantic communication systems,â€ IEEE Trans. Signal Process., vol. 69,
pp. 2663â€“2675, Apr. 2021.
[23] Z. Weng, Z. Qin, X. Tao, C. Pan, G. Liu, and G. Y. Li, â€œDeep
learning enabled semantic communications with speech recognition and
synthesis,â€ IEEE Trans. Wireless Commun., vol. 22, no. 9, pp. 6227â€“
6240, Sept. 2023.
[24] S. Wang, J. Dai, Z. Liang, K. Niu, Z. Si, C. Dong, X. Qin, and P. Zhang,
â€œWireless deep video semantic transmission,â€ IEEE J. Select. Areas
Commun., vol. 41, no. 1, pp. 214â€“229, Jan. 2023.
[25] T. Han, K. Chi, Q. Yang, and Z. Shi, â€œSemantic-aware transmission for
robust point cloud classification,â€ in Proc. IEEE Glob. Commun. Conf.
(GLOBECOM), Kuala Lumpur, Malaysia, Dec. 2023, pp. 7617â€“7622.
[26] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, â€œPoint transformer,â€ in
Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 16 259â€“
16 268.
[27] C. Bian, Y. Shao, and D. GÂ¨undÂ¨uz, â€œWireless point cloud transmission,â€
in Proc. IEEE Int. Workshop Signal Process. Adv. Wireless Commun.
(SPAWC), Lucca, Italy, Sept. 2024, pp. 851â€“855.
[28] S. Xie, Q. Yang, Y. Sun, T. Han, Z. Yang, and Z. Shi, â€œSemantic
communication for efficient point cloud transmission,â€ in Proc. IEEE
Glob. Commun. Conf. (GLOBECOM), Cape Town, South Africa, Dec.
2024, pp. 2948â€“2953.
[29] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, â€œPointNet++: Deep hierarchical
feature learning on point sets in a metric space,â€ in Proc. Adv. Neural
Inf. Process. Syst. (NeurIPS), Long Beach, CA, USA, Dec. 2017, pp.
5099â€“5108.
[30] J. Park, Y. Oh, S. Kim, and Y.-S. Jeon, â€œJoint source-channel coding for
channel-adaptive digital semantic communications,â€ IEEE Trans. Cognit.
Comm. Netw., vol. 11, no. 1, pp. 75â€“89, Feb. 2025.
[31] L. Guo, W. Chen, Y. Sun, and B. Ai, â€œDigital-SC: Digital semantic
communication with adaptive network split and learned non-linear
quantization,â€ IEEE Trans. Cognit. Comm. Netw., vol. 11, no. 4, pp.
2499â€“2511, Aug. 2025.
[32] P. Yang, G. Zhang, and Y. Cai, â€œDigital wireless image transmission
via distribution matching,â€ in Proc. IEEE Int. Symp. Personal, Indoor &
Mobile Radio Commun. Workshops (PIMRC WKSHPS), Valencia, Spain,
Sept. 2024.
[33] G. Zhang, P. Yang, Y. Cai, Q. Hu, and G. Yu, â€œFrom analog to digital:
Multi-order digital joint coding-modulation for semantic communica-
tion,â€ IEEE Trans. Commun., vol. 73, no. 6, pp. 4257â€“4271, Jun. 2025.
[34] E. Jang, S. Gu, and B. Poole, â€œCategorical reparameterization with
gumbel-softmax,â€ in Proc. Int. Conf. Learn. Represent. (ICLR), Toulon,
France, Api. 2017.
[35] ISO/IEC JTC1/SC29/WG11, â€œCommon test conditions for point cloud
compression,â€ MPEG, Tech. Rep. N19084, Feb. 2020.
[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, â€œPointNet: Deep learning on
point sets for 3D classification and segmentation,â€ in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recog. (CVPR), Honolulu, HI, USA, Jul.
2017, pp. 214â€“229.
[37] M. Yang and H.-S. Kim, â€œDeep joint source-channel coding for wireless
image transmission with adaptive rate control,â€ in Proc. IEEE Int. Conf.
Acoust., Speech Signal Process. (ICASSP).
Singapore: IEEE, May.
2022, pp. 5193â€“5197.
[38] L. Wiesmann, A. Milioto, X. Chen, C. Stachniss, and J. Behley, â€œDeep
compression for dense point cloud maps,â€ IEEE Robot. Autom. Lett.,
vol. 6, no. 2, pp. 2060â€“2067, Feb. 2021.
[39] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
S. Savarese, M. Savva, S. Song, H. Su et al., â€œShapeNet: An information-
rich 3D model repository,â€ arXiv preprint arXiv:1512.03012, Dec. 2015.
[40] A. Hamdi, F. AlZahrani, S. Giancola, and B. Ghanem, â€œMVTN: Learn-
ing multi-view transformations for 3D understanding,â€ Int. J. Comput.
Vis., vol. 133, no. 4, pp. 2197â€“2226, Nov. 2024.
[41] J. Wang, D. Ding, Z. Li, and Z. Ma, â€œMultiscale point cloud geometry
compression,â€ in Proc. IEEE Data Compress. Conf. (DCC).
Snowbird,
UT, USA: IEEE, May. 2021, pp. 73â€“82.
[42] J. Hoydis, S. Cammerer, F. A. Aoudia, A. Vem, N. Binder, G. Marcus,
and A. Keller, â€œSionna: An open-source library for next-generation
physical layer research,â€ arXiv preprint arXiv:2203.11854, Mar. 2022.
[43] G. Forney, R. Gallager, G. Lang, F. Longstaff, and S. Qureshi, â€œEfficient
modulation for band-limited channels,â€ IEEE J. Select. Areas Commun.,
vol. 2, no. 5, pp. 632â€“647, Jan. 2003.
[44] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
and J. Gall, â€œSemanticKITTI: A dataset for semantic scene understand-
ing of LiDARg sequences,â€ in Proc. IEEE/CVF Int. Conf. Comput. Vis.
(ICCV), Seoul, South Korea, Oct. 2019, pp. 9297â€“9307.
