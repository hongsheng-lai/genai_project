AVATAAR: Agentic Video Answering via Temporal
Adaptive Alignment and Reasoning
Accepted in the 5th IEEE Big Data Workshop on Multimodal AI (MMAI 2025), Preprint Copy
Dec 8-11, Macau, China, 2025
Fang-Chun Yeh
Ratings Data Science
S&P Global
New York, USA
jessie.yeh@spglobal.com
Urjitkumar Patel
Ratings Data Science
S&P Global
New York, USA
urjitkumar.patel@spglobal.com
Chinmay Gondhalekar
Ratings Data Science
S&P Global
New York, USA
chinmay.gondhalekar@spglobal.com
Abstract—With the increasing prevalence of video content,
effectively understanding and answering questions about long
form videos has become essential for numerous applications.
Although large vision language models (LVLMs) have enhanced
performance, they often face challenges with nuanced queries
that demand both a comprehensive understanding and detailed
analysis. To overcome these obstacles, we introduce AVATAAR, a
modular and interpretable framework that combines global and
local video context, along with a Pre Retrieval Thinking Agent and
a Rethink Module. AVATAAR creates a persistent global summary
and establishes a feedback loop between the Rethink Module and
the Pre Retrieval Thinking Agent, allowing the system to refine
its retrieval strategies based on partial answers and replicate
human-like iterative reasoning. On the CinePile benchmark,
AVATAAR demonstrates significant improvements over a baseline,
achieving relative gains of +5.6% in temporal reasoning, +5% in
technical queries, +8% in theme-based questions, and +8.2%
in narrative comprehension. Our experiments confirm that
each module contributes positively to the overall performance,
with the feedback loop being crucial for adaptability. These
findings highlight AVATAAR’s effectiveness in enhancing video
understanding capabilities. Ultimately, AVATAAR presents a
scalable solution for long-form Video Question Answering (QA),
merging accuracy, interpretability, and extensibility.
Index Terms—Video Question Answering, Agentic Reasoning,
Global and Local Context, Retrieval Augmented Generation,
Large Vision Language Models
I. INTRODUCTION
Video is a crucial medium for sharing information, sup-
porting a wide range of applications from online education
and entertainment to surveillance and autonomous systems.
Consequently, video question answering (VQA) has become
a key challenge in artificial intelligence, requiring systems to
interpret complex visual and textual cues that are spread across
time [1].
Recent work has explored various strategies to advance
video understanding, including frame aggregation for improved
action recognition, multimodal embedding models for better
vision language alignment, and video augmentation techniques.
Multi-agent architectures have also been proposed [2], where
specialized modules for different modalities collaborate to
generate answers. Another promising direction involves training
larger foundation models [3], although these approaches are
often computationally intensive and challenging to scale [4].
Despite this progress, many existing systems continue to
struggle with limited context windows, a lack of persistent
memory, and static retrieval mechanisms. These constraints
hinder their ability to handle complex, multi step reasoning
over long videos. To address these challenges, we introduce
AVATAAR, a flexible framework that integrates global video
summaries, dynamic query refinement via a Pre Retrieval
Thinking Agent, and iterative feedback through a Rethink
Module. This design enables more adaptable, context aware
inference across diverse question types and domains, going
beyond static architectures and fixed retrieval strategies.
As shown in Figure 2, AVATAAR starts by breaking the
video into segments that are easy to manage, keeping in mind
the context window limits of LVLMs. Next, it builds a global
summary that highlights the main storyline, important events,
and interactions between characters throughout the entire video.
This summary acts as a lasting memory, helping to guide the
reasoning process that follows.
When a user poses a question, the Pre Retrieval Thinking
Agent leverages the global summary, along with prior feedback
from the Rethink Module (if available), to determine the
appropriate reasoning steps, including triggering function calls.
It then refines and expands the query by injecting relevant
context and disambiguating intent, ensuring that retrieval
focuses on the most salient local segments, specific frames,
and transcripts aligned with the user’s informational needs.
If the LVLM’s initial response is incomplete or ambiguous,
the Rethink Module re-analyzes the question, global context,
retrieved evidence, and the generated answer to diagnose
potential gaps. It then issues targeted feedback to the Pre
Retrieval Thinking Agent, prompting further query refinement
or an updated retrieval strategy. This feedback loop contin-
ues for a fixed number of iterations or until a satisfactory
answer is produced, mimicking human-like reasoning through
progressive, context aware inference.
We evaluate AVATAAR on CinePile [5], demonstrating that
our architectural and data processing enhancements substan-
tially improve performance on challenging video QA tasks,
arXiv:2511.15578v1  [cs.CV]  19 Nov 2025


Fig. 1. Traditional video question answering pipeline
especially those requiring nuanced spatio-temporal reasoning
and multi-step evidence integration.
Our main contributions are as follows:
• We present a modular framework for long-form video QA
that combines global summarization, local retrieval, and
iterative reasoning to improve accuracy and interpretability
over a traditional video chat baseline.
• We describe a dynamic summary generation module using
LVLMs, which adaptively selects frames and transcriptions
to create a persistent global context for video QA.
• A Pre Retrieval Thinking Agent that refines user queries
using global context and prior feedback, enabling more
precise retrieval, especially for complex, entity-based, or
quantitative questions that are difficult for static RAG
pipelines.
• A Rethink Module that diagnoses gaps in earlier answers
and issues targeted instructions to guide the Pre Retrieval
Agent for the next round of retrieval and reasoning,
yielding a simple form of agentic RAG.
• We show that incorporating timestamp information into
retrieval and reasoning modules significantly improves
performance on temporally grounded questions on the
CinePile benchmark, and we discuss the computational
and latency trade-offs introduced by AVATAAR’s iterative
design.
II. RELATED WORK
A. Multimodal Embedding Models
Aligned vision language spaces are central to contemporary
video QA. Dual encoder foundations such as CLIP [6] and
ALIGN [7] project images (or individual frames) and text
into a shared latent space, making them natural backbones
for this task. BridgeTower [8] generalizes this approach by
inserting bridge layers that couple intermediate features from
pretrained encoders using CLIP for the visual stream and
RoBERTa [9] for the language stream to enable fine grained
semantic correspondence. In video QA practice, these models
are adapted by temporally aggregating per frame embeddings
and aligning them with transcripts or with shot level segments.
B. Large Vision Language Models for Video
Large Vision Language Models (LVLMs) have been pivotal
in advancing multimodal reasoning for video understanding
tasks. Early approaches such as Flamingo [10] and BLIP 2 [11]
were among the first to leverage vision language pretraining for
open-ended video question answering and temporal comprehen-
sion. Building on this foundation, instruction tuned models like
InternVideo [12], Video LLaMA [13], and Video LLaVA [14]
have demonstrated improved performance in event recognition
and detailed visual text alignment by densely associating frames
and transcripts.
The field has recently seen the emergence of more general
purpose and proprietary LVLMs from leading research organi-
zations. Models such as GPT 4o from OpenAI [15], Gemini 2.5
from Google DeepMind [3], Claude 3 Opus from Anthropic
[16], and Meta’s LLaMA 3 [17] and Chameleon [18] extend
multimodal reasoning capabilities to the unified processing of
images, videos, and audio.
Nevertheless, the majority of LVLMs continue to face
challenges, such as fixed input context windows and the absence
of mechanisms for dynamic context management or iterative
reasoning, which can hinder their effectiveness on long form
video QA.
C. Retrieval Augmented and Agentic Reasoning
Retrieval Augmented Generation (RAG) anchors generated
answers in evidence retrieved from a corpus [19], [20]. Classic
RAG pipelines are single shot: a fixed query is issued once to a
fixed retriever. Agentic variants depart from this by introducing


Fig. 2. AVATAAR: Think, Retrieve, Rethink - Agentic Video QA Framework
multi-turn planning and self reflection. Reflexion [21] equips
agents to critique prior outputs and revise them through
memory updates. A recent survey [22] systematizes this design
space, emphasizing tool use, iterative query reformulation, and
modular planning capabilities crucial for ambiguous, multi step
problems. Modular RAG frameworks have also been explored
in domain specific settings such as cyber [23] and finance [24],
[25].
In video QA, retrieval must respect the spatio-temporal
structure. VideoRAG [26] fuses structured concept graphs with
visual descriptors to enable frame level lookup. Hierarchical
alignment methods (e.g., Dig into Multi modal Cues [27])
tailor layered alignment to strengthen video-to-text retrieval.
VideoMultiAgents [2] further distributes reasoning across
modality specialist agents under a centralized coordinator.
Despite these advances, many systems still lack long lived
memory, run-time adaptive retrieval policies, and feedback
driven refinement. AVATAAR closes these gaps by combining
a global video summary, a Pre Retrieval Thinking Agent, and
a Rethink Module within an iterative reasoning loop, yielding
more context aware, human-like inference on long-form video
content.
III. METHODS
The frameworks for video question answering in this study: a
baseline pipeline that uses traditional RAG, and AVATAAR, our
advanced agentic architecture that integrates global and local
context, dynamic query refinement, and iterative reasoning.
Below, we describe the methodology and components of both
frameworks in detail.
A. Baseline Framework
As depicted in Figure 1, the basic framework for video
question answering consists of three primary stages: data
preprocessing, retrieval, and answer generation.
1) Data Pre-processing: We pre-process raw videos with au-
tomated Python tooling. Whisper [28] produces a time stamped
sequence of transcriptions Tj, where each Tj corresponds to
segment j. In parallel, OpenCV (CV2) samples frames every
y seconds to obtain Fi. Temporal alignment of these two
streams yields synchronized visual and textual signals, enabling
coherent analysis and remaining robust even when audio tracks
are absent.
2) Embedding and Retrieval: Video frames and transcript
segments are encoded into a shared z-dimensional space using
a CLIP vision encoder and a compatible text encoder. Each
video frame Fi and transcript segment Tj is mapped into the
same z-dimensional embedding space:
Embfi = Embedimg(Fi) ∈Rz,
Embtj = Embedtxt(Tj) ∈Rz,
Embq = Embedtxt(q) ∈Rz
(1)
where Embfi, Embtj, and Embq denote the embeddings of
frames, transcripts, and the user query, respectively.
To retrieve the most relevant frames, we compute the cosine
similarity between the query embedding Embq and each frame
embedding Embfi:
sim(Embq, Embfi) =
Embq · Embfi
∥Embq∥∥Embfi∥
(2)
The indices of the top n relevant frames are:
Iframe
top n =
arg max
I⊂{1,...,N},|I|=n
X
i∈I
sim(Embq, Embfi)
(3)


TABLE I
FUNCTION SCHEMA FOR THE PRE RETRIEVAL AGENT
Function Name
Purpose
expand_query
Adds missing details, such as character descriptions, to enrich the query.
extract_temporal_anchors
Returns timestamps of retrieved frames or estimates time based cues from the video summary.
term_frequency
Counts how many times a term shows up in the transcription.
get_action_before_an_event
Suggests actions by inputting six consecutive frames before the event timestamp to the LVLMs.
get_action_after_an_event
Suggests actions by inputting six consecutive frames after the event timestamp to the LVLMs.
web_search
Search web for unfamiliar terms in user query.
multi_hop_query_generation
Break down a complex question into a sequence of simpler sub queries, retrieve for each, and
aggregate the results.
Similarly, for transcriptions, we compute:
sim(Embq, Embtj) =
Embq · Embtj
∥Embq∥∥Embtj∥
(4)
and select the indices of the top n relevant transcriptions:
Jtrans
top n =
arg max
J⊂{1,...,M},|J|=n
X
j∈J
sim(Embq, Embtj)
(5)
The retrieved frames and transcriptions are then:
{Fi | i ∈Iframe
top n },
{Tj | j ∈Jtrans
top n}
(6)
3) Answer Generation: The selected image transcription
pair (Fi∗, Tj∗), together with the user query q, is then passed
to an LVLM, denoted as function A, to generate the answer:
Answer = A(Fi∗, Tj∗, q)
(7)
where A(·) denotes the LVLM’s answer generation operator,
which leverages both the visual and textual context to produce
a natural language response.
While this baseline approach effectively integrates multi-
modal information for basic video QA tasks, it is limited by
its reliance on static retrieval and shallow context aggregation.
These constraints motivate the development of more advanced
architectures, as described in subsequent sections.
B. AVATAAR Framework
Building on conventional video QA pipelines, we introduce
AVATAAR, a framework that explicitly models a think–retrieve
loop and triggers a rethinking stage when an initial answer is
inadequate. As illustrated in Figure 2, AVATAAR comprises
three core components:
1) Global and Local Context Assisted Retrieval
2) Pre Retrieval Thinking Agent
3) Rethink Module
The overall architecture is shown in Figure 2; we describe
each component in detail below.
1) Global and Local context Assisted Retrieval:
In
AVATAAR, we explicitly separate global and local context,
each serving a distinct role in video understanding. The global
context is a video level summary computed once per video
and cached as a persistent reference. It captures overarching
themes, principal characters, and salient background details,
providing a stable foundation that remains consistent across
all subsequent user queries.
By contrast, the local context is assembled on demand
for each question. It consists of the most relevant frames
and transcript spans retrieved conditionally on the query, and
therefore changes dynamically from one query to the next.
This query tailored evidence focuses the system on precisely
the information needed to answer accurately, while the global
summary anchors interpretation and maintains coherence across
interactions.
Dynamic Summary Generation (Global Context):
Leveraging LVLMs, we generate a comprehensive summary
of the video by aligning extracted frames with transcriptions,
clustering content by topic, and describing key characters and
backgrounds. This summary encapsulates the global context
and serves as a persistent reference for all downstream queries,
as depicted in the upper section of Figure 2.
To accommodate the limited context window of the LVLM,
we partition the video’s transcriptions and corresponding
frames into batches, where each batch fits within the model’s
maximum input capacity. For each batch, the LVLM generates a
partial summary, including topic clusters, character descriptions,
and background information. These partial summaries are
then aggregated to form the global summary for the entire
video. This batching strategy ensures that even for long
videos exceeding the LVLM’s native context window, the
framework can systematically process all available content
without information loss, maintaining coherence across the
global summary.
The prompt template used to generate the summary for each
batch is as follows:
You are provided with a transcript in WEBVTT format and
a set of frames extracted from the corresponding video.
Please perform the following tasks:
Topic Clustering: Analyze the captions and cluster them


TABLE II
QUESTION TYPES IN THE CINEPILE DATASET [5]
#
Question Type
Description
Example
Count
1
Character and Relation-
ship Dynamics
Templates about characters’ actions, motivations,
and relationships.
How are Darren and Reed related
to each other?
2068
2
Narrative and Plot Anal-
ysis
Templates focused on the movie’s storyline, plot
twists, and narrative structure.
What does Peter notice about
Edgar during their interaction?
463
3
Setting and Technical
Analysis
Templates about the movie’s setting, environment,
technical aspects, and object usage.
What does Reed do after opening
the door?
1521
4
Temporal
Templates assessing understanding of a movie’s
timing and sequence of events.
How many times does Darren say
"Ow!"?
683
5
Theme Exploration
Templates about the movie’s themes, symbols,
motifs, subtext, and emotional or moral elements.
How does the emotional tone tran-
sition during the scene?
189
into coherent topics.
For each cluster, provide:
The start and end timestamps covered by the topic.
The start_time of each topic cluster must be exactly
equal to the end_time of the previous topic cluster,
except for the first cluster.
A brief topic title and a short summary.
Character Description: Identify any characters
mentioned or appearing in each topic cluster. For each
character, provide a brief description based on both
the transcript and the video frames (e.g., appearance,
attire, actions, emotions). Ensure that the character’s
name appears in the topic title or summary for clusters
where they are relevant.
Frame References: For each topic cluster, reference the
specific video frames that correspond to the start and
end timestamps. If a character is described, refer to
the frame(s) where their appearance or actions are most
clearly depicted.
Output Format: For each topic cluster, include:
Start timestamp
End timestamp
Topic Title (include character names if relevant)
Short summary of the topic
Character Descriptions
Background Descriptions
Referenced Video Frames
2) Pre Retrieval Thinking Agent: The Pre Retrieval Thinking
Agent is an agent that receives three types of input: the user
query q, the global context summary, and instructions from the
Rethink Module. Leveraging these inputs, the agent examines
the user query alongside the descriptions of available functions
{f1, f2, . . . , fn}, and, using its own language understanding ca-
pabilities, dynamically selects and applies the most appropriate
specialized functions to refine the query.
Let c denote the set of contextual information or outputs
fetched by the selected function callings. The refined query qr
is constructed by augmenting the original query q with c:
qr = Refine(q, c)
(8)
where Refine(·) denotes the process of incorporating additional
context or information into the original query.
The refined query qr is then embedded as:
Embqr = Embedtxt(qr) ∈Rz
(9)
Given sets of frame embeddings {Embfi} and transcription
embeddings {Embtj}, the most relevant frame and the most
relevant transcription are retrieved independently:
i∗= arg max
i
sim(Embqr, Embfi)
(10)
j∗= arg max
j
sim(Embqr, Embtj)
(11)
where sim(·, ·) denotes the cosine similarity:
sim(Embqr, x) =
Embqr · x
∥Embqr∥∥x∥
(12)
with x representing either a frame embedding Embfi or a
transcription embedding Embtj.
To provide local context for the LVLM, we select a window
of frames centered at i∗:
Flocal = {Embfi∗−1, Embfi∗, Embfi∗+1}
(13)
For transcriptions, we retrieve the top n most relevant
segments based on similarity to Embqr:
Jtrans
top n =
arg max
J⊂{1,...,M},|J|=n
X
j∈J
sim(Embqr, Embtj)
(14)
The corresponding set of most relevant transcriptions is:
Tlocal = {Tj | j ∈Jtrans
top n}
(15)
This approach ensures that the LVLM receives both the
most contextually relevant frames (as a local temporal window)
and the top n most pertinent transcript segments, along with
the refined query qr and any other contextual cues to support
robust answer generation.
The agent’s design is flexible: it can be adapted to select from
a variety of function calls and can even be extended to interact
with other agents or external sources, such as web search,
depending on the needs of the application domain. For the
experiments in this work, we focused on five core functions
(see Table I) to evaluate our methodology on the CinePile
dataset. The agent selects which function to use based on its
interpretation of the input query and the available function
descriptions. For instance, when asked, "How many times did
the character say ’fishing rod’?", the agent recognizes that this


is a counting task and invokes the term frequency function to
tally the occurrences of the phrase in the transcript.
Likewise, if a user query lacks specificity, the agent can
employ the query expansion function to enrich the question
with additional details from the global context. For example,
"How does Yogi’s mood change during the scene?" could be
expanded to "How does Yogi (the bear wearing a green hat
and white collar)’s mood change during the scene?", making
the retrieval process more precise and targeted.
3) Rethink Module: The Rethink Module is responsible for
diagnosing and addressing cases where the LVLM’s response
indicates insufficient information to answer the user’s query. It
receives five types of input: (1) the user query, (2) expanded
information from the Pre Retrieval Thinking Agent, (3) the
global context summary, (4) the local context used (retrieved
frames and transcriptions), and (5) the LVLM generated answer.
Leveraging these inputs, the Rethink Module analyzes the
reasoning process and identifies gaps or ambiguities that
prevent the LVLM from producing a satisfactory answer. It then
generates targeted instructions for the Pre Retrieval Thinking
Agent to refine the query or retrieval process in subsequent
iterations.
For example, if the user asks, “What is the correct sequence
of events in the scene?” and the LVLM generated answer lacks
temporal specificity, the Rethink Module may determine that the
missing information is the exact timing of when “Ranger Smith
confronts Yogi” occurs. In response, it issues an instruction such
as “retrieve timestamps for the confrontation event,” prompting
the Pre Retrieval Thinking Agent to invoke the temporal
anchor extraction function. This iterative process continues
for a predefined number of cycles or until a satisfactory answer
is produced. By systematically diagnosing information gaps
and orchestrating targeted retrieval strategies, the Rethink
Module embodies an agentic RAG approach that enhances
the robustness and accuracy of the overall framework.
In practice, we cap the number of rethink iterations at x = 3
per question. The Rethink Module is prompted to review when
we do not have enough information. This policy bounds the
overhead of the iterative loop. The modest yet consistent gains
from the Rethink Module therefore come at a controllable
cost that can be tuned or disabled entirely in latency sensitive
deployments.
IV. EVALUATION
A. Video QA Dataset
Selecting the evaluation corpus is pivotal for Video QA, as
datasets differ markedly in their coverage of question categories,
answer modalities, and annotation rigor. We surveyed widely
used benchmarks: ActivityNetQA [29], NExT QA [30], and
CinePile [5] against three criteria:
1) Breadth across question types (temporal, attribute, narra-
tive, thematic);
2) Answer modality (open-ended vs. multiple choice);
3) Practicality and reliability of automatic validation.
Existing benchmarks such as ActivityNetQA and NExT-QA
emphasize open-ended activity recognition and causal/temporal
reasoning, but their free-form answers make automatic evalua-
tion noisier and narrow their question coverage for narrative
and thematic analysis.
By contrast, CinePile offers a balanced distribution of
temporal, attribute, narrative, and thematic questions, all
delivered as multiple choice. This design substantially reduces
ambiguity during validation and supports a more reliable,
objective evaluation. CinePile also features diverse video
content and careful annotation; accordingly, we adopt it for
our Video QA benchmarking. For definitions and examples of
each question type, see Table II.
B. Evaluation Metrics
We evaluate the different framework designs using two
standard metrics: accuracy and F1 score. Accuracy captures
the overall proportion of correct predictions, providing a
straightforward measure of performance. The F1 score balances
precision and recall, making it especially useful when the
dataset is uneven across classes. Together, these metrics provide
a reliable view of both general correctness and performance
under imbalance, and we report both in all experiments.
Evaluation protocol: CinePile is a multiple choice bench-
mark: each question q is paired with five candidate answers
and we add one more answer option making total 6 choices
{a1, . . . , a6}. At inference time, we prompt the LVLM to
output only a single option label (1, 2, 3, 4, 5 , or 6),
which we deterministically map to the corresponding ans index.
Accuracy is computed as the proportion of questions where the
predicted index matches y⋆. F1 is computed over the induced
six way classification problem and macro-averaged across
answer options. The model is constrained to select from the
finite choice set, there is no ambiguity in mapping generated
outputs to ground truth labels.
C. Component Impacts
To assess the effectiveness of each component in the
AVATAAR framework, we compare four system variants: (1)
the baseline video chat framework without any AVATAAR
components, (2) the baseline enhanced with the global context
summary, (3) the baseline further augmented with the Pre
Retrieval Thinking Agent, and (4) the complete AVATAAR
framework, which also includes the Rethink Module. All
models are evaluated on the same test split of the CinePile
dataset, with accuracy serving as the primary metric. All
experiments use the same models and model configurations to
ensure a fair comparison.
Adding the Pre Retrieval Thinking Agent yields further
accuracy gains across all categories, such as increases of 3.7%
(from 51.3% to 55.0%) for Theme Exploration and 5.6% (from
37.4% to 43.0%) for Narrative. Although these improvements
are moderate, they demonstrate that dynamic function selection
and query refinement enhance AVATAAR’s ability to handle
tasks requiring more complex reasoning.


TABLE III
COMPARISON OF ACCURACY (%) ACROSS SYSTEM VARIANTS AND QUESTION TYPES.
System Variant
Theme
Exploration
Narrative and
Plot Analysis
Character and
Relationship
Dynamics
Setting and
Technical
Analysis
Temporal
Baseline
47.6
36.5
33.1
21.6
19.9
+ Global Context Summary
51.3
37.4
35.1
23.2
23.1
+ Pre Retrieval Thinking Agent
55.0
43.2
38.4
26.2
24.9
+ Rethink Module (AVATAAR)
55.6
44.7
39.2
26.6
25.5
Fig. 3. F1 Score by System Variant and Category
Table III shows that adding global context yields consistent
gains over the baseline, particularly for Temporal and Theme
Exploration questions. The Pre Retrieval Thinking Agent further
improves all categories, and the full AVATAAR system with
the Rethink Module brings modest but consistent additional
improvements (0.4 – 1.7 percentage points). Figure 3 confirms
the same trend in terms of F1 scores across question types.
To give a broader evaluation of our different system setups,
we also looked at F1 scores, as you can see in Figure 3. Using
F1 scores helps paint a fuller picture of how the systems
perform, especially when there are class imbalances or when
answers are not completely correct. Our findings show that
AVATAAR generally does better than the baseline models,
particularly when it comes to Theme Exploration, a category
where understanding themes and emotional nuances is key.
Interestingly, even though Theme Exploration has the least
number of questions, just 189 per Table II, we saw quite a bit
of improvement there, which suggests AVATAAR can handle
abstract concepts pretty well.
On the flip side, Figure 3 also highlights how Temporal
questions, crucial for understanding sequences in videos (with
683 questions according to the table), remain tricky. Still, the
F1 scores back up the accuracy findings, giving us confidence
in the results. These tests show that the different parts of
AVATAAR, like how it models global context and refines
queries, play their roles in boosting video Q&A. Our setup
does show improvements over older methods, especially for
Theme Exploration and Narrative questions, with certain gains
around 8.0%.
Although recent models like Gemini 1.5 Pro and GPT 4o
have achieved higher performance than our framework on the
CinePile benchmark (as reported in the CinePile paper [5]),
our goal is not to introduce the absolute best performing video
understanding model. Instead, we aim to present a flexible
framework that can be deployed or built with any multimodal
models, including those that may become available in the
future. AVATAAR is specifically designed for enterprise use on
proprietary infrastructure, which allows organizations to avoid
reliance on external APIs whose internal mechanisms may be
opaque or outside their control.
V. CONCLUSION
In this work, we introduced AVATAAR, a modular and
comprehensible framework designed for long-form video
question answering that prioritizes retrieval augmented, agentic
reasoning over simply scaling up models. By clearly distin-
guishing global summarization from local evidence retrieval
and integrating an iterative feedback loop between the Pre
Retrieval Thinking Agent and the Rethink Module, AVATAAR
enables more context aware and precise responses across a
wide array of complex video queries. Our evaluation of the
CinePile dataset shows notable improvements, especially in
addressing intricate spatio-temporal and narrative questions.
AVATAAR stands out as a transparent and flexible alternative
to proprietary video QA systems, offering enterprises greater
adaptability and control over their applications. We are confi-
dent that the architectural innovations provided by AVATAAR
establish a robust foundation for the future of multimodal
video understanding. This framework is poised to facilitate not
just efficient adaptation to diverse domains but also to ensure
strong performance in real-world, content heavy scenarios. Its
design supports further innovation, making it a valuable tool
for ongoing developments in the field.


REFERENCES
[1] Y. Zhu, X. Li, C. Liu, M. Zolfaghari, Y. Xiong, C. Wu, Z. Zhang, J. Tighe,
R. Manmatha, and M. Li, “A comprehensive study of deep video action
recognition,” 2020. [Online]. Available: https://arxiv.org/abs/2012.06567
[2] N. Kugo, X. Li, Z. Li, and A. G. et al., “Videomultiagents: A
multi-agent framework for video question answering,” 2025. [Online].
Available: https://arxiv.org/abs/2504.20091
[3] G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva,
I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, L. Marris,
S. Petulla, C. Gaffney, A. Aharoni, N. Lintz, T. C. Pais, H. Jacobsson,
I. Szpektor, N.-J. Jiang, K. Haridasan, A. Omran, N. Saunshi, D. Bahri,
G. Mishra, E. Chu, T. Boyd, B. Hekman, A. Parisi, C. Zhang,
K. Kawintiranon, T. Bedrax-Weiss, O. Wang, Y. Xu, and O. Purkiss,
“Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality,
long context, and next generation agentic capabilities,” 2025. [Online].
Available: https://arxiv.org/abs/2507.06261
[4] R. Bommasani, D. A. Hudson, E. Adeli, and R. A. et al., “On the
opportunities and risks of foundation models,” 2022. [Online]. Available:
https://arxiv.org/abs/2108.07258
[5] R. Rawal, K. Saifullah, R. Basri, D. Jacobs, G. Somepalli, and
T. Goldstein, “Cinepile: A long video question answering dataset and
benchmark,” arXiv preprint arXiv:2405.08813, 2024.
[6] A. Radford, J. W. Kim, C. Hallacy et al., “Learning transferable visual
models from natural language supervision,” in ICML, 2021.
[7] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H.
Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language
representation learning with noisy text supervision,” in International
conference on machine learning.
PMLR, 2021, pp. 4904–4916.
[8] H. Xu, Z. Gan, Y. Chen et al., “Bridgetower: Building bridges between
encoders in vision-language representation learning,” in NeurIPS, 2022.
[9] Y. Liu, M. Ott, N. Goyal et al., “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.
[10] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, and I. B. et al., “Flamingo:
a visual language model for few-shot learning,” 2022. [Online].
Available: https://arxiv.org/abs/2204.14198
[11] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language
models,” 2023. [Online]. Available: https://arxiv.org/abs/2301.12597
[12] W. Wang, Y. Lin, X. Zhang et al., “Internvideo: General video foundation
models via generative and discriminative learning,” in CVPR, 2024.
[13] S. Zhao et al., “Video-llama: An instruction-tuned audio-visual language
model for video understanding,” arXiv preprint arXiv:2306.02858, 2023.
[14] H. Zhang et al., “Video-llava: Learning united visual representation for
video understanding with llms,” arXiv preprint arXiv:2311.16502, 2023.
[15] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh,
A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, A. M ˛adry,
A. Baker-Whitcomb, and A. Beutel, “Gpt-4o system card,” 2024.
[Online]. Available: https://arxiv.org/abs/2410.21276
[16] Anthropic, “Claude 3 model card,” 2024, accessed: 2025-08-01. [Online].
Available: https://www.anthropic.com/news/claude-3-model-card
[17] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, and
A. Al-Dahle, “The llama 3 herd of models,” 2024. [Online]. Available:
https://arxiv.org/abs/2407.21783
[18] Chameleon, “Chameleon: Mixed-modal early-fusion foundation models,”
2025. [Online]. Available: https://arxiv.org/abs/2405.09818
[19] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,
H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel et al., “Retrieval-
augmented generation for knowledge-intensive nlp tasks,” in Advances
in Neural Information Processing Systems, 2020.
[20] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and
W.-t. Yih, “Dense passage retrieval for open-domain question answering,”
in Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2020.
[21] M. Shinn, E. Wallace, J. Ling, D. Dohan, E. Akyürek, J. Austin, T. Brown,
D. Zhou, W. W. Cohen, and K. Guu, “Reflexion: Language agents with
verbal reinforcement learning,” arXiv preprint arXiv:2303.11366, 2023.
[22] A. Singh, A. Xu et al., “A survey on agentic retrieval-augmented
generation,” arXiv preprint arXiv:2401.10536, 2025.
[23] U. Patel, F. Yeh, and C. Gondhalekar, “Canal – cyber activity news
alerting language model: Empirical approach vs. expensive LLM,” arXiv
preprint arXiv:2405.06772, 2024.
[24] U. Patel, F. Yeh, C. Gondhalekar, and H. Nalluri, “Fanal – financial
activity news alerting language modeling framework,” arXiv preprint
arXiv:2412.03527, 2024.
[25] C. Gondhalekar, U. Patel, and F. Yeh, “MultiFinRAG: An optimized mul-
timodal retrieval-augmented generation (RAG) framework for financial
question answering,” arXiv preprint arXiv:2506.20821, 2025.
[26] L. Chen et al., “Videorag: Retrieval-augmented video question answering
at scale,” arXiv preprint arXiv:2402.02267, 2024.
[27] W. Wang, M. Zhang, R. Chen, and G. C. et al., “Dig into
multi-modal cues for video retrieval with hierarchical alignment,” in
Proceedings of the Thirtieth International Joint Conference on Artificial
Intelligence (IJCAI-21), 2021, pp. 1110–1116. [Online]. Available:
https://www.ijcai.org/proceedings/2021/0154.pdf
[28] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I. Sutskever, “Robust speech recognition via large-scale weak supervision,”
arXiv preprint arXiv:2212.04356, 2022.
[29] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao, “Activitynet-
qa: A dataset for understanding complex web videos via question
answering,” 2019. [Online]. Available: https://arxiv.org/abs/1906.02467
[30] J. Xiao, X. Shang, A. Yao, and T.-S. Chua, “Next-qa: Next phase of
question-answering to explaining temporal actions,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2021, pp. 9777–9786.
