First Frame Is the Place to Go for Video Content Customization
Jingxi Chen1∗†, Zongxia Li1∗†, Zhichao Liu1, Guangyao Shi2, Xiyang Wu1, Fuxiao Liu1,
Cornelia Fermüller1, Brandon Y. Feng3†, Yiannis Aloimonos1
1University of Maryland
2University of Southern California
3Massachusetts Institute of Technology
http://firstframego.github.io
First Frame
Robot Manipulation
First Frame
Aerial View Simulation
First Frame
Multi-product Demonstration
First Frame
Driving Simulation
First Frame
Filmmaking
Figure 1. FFGo is a lightweight add-on that invokes the innate capabilities of pre-trained video generation models, such as Wan2.2 [30], to
treat the first frame as a compositional blueprint, enabling natural subject mixing and interaction throughout the video. Given a single input
image with multiple elements and a guiding text prompt, FFGo generates coherent, customized videos across diverse applications including
robotic manipulation, driving/aerial/underwater simulation, multi-product demonstration, and filmmaking. It requires no architectural
modifications, and achieves strong subject-mixing performance with just 20–50 LoRA fine-tuning examples.
Abstract
What role does the first frame play in video generation
models? Traditionally, it’s viewed as the spatial-temporal
starting point of a video, merely a seed for subsequent an-
imation. In this work, we reveal a fundamentally different
perspective: video models implicitly treat the first frame as
a conceptual memory buffer that stores visual entities for
*Equal contributions † Corresponding author.
later reuse during generation. Leveraging this insight, we
show that it’s possible to achieve robust and generalized
video content customization in diverse scenarios, using only
20–50 training examples without architectural changes or
large-scale finetuning. This unveils a powerful, overlooked
capability of video generation models for reference-based
video customization.
1
arXiv:2511.15700v1  [cs.CV]  19 Nov 2025


1. Introduction
Recent advances in video generation models [1, 2, 4, 21, 26,
30, 34, 39–41] have made them powerful tools for content
creation, filmmaking, simulation, and other applications in-
volving creative visual experiences. A key application of
these models is reference-based video generation, where one
or more reference inputs are used to compose and synthesize
visually consistent videos. This capability is essential for
real-world scenarios such as film production and simulation,
where customization and controllability are critical. Unlike
standard Text-to-Video (T2V) models that rely solely on
textual prompts, reference-based generation allows users to
guide video synthesis through visual references, enabling
finer control over generated video contents to be customized
for the specific user guidance.
The simplest case is using a single reference image,
known as the Image-to-Video (I2V) paradigm. I2V models
animate a given image to generate videos that maintain vi-
sual consistency with the reference content. While effective
as a basic form of video customization, this single-image
setup constrains both the spatial diversity and content compo-
sition of the generated video. To overcome these limitations,
recent research has focused on developing multi-reference
video generation models [5, 7, 13, 17, 31], which can incor-
porate multiple reference images to achieve richer and more
flexible video content customization.
Existing multi-reference video generation models gener-
ally follow two strategies: 1) Architectural modification of
pre-trained video generation models to accommodate ad-
ditional reference inputs.
2) Fine-tuning on large-scale,
task-specific video customization datasets, such as insert-
ing humans into animations or product demonstration videos.
However, fine-tuning often leads to performance degradation
and loss of generalization in the adapted models. Since video
customization typically occurs post-training, the diversity
and quality of the adaptation datasets are far lower than those
used during pre-training. Consequently, fine-tuned models
tend to overfit to specific customization scenarios, forget-
ting the broad generative priors learned during pre-training,
effectively transforming once generalist video models into
narrow, task-specialized systems.
Is it possible to incorporate content from multiple refer-
ence images into pre-trained video generation models with-
out modifying their architecture or relying on large-scale
video training datasets?
To address this question, we investigate existing video
generation models and uncover a previously overlooked yet
fundamental capability: their innate ability to incorporate
multiple reference concepts into the generation process with-
out any architectural modifications or large-scale fine-tuning.
As in Figure 2, standard video generation models have po-
tentials to naturally embed visual concepts from multiple
references into the first frame, functioning as a memory
buffer, and then fuse them consistently through scene tran-
sitions during generation. However, this ability is difficult
to invoke directly: prompt engineering alone often leads to
unstable outcomes and struggles to precisely preserve object
identities or control visual composition. To transform this
observation into a practical capability for multi-reference-
based video customization, we propose a simple yet effective
approach that reliably activates this innate ability. Using only
20-50 training video examples for lightweight training, our
method enables the model to select visual concepts in the
first frame and transition scenes coherently, achieving gen-
eralized multi-reference video customization. Importantly,
this is done without modifying the model architecture or
compromising the rich generative priors of the pre-trained
video generation model.
To summarize our contributions in this work:
• We investigate the innate and general ability of video gen-
eration models, showing that the first frame not only serves
as the spatiotemporal start of generation but also acts as
a conceptual buffer, which enables multi-reference-based
video content customization.
• We develop a simple yet effective pipeline that leverages
Vision-Language Models (VLMs) for high-quality training
data curation and achieves invocation through just 20–50
examples via few-shot LoRA adaptation.
• We evaluate and compare our proposed invocation add-
on, FFGo, with state-of-the-art video generation models
across diverse applications, including filmmaking, gener-
alized multi-object interaction, driving/aerial/underwater
simulation, and robotic manipulation, etc.
2. Related Work
Video Generation and Video Content Customization.
Video generation models
[1, 21, 26, 30, 34, 39] are a
powerful class of generative models that synthesize videos
conditioned on user-provided text prompts, primarily for
creative content generation. Most state-of-the-art models
are based on diffusion frameworks [25, 28] with U-Net-
based denoising backbones, and more recently, Diffusion
Transformers (DiT). A key limitation of these pre-trained
models is their reliance solely on text prompts, which re-
stricts controllability, particularly in real-world applications
such as product demonstrations or simulations, where vi-
sual references are often required for precise customization.
To address this, recent works in video content customiza-
tion [5, 7, 10, 12, 13, 17, 33] explore extending video gen-
eration models to accept additional visual inputs. These
approaches typically require: 1) architectural modifications,
and 2) large-scale training on specialized datasets of cus-
tomized videos. However, both requirements come with
significant drawbacks. Architectural changes often compro-
mise model efficiency and compatibility, while task-specific
fine-tuning on limited domains may degrade the general
2


First Frame
Veo 3
First Frame
Sora 2
First Frame
Wan 2.2
Text Prompt
<transition> + Text Prompt
Figure 2. In this figure, we illustrate a general yet under-explored observation: video generation models possess an innate ability to perform
subject mixing via scene transitions from a mixed-subject first frame. As shown, the red-boxed results (without the transition phrase:
<transition>) contrast with the blue-boxed results (with a carefully chosen <transition> e.g., “The camera view suddenly zoom in to
show") revealing significant differences in composition. However, this phenomenon faces three key limitations that hinder practical use: 1)
The prompt engineering process for <transition> is highly manual, time-consuming, and model/video-dependent. 2) Scene transitions are
often unstable. 3) Object identity is often lost, resulting in changes in appearance or the disappearance of reference objects.
knowledge acquired during large-scale pretraining, where
data is more diverse, higher in quality, and broader in scope.
The Innate Abilities of Pre-trained Generative Models
Pre-trained generative models are typically trained on mas-
sive and diverse datasets, which endows them with general-
purpose capabilities that often extend beyond their original
design goals. These emergent properties, what we refer to as
innate abilities, remain under-explored in current research,
despite growing evidence of their utility in real-world appli-
cations. For example, recent work [11] demonstrates that
with just a few fine-tuning samples and using Low-Rank
Adaptation (LoRA) [9] based fine-tuning, a pre-trained im-
age generation model can be prompted to generate grid-
aligned images with coherent content. In the video domain,
study [34] havs shown that pre-trained image-to-video (I2V)
models can perform various frame-level perception tasks
such as edge detection, segmentation, and super-resolution,
despite not being explicitly trained for them. Motivated
by these findings, we explore whether similar innate abili-
ties exist in pre-trained video generation models related to
video content customization, and how they might be invoked
through minimal adaptation, without architectural changes
or large-scale retraining.
Vision-language models (VLMs) for multimodal data
curation.
Vision-language models (VLMs) have become
essential across a wide range of multi-modal applica-
tions, demonstrating strong capabilities in video understand-
ing [8, 15, 20], video captioning [14, 37, 42], and visual
recognition tasks such as object detection and 3D scene un-
derstanding [3, 16, 23, 24, 29]. Recent progress in unified
VLMs, e.g., Gemini-2.5-Pro [6], Qwen2.5-Omni [36], GPT-
4o [24] have further extended these capabilities by enabling
seamless processing and generation across images, videos,
and text within a shared representation space. These models
operate over a unified multimodal input space, capable of
processing images, text instructions, and videos, and gener-
ating outputs across the same modalities, including images,
textual responses, and videos [18, 32, 38, 43]. In particu-
lar, unified VLMs demonstrate strong instruction-following
capabilities for image editing tasks [35] and video under-
standing tasks [8], enabling us to leverage them for curating
high-quality training data.
3. Proposed Approach
3.1. Pipeline Overview
Overview of our pipeline is shown in Figure 3. It consists of
three main components: 1) Dataset Curation, which utilizes
VLMs to generate high-quality paired training data inputs for
corresponding videos. 2) Few-shot LoRA Adaptation, which
invokes the model’s innate fusion and transition abilities,
and 3) Clean Customized Video Inference, which enables
generalized multi-reference video generation.
3


Fc
Fg
Cake
Christmas hat
Man
Unified VLM
SAM 2
Element 
Identification
<transition> + “The video features a person in 
a Santa hat and festive sweater decorating a 
layered cake. The cake, positioned on a 
turntable, is a focal point as the person 
meticulously works on its design.”
LoRA
I2V Model
Ctrans
Imix
Vmix: Video of F frames 
Training/Inference
Dataset Curation:
Figure 3. The overview of our proposed pipeline FFGo, consists of 1) Dataset Curation for getting the high quality finetuning data from
existing videos, 2) Few-shot LoRA Adaptation for training/inference to invoke the I2V model’s innate ability in fusing the subjects in the
first frame and perform a scene trasition to generate a video Vmix following subjects in the first mixing frame Imix and the text prompt.
3.2. Dataset Curation
Previous subject mixing models such as SkyReels-A2 [7],
VACE [13] modified model architectures to take in multiple
reference images and uses millions of training samples to
train models to mix subjects, where the subjects are mainly
with humans and does not generalize to other applications
such as autonomous driving, robot manipulation. Instead
of modifying the models’ architecture and using millions of
training data to learn subject mixing, we leverage the models’
pre-trained abilities to learn subject mixing without modify-
ing its architecture. To do so, we need to prepare a dataset
where the input aligns with the base model (Wan2.2 [30])’s
input, which is an image and a text prompt.
Training Data Selection.
We gather two thousand videos
from varies sources include Veo 3 showcases [34], HOIGen-
1M [19] and licensed short videos. Each of the videos is
ranging from six to twenty seconds long, we crop all the
videos to keep the first 81 frames. Next, we carefully go
over a subset of the videos, and pick the high quality ones
that have clear interaction or combinations of different ele-
ments, with clear and complete boundaries that can be easily
segmented. Then for each video we think it is high quality,
we write down the elements we want to segment out from
the video in text form. For example, in a video where a man
wearing a Chrismas hat is making a cake, we write down
the element names the man, cake, Chrismas hat, which will
be used to segment out later. We use the first frame of the
selected videos to extract elements and process to be the
input for the training data. We sample a total of 50 care-
fully selected videos with human annotations of objects to
be segmented out.
Element Extraction using unified VLMs.
Given an im-
age I, and a set of elements in textual representation O, our
goal is to use a unified VLM to (1) recognize and extract and
generate each of the individual O from I, (2) remove all O
from I and generate a complete background of I. Specifi-
cally, we prompt Gemini-2.5-Pro [6] to do this process. At
them end, we use SAM 2 [27] to remove the white back-
grounds of the individually extracted or generated elements
and keep an RGBA layer for each O, where only the element
has RGB layer. Given the set of elements O in RGBA and a
background, we combine them into a single image, with the
background always on the right and the elements always on
the left with automatic resize to fit the elements.
Prompt generation using VLMs.
Given a set of RGBA
elements, and a ground truth video, we prompt Gemini-2.5-
Pro to generate a prompt that not only describe the video,
but also focuses on the appearance and interaction of these
elements. Thus, we get a final training dataset with combined
images with their respective background and elements and a
4


caption that describes how these elements fit into the video
and background.
3.3. Few-shot LoRA Adaptation
As shown in Figure 3, with only 20–50 training examples,
we apply a simple LoRA-based few-shot adaptation to a
pre-trained model, enabling it to learn subject mixing by
fusing subjects in the first frame and performing a scene
transition. Given a pre-trained I2V model gθ, input image I,
and text prompt C, the standard generation process is: V =
gθ(I, C), where V is a video of F frames. In our adapted
pipeline: The input image is replaced by a subject-mixed
image Imix. The prompt is modified by appending a special
identifier to indicate a transition Ctrans =< transition >
+C. A LoRA-based weight update ∆θ is applied to the
model. LoRA learns an updated weight matrix θ defined
as θ = θ + ∆θ, where θ represents the original pre-trained
weights and ∆θ is a learnable low-rank update. Instead of
learning a full-rank matrix of size d × k, LoRA factorizes
the update as: θ = θ + αAB, where α is a scaling factor,
A ∈Rd×r , B ∈Rr×k, and r is a low-rank dimension
much smaller than both d and k. The target output video
Vmix exhibits both subject mixing and a temporal scene
transition. Specifically, the video has frame length F =
{Fc, Fg} , where: Fc represents the temporal compression
frames, namely the temporal compression ratio (e.g., Fc = 4
in the Wan2.2), Fg contains the generated subject-mixed
content, following a sudden transition after frames Fc. Thus,
the adapted video generation process becomes: Vmix =
gθ+∆θ(Imix, Ctrans).
3.4. Clean Customized Video Inference
During the inference, to generate a content customized video,
the processing is very simple, since the model will generate
the Vmix has frame length F = {Fc, Fg} with both subject
mixing and a temporal scene transition. The users can easily
cut off the first Fc frames to get the clean subject mixing
videos. As an example, in Wan2.2 as the base model, to
generate a F = 81 frame video, the first Fc = 4 frames will
be discarded and the rest Fg = 77 frames will be the clean
customized content videos.
4. Experimental Results
The results and comparisons are best viewed through video
examples. We include video results in our project webpage.
4.1. Datasets and Implementation Details
Few-shot Training Data.
We carefully selected 50 videos
featuring object-object, human-human, or human-object in-
teractions with clearly defined boundaries suitable for seg-
mentation. These were curated from a pool of 2,000 videos
sampled from HOIGen-1M [19], licensed video clips, and
Veo 3 samples [34]. Each curated video contains 81 frames.
The dataset spans four main categories: human-object in-
teraction (60%), human-human interaction (14%), element
insertion (20%), and robot manipulation (6%). Details of
the curated training set are provided in the Supplementary
Materials.
LoRA Adaptation Training Details
We use Wan2.2-I2V-
A14B [30] as our base model. For LoRA adaptation, we
train with a LoRA rank of 128 and introduce a unique tran-
sition phrase, <transition>: “ad23r2 the camera view sud-
denly changes.” This phrase serves as a prompt to trigger the
model’s innate ability for subject selection and scene transi-
tion from the first frame. Since Wan2.2-I2V-A14B employs
two separate denoising transformers for low- and high-noise
regimes, we train each independently for 5 hours using 2
NVIDIA H200 GPUs, with a batch size of 4.
4.2. Evaluation Strategy
In this section, we outline our evaluation strategy to demon-
strate the effectiveness of our proposed model and its perfor-
mance relative to baseline methods.
Test set spanning diverse application scenarios.
To rig-
orously evaluate the effectiveness and generalization of our
approach in video content customization, we curate a test set
of 50 video materials spanning diverse applications such as
robot manipulation, filmmaking, aerial/driving/underwater
simulation, and product demonstrations. Compared to prior
works [7, 13], our test set offers two key advantages: (1)
it covers a broader range of real-world customization sce-
narios beyond the typical human-human or human-object
interactions; and (2) it supports up to 5 reference subjects,
exceeding the 3-subject limit (e.g., object1, object2, scene)
seen in previous works.
Baseline Models
We compare our method against three
strong baselines built on the Wan architecture with 14 bil-
lion parameters: Wan2.2-14B-I2V [30], VACE [13], and
SkyReels-A2 [7]. Wan2.2-14B-I2V is our base I2V model,
to which we apply our lightweight adaptation for invoking its
innate subject mixing and scene transition capabilities. As
shown in Figures 5, 6 and 7, our adaptation significantly en-
hances its performance in video content customization while
preserving the original generation quality (Figure 4). VACE
and SkyReels-A2 represent state-of-the-art video customiza-
tion models trained on millions of high-quality examples.
Despite their scale, we demonstrate that our method, lever-
aging the overlooked role of the first frame, can outperform
both using only 50 training videos across a wide range of
scenarios.
4.3. Qualitative Comparison
In this section, we qualitatively compare our proposed
method with baseline models across diverse scenarios.
5


Model
Overall Quality ↑
Object Identity ↑
Scene Identity ↑
Avg. Rank ↓
% Ranked 1st ↑
Wan2.2-I2V-A14B
2.09
3.32
3.01
3.27
3.4%
SkyReels-A2
2.34
2.89
3.43
3.02
4.3%
VACE
3.00
3.50
3.66
2.50
11.1%
Ours (FFGo)
4.28
4.53
4.58
1.21
81.2%
Table 1. User Study Results: We report ratings and rankings from 200 annotations across 40 users. FFGo consistently outperforms all
baseline models across evaluation aspects, despite being trained on only 50 examples, demonstrating significantly better generalization
across diverse application scenarios.
Comparison with the Base Model.
We first compare our
adapted model with the base model (Wan2.2-I2V-A14B) to
demonstrate the effectiveness of our approach in video con-
tent customization, as shown in Figures 5, 6, and 7. For the
base model, we use the mixed image input along with a fixed
transition phrase (<transition>) that empirically performs
best. As observed, the base model often animates elements
independently, and the referred objects tend to disappear
post-transition. In contrast, our adapted model consistently
preserves object identities and performs coherent scene tran-
sitions, indicating a substantial improvement in handling
reference-based video customization.
Preserving Pre-trained Knowledge in the Base Model.
As demonstrated earlier, our add-on significantly enhances
the base model’s performance in reference-based video cus-
tomization. Crucially, it also preserves the base model’s orig-
inal pre-trained knowledge. Since our method is designed to
invoke the model’s innate capabilities rather than overwrite
them, it retains the core generative priors embedded through
pre-training. This preservation is illustrated in Figure 4. In
the rare successful cases where the base model maintains all
object identities and executes a coherent scene transition, our
results closely mirror the base output, in this instance, repli-
cating the motion and positioning of the wingsuit performer
and the car. This fidelity underscores an important advantage:
our method enables customization without compromising
the valuable general knowledge encoded during pre-training.
Given that post-training data for customization is often nar-
rower in scope and lower in quality than pre-training corpora,
it is critical for video content customization approaches to
integrate new reference inputs while preserving the strengths
of the base model. Our add-on achieves exactly this, offering
a lightweight yet effective pathway to transform general-
purpose I2V models into powerful, user-controllable video
customization systems.
Comparison with State-of-the-Art Video Customization
Baselines.
We compare our method with two Wan-based
state-of-the-art baselines: VACE and SkyReels-A2. 1) As
shown in Figures 5 and 7, both baselines are trained on
millions of videos for specific customization tasks involv-
ing the composition of three elements: human, object, and
scene. However, this design leads to overfitting, limiting
their generalization to novel application scenarios. In con-
Wan2.2-I2V-A14B
A high-angle, third-person aerial tracking shot 
follows a silver Tesla Cybertruck as it drives along 
a winding, S-curved asphalt road through a 
dense, green forest. Flying in perfect formation 
directly above the vehicle is a person wearing a 
blue and black wingsuit. The camera maintains its 
high vantage point, capturing both the Cybertruck 
navigating the curves of the road below and the 
wingsuit flyer mirroring its exact path in the air.
Ours
References
Text Prompt
Figure 4. As shown in the figure, in rare cases where the base
model Wan2.2-I2V-A14B successfully performs a scene transition
while preserving all reference object identities, the output closely
resembles ours. This demonstrates that our add-on approach effec-
tively retains the base model’s pre-trained generative capabilities.
trast, our method, trained on just 50 examples, activates the
pre-trained base model’s innate capabilities while preserving
its general knowledge, enabling superior performance across
diverse use cases. 2) Figure 6 highlights a key limitation of
these baselines: their architecture only supports up to three
reference inputs. As a result, they fail to handle scenarios
with five references. In contrast, our approach has no such
architectural constraint, as our multi-reference capability
stems from the the utilization of the first frame as conceptual
memory buffer, not architectural modification.
4.4. Quantitative Comparison
To quantitatively evaluate our method and compare it with
baselines, we conducted a user study on the full test set
across all methods. Details of the user study setup are pro-
vided in the Supplementary Materials. We built an online
interface where users are shown the prompt and a reference
image containing all foreground objects and background
context. The system then displays four generated videos in
randomized order. Users were asked to: 1) Rank the four
videos from best (1st place) to worst (4th place). 2) Rate
each video on three aspects: Object Identity: with a score
range from 1 (worst) to 5 (best), How well are foreground
object identities preserved from the reference image?. Scene
Identity:
with a score range from 1 (worst) to 5 (best),
How accurately is the background retained? and Overall
Quality:
with a score range from 1 (worst) to 5 (best),
6


Wan2.2-I2V-A14B
Professional-quality video with rich details. The 
video features a charming Teddy Bear sipping 
apple juice from a bottle using its hand, while 
delicately holds a vibrant red rose using its 
hand, admiring its beauty, perhaps as an 
offering or a gesture of affection.
SkyReels-A2
VACE
Ours
References
Text Prompt
Figure 5. Qualitative comparison with baseline methods. This test scenario involves generalized multi-object interactions. As shown in the
figure, our method best preserves the identities of input objects and the scene, while generating a customized video with coherent motion
that aligns with the text prompt description.
Wan2.2-I2V-A14B
A video scene unfolds in a vast, golden wheat 
field under a blue sky, with a red smoke flare 
billowing from a supply crate in the mid-ground. 
In the foreground, the character Wukong, in his 
ornate armor, stands next to a soldier wearing a 
helmet and tactical gear. The soldier holds a 
blue iPhone, gesturing with it, while Wukong 
holds a VR headset. The two characters are 
positioned side-by-side, appearing to be in an 
animated discussion, comparing and talking 
about the two different technology products 
they are holding.
SkyReels-A2
VACE
Ours
References
Text Prompt
Figure 6. Qualitative comparison with baseline methods. This scenario evaluates performance with an excessive number of references, five
in total (four objects and one scene). VACE and SkyReels-A2, due to their architecture-based limitations, support only up to three references
and fail to include all four reference objects in the generated video. In contrast, our model successfully fuses all four objects into a coherent,
customized video with natural interactions. Notably, our model also enables precise selection via text prompt (e.g., blue iPhone), preserving
key visual traits such as the triple-camera design while modifying appearance (e.g., changing the color to blue).
7


Wan2.2-I2V-A14B
A man wearing a short-sleeved shirt gently 
hands over a toy rocket to another man 
dressed in a black blazer over a dark graphic 
T-shirt. The man dressed in a black blazer 
looks at the rocket with mild curiosity and 
intrigue.
SkyReels-A2
VACE
Ours
References
Text Prompt
Figure 7. Qualitative comparison with baseline methods. This scenario evaluates generalized human-object interactions involving multiple
humans. While both VACE and SkyReels-A2 excel in customized single human-object video generation, they struggle in more complex
multi-human scenarios where interactions are mediated by shared objects. In such cases, both baselines fail to maintain object integrity and
coherent interaction. In contrast, our method reliably generates consistent videos with preserved object identity and realistic multi-human
interactions.
How realistic and coherent is the video overall?. Table 1
presents the results. Our method (FFGo) outperformed all
baselines across every metric, rank, object identity, scene
identity, and overall quality, despite using only a lightweight
adaptation. Notably, over 80% of users selected our results
as their top choice, indicating strong alignment with real user
preferences. Importantly, FFGo transforms the base model
Wan2.2-I2V-A14B from the lowest-performing baseline to
the top performer in user evaluations. This highlights the
strength of our add-on approach, which achieves state-of-the-
art customization performance without architectural changes
or large-scale training.
5. Limitations
While our adaptation effectively invokes the innate ability
of pre-trained I2V models for video content customization
via the first frame, several limitations remain. Although it is
theoretically possible to incorporate an arbitrary number of
reference subjects in the first frame, in practice, increasing
the number of subjects reduces the resolution available to
each, making identity preservation more difficult. Another
challenge is selective control: as the number of reference
subjects grows, it becomes harder to reliably target specific
objects using only text prompts. Empirically, we find our
method performs well up to four subjects plus a reference
scene (five references in total), beyond which identity preser-
vation and prompt-based selection degrade. We believe these
limitations are not fundamental and can be addressed through
engineering improvements. For example, using multiple start
frames as an extended conceptual memory buffer could al-
low for higher-capacity reference encoding. We leave such
enhancements to future work.
6. Conclusions
In this work, we propose a fundamentally different perspec-
tive on the role of the first frame in video generation models.
Contrary to the standard view that treats it merely as the spa-
tiotemporal starting point of an animation, we show that the
first frame functions as a conceptual memory buffer, capable
of storing and fusing disjoint reference subjects for down-
stream generation. Building on this insight, we introduce a
lightweight, add-on method to invoke this overlooked innate
ability for video content customization. Without modify-
ing the model architecture or requiring large-scale finetun-
ing, our few-shot adaptation turns a base video generation
model into a state-of-the-art video customization system.
We demonstrate strong performance across a wide range
of real-world scenarios and validate our approach through
a comprehensive user study, showing clear alignment with
user preferences.
8


References
[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel
Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,
Zion English, Vikram Voleti, Adam Letts, et al.
Stable
video diffusion: Scaling latent video diffusion models to
large datasets. arXiv preprint arXiv:2311.15127, 2023. 2
[2] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski,
Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingx-
iao Li, Mohsen Mousavi, et al. Go-with-the-flow: Motion-
controllable video diffusion models using real-time warped
noise. In Proceedings of the Computer Vision and Pattern
Recognition Conference, pages 13–23, 2025. 2
[3] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny
Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and
Fei Xia. Spatialvlm: Endowing vision-language models with
spatial reasoning capabilities, 2024. 3
[4] Jingxi Chen, Brandon Y Feng, Haoming Cai, Tianfu Wang,
Levi Burner, Dehao Yuan, Cornelia Fermuller, Christopher A
Metzler, and Yiannis Aloimonos. Repurposing pre-trained
video diffusion models for event-based video interpolation. In
Proceedings of the Computer Vision and Pattern Recognition
Conference, pages 12456–12466, 2025. 2
[5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace,
Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aber-
man, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov.
Multi-subject open-set personalization in video generation. In
Proceedings of the Computer Vision and Pattern Recognition
Conference, pages 6099–6110, 2025. 2
[6] Google DeepMind. Gemini 2.5: Our most intelligent ai model.
Google DeepMind, 2025. 3, 4
[7] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou,
Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li,
and Yahui Zhou. Skyreels-a2: Compose anything in video
diffusion transformers, 2025. 2, 4, 5
[8] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yib-
ing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue.
Video-r1: Reinforcing video reasoning in mllms.
arXiv
preprint arXiv:2503.21776, 2025. 3
[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora:
Low-rank adaptation of large language models. ICLR, 1(2):3,
2022. 3
[10] Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po
Chang, Fu-En Yang, and Yu-Chiang Frank Wang. Videomage:
Multi-subject and motion customization of text-to-video dif-
fusion models. In Proceedings of the Computer Vision and
Pattern Recognition Conference, pages 17603–17612, 2025.
2
[11] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi,
Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jin-
gren Zhou. In-context lora for diffusion transformers. arXiv
preprint arXiv:2410.23775, 2024. 3
[12] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si,
Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Video-
booth: Diffusion-based video generation with image prompts.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 6689–6700, 2024. 2
[13] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin
Pan, and Yu Liu. Vace: All-in-one video creation and editing,
2025. 2, 4, 5
[14] Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy
Nghiem, and Guangyao Shi. A survey of state of the art
large vision language models: Benchmark evaluations and
challenges. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops,
pages 1587–1606, 2025. 3
[15] Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin,
Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan Lee
Boyd-Graber. Videohallu: Evaluating and mitigating multi-
modal hallucinations on synthetic video understanding. In
The Thirty-ninth Annual Conference on Neural Information
Processing Systems, 2025. 3
[16] Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhen-
wen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-
Graber, Haitao Mi, and Dong Yu. Self-rewarding vision-
language model via reasoning decomposition, 2025. 3
[17] Tingting Liao, Chongjian Ge, Guangyi Liu, Hao Li, and Yi
Zhou. Character mixing for video generation. arXiv preprint
arXiv:2510.05093, 2025. 2
[18] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Ya-
coob, and Lijuan Wang. Mitigating hallucination in large
multi-modal models via robust instruction tuning.
arXiv
preprint arXiv:2306.14565, 2023. 3
[19] Kun Liu, Qi Liu, Xinchen Liu, Jie Li, Yongdong Zhang, Jiebo
Luo, Xiaodong He, and Wu Liu. Hoigen-1m: A large-scale
dataset for human-object interaction video generation, 2025.
4, 5, 11
[20] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your
multi-modal model an all-around player?, 2024. 3
[21] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao,
Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jian-
feng Gao, et al. Sora: A review on background, technology,
limitations, and opportunities of large vision models. arXiv
preprint arXiv:2402.17177, 2024. 2
[22] Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay
regularization in adam. arXiv preprint arXiv:1711.05101, 5
(5):5, 2017. 13
[23] Anish Madan, Neehar Peri, Shu Kong, and Deva Ramanan.
Revisiting few-shot object detection with vision-language
models. In Advances in Neural Information Processing Sys-
tems, pages 19547–19560. Curran Associates, Inc., 2024. 3
[24] OpenAI. Gpt-4o system card, 2024. 3
[25] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In Proceedings of the IEEE/CVF inter-
national conference on computer vision, pages 4195–4205,
2023. 2
[26] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,
Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-
Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of
media foundation models. arXiv preprint arXiv:2410.13720,
2024. 2
[27] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang
Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman
9


Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting
Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan
Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer.
Sam 2: Segment anything in images and videos, 2024. 4
[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684–10695, 2022. 2
[29] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree
Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin,
Karan Sapra, Yaser Yacoob, et al. Eagle: Exploring the
design space for multimodal llms with mixture of encoders.
arXiv preprint arXiv:2408.15998, 2024. 3
[30] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao,
Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao
Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren
Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu
Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pan-
deng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang,
Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu
Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng
Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong
Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei
Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang,
Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao
Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han,
Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-
scale video generative models, 2025. 1, 2, 4, 5
[31] Lizhen Wang, Zhurong Xia, Tianshu Hu, Pengrui Wang,
Pengfei Wei, Zerong Zheng, Ming Zhou, Yuan Zhang, and
Mingyuan Gao. Dreamactor-h1: High-fidelity human-product
demonstration video generation via motion-designed diffu-
sion transformers. arXiv preprint arXiv:2506.10568, 2025.
2
[32] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun,
Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen
Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li,
Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang
Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun
Huang, and Zhongyuan Wang. Emu3: Next-token prediction
is all you need, 2024. 3
[33] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhi-
heng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hong-
ming Shan. Dreamvideo: Composing your dream videos
with customized subject and motion.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6537–6549, 2024. 2
[34] Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane
Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini,
and Robert Geirhos. Video models are zero-shot learners and
reasoners, 2025. 2, 3, 4, 5, 11
[35] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan
Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei
Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi
Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, De-
qing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai
Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng
Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu,
Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan
Cai, and Zenan Liu. Qwen-image technical report, 2025. 3
[36] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He,
Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin
Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-
omni technical report, 2025. 3
[37] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-
toine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and
Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual
language model for dense video captioning, 2023. 3
[38] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,
Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong
Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei
Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun,
Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai,
Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao
Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie,
Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu
Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan
Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun
Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao,
Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2:
Open large-scale language models, 2025. 3
[39] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu
Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-
han Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video
diffusion models with an expert transformer. arXiv preprint
arXiv:2408.06072, 2024. 2
[40] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yu-
jun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-
preserving text-to-video generation by frequency decompo-
sition. In Proceedings of the Computer Vision and Pattern
Recognition Conference, pages 12978–12988, 2025.
[41] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y Feng,
Changxi Zheng, Noah Snavely, Jiajun Wu, and William T
Freeman. Physdreamer: Physics-based interaction with 3d
objects via video generation. In European Conference on
Computer Vision, pages 388–406. Springer, 2024. 2
[42] Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu,
Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing
Gong, Philipp Krähenbühl, and Liangzhe Yuan. Distilling
vision-language models on millions of videos, 2024. 3
[43] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michi-
hiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma,
Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the
next token and diffuse images with one multi-modal model,
2024. 3
10


First Frame Is the Place to Go for Video Content Customization
Supplementary Material
Contents
1. Introduction
2
2. Related Work
2
3. Proposed Approach
3
3.1. Pipeline Overview . . . . . . . . . . . . . .
3
3.2. Dataset Curation . . . . . . . . . . . . . . .
4
3.3. Few-shot LoRA Adaptation
. . . . . . . . .
5
3.4. Clean Customized Video Inference
. . . . .
5
4. Experimental Results
5
4.1. Datasets and Implementation Details
. . . .
5
4.2. Evaluation Strategy . . . . . . . . . . . . . .
5
4.3. Qualitative Comparison
. . . . . . . . . . .
5
4.4. Quantitative Comparison . . . . . . . . . . .
6
5. Limitations
8
6. Conclusions
8
A. Video Results
11
B. Details about Training and Testing Set
11
B.1. Training Dataset Curation Details . . . . . .
11
B.2. Test Set Curation . . . . . . . . . . . . . . .
11
C. Details about User Study
12
C.1. User Study Platform . . . . . . . . . . . . .
12
C.2. User Interface Details
. . . . . . . . . . . .
12
D. More Training and Inference Details
12
A. Video Results
Please
refer
to
our
project
page:
http : / /
firstframego . github . io
for
video
results,
which clearly demonstrate the effectiveness of our method
and its comparison with baseline models.
B. Details about Training and Testing Set
B.1. Training Dataset Curation Details
Our training corpus is sourced from three datasets: one
randomly selected folder from HOIGen-1M [19] (≈2,000
clips), all five Veo 3 demonstration videos [34], and 200
licensed short videos. This yields 2,205 candidate clips.
We manually curate the data to select videos with clearly
separable foregrounds, humans or manipulable objects, set
against uncluttered backgrounds.
Only clips depicting
cleanly segmentable single- or multi-object interactions are
retained.
This filtering results in 50 high-quality training exam-
ples (Figure 10), distributed across four scene types: hu-
man–object interaction (60%), human–human interaction
(14%), element insertion (20%), and robot manipulation
(6%).
Training Data Processing.
After curation, all clips are
standardized to 81 frames for consistent training. From each
video, we extract the first frame as a reference image and
manually tag all foreground entities of interest, e.g., cake,
party hat, male presenter, mouse. Using a prompt-to-prompt
workflow with Gemini-2.5-Pro, we then perform:
• Object Extraction: Apply Prompt 8 to generate high-
fidelity renditions of each tagged entity, preserving their
original appearance and scale. We refine results using
SAM 2 or Adobe Photoshop to isolate each object as an
RGBA layer.
• Background Cleanup: Use Prompt 9 to produce a clean
companion image with all tagged objects removed, yield-
ing a pristine background plate.
This paired set of object cut-outs and object-free back-
grounds forms the compositional basis of the training first
frame.
Caption Generation.
We use Gemini-2.5-Pro to generate
rich, element-aware captions for each training sample, based
on the individual object cut-outs, clean background plate,
and the full 81-frame video. These inputs are paired with a
structured prompt template (Fig. 11) to ensure consistency
and relevance.
Element Composition for First Frame.
For each training
clip, we synthesize a 1280×720 reference canvas: all fore-
ground cut-outs are vertically tiled on the left half, while
the clean background is centered on the right (see Fig. 10).
This composite serves as both the conditioning input and the
initial frame, guiding the video generation model to blend
the elements into a coherent sequence.
B.2. Test Set Curation
We manually curated a diverse test set of foreground objects
and backgrounds from our self-collected images. Each ob-
ject was segmented using SAM 2 or Adobe Photoshop and
saved as an RGBA cut-out. These cut-outs were then com-
posited with their respective backgrounds on a 1280×720
canvas, following the same layout used in training.


For each object-background pair, we drafted an initial
prompt and refined it using Gemini-2.5-Pro with the template
shown in Fig. 12. This process produced 50 high-quality
prompts paired with composite reference images, forming
our final test set.
Object Extraction Task Prompt Template
Prompt – Given the input image, extract the subset
{IDENTIFIED OBJECT} (i.e., only the specified
foreground objects)— return them alone with no re-
sizing, compression, or background so the output
resolution exactly matches the original image.
Figure 8. Prompt for extracting identified foreground objects using
a unified VLM.
Object Removal Task Prompt Template
Prompt – Given the input image, remove the sub-
set {IDENTIFIED OBJECTS} entirely. Return the
edited image only—it must preserve the source resolu-
tion (no scaling or compression) and contain neither
the specified objects nor any artifacts of their removal.
Figure 9. Prompt and specifications for the object removal task.
C. Details about User Study
To ensure a smooth user study and annotation experience,
we developed an HTML-based interface for participants
to annotate and submit data. In this section, we describe
the hiring platform, the job posting, and the design of the
annotation interface.
C.1. User Study Platform
We recruit participants through Prolific,1 a research platform
designed for user studies. Prolific offers an AI user study
beta program that targets participants with experience in
generative AI annotation.
To ensure quality, we apply screening filters to select
participants with prior video annotation experience and flu-
ent English proficiency, as understanding nuanced textual
prompts is crucial for this task.
We hire 40 participants, each tasked with annotating five
video sets, where each set contains generated outputs from
four different models. The annotation process takes approx-
imately 15 minutes per participant. Each is compensated
$5.50, reflecting the expected time and effort.
1https://www.prolific.com/
Figure 10. Our training dataset comprises four categories: hu-
man–object interaction (60%), human–human interaction (14%),
element insertion (20%), and robot manipulation (6%).
Our recruitment post and task instructions are shown in
Figure 13.
C.2. User Interface Details
Participants first arrive at a login screen, where they en-
ter their unique Prolific ID to match their responses with
task-completion records. After authentication, they are pre-
sented with the textual prompt used to generate the videos,
along with a composite reference image showing the required
foreground objects and background. Below, four candidate
videos are displayed in a randomized order to eliminate posi-
tion bias. Participants then rank the videos based on overall
quality, as shown in Figure 14a.
Next, participants scroll down to rate each video on three
criteria, Object Identity, Scene Identity, and Overall Quality,
using a 5-point Likert scale (Figure 14b).
D. More Training and Inference Details
We train LoRA modules of rank 128 for both high- and low-
noise regime transformers in the base model Wan2.2-I2V-
A14B. Training videos are resized to a resolution of 1344 ×
768 with 81 frames. We use a batch size of 4 and optimize
12


Training Data Prompt Generation Prompt Template
Task Description
You are given a video and several images. Generate a descriptive caption for the video that prominently features the components
shown in the images. Wrap your final text in <caption>. . . </caption> tags. The caption must highlight the significance and
role of these components throughout the video, while omitting filler such as “The scene unfolds with a whimsical and heartwarming
narrative, emphasizing the simple joys of life through the Teddy Bear’s endearing actions”.
Examples of Descriptive Captions
1. Film quality, professional quality, rich details. The video begins to show the surface of a pond, and the camera slowly zooms in
to a close-up. The water surface begins to bubble, and then a blonde woman is seen coming out of the lotus pond soaked all over,
showing the subtle changes in her facial expression.
2. A professional male diver performs an elegant diving maneuver from a high platform. Full-body side view captures him wearing
bright red swim trunks in an upside-down posture with arms fully extended and legs straight and pressed together. The camera
pans downward as he dives into the water below.
Figure 11. Prompt template used to generate captions for our training data.
Video-Prompt Enhancement Output
Task Description
You will be given a prompt and several images for video generation. You task is to make the prompt richer in description so the
model can understand better. Enclose your caption within <caption></caption> tags. The caption must emphasize the
significance and role of these components (and some description of each component) throughout the video. Your caption should
exclude unnecessary information such as “The scene unfolds with a whimsical and heartwarming narrative, emphasizing the simple
joys of life through the Teddy Bear’s endearing actions”.
Example of a Descriptive Caption
1. Film quality, professional quality, rich details. The video begins to show the surface of a pond, and the camera slowly zooms in
to a close-up. The water surface begins to bubble, and then a blonde woman is seen coming out of the lotus pond soaked all over,
showing the subtle changes in her facial expression.
2. A professional male diver performs an elegant diving maneuver from a high platform. Full-body side view captures him wearing
bright red swim trunks in an upside-down posture with arms fully extended and legs straight and pressed together. The camera
pans downward as he dives into the water below.
Prompt to Optimize
{Insert your test prompt to optimize here}
Figure 12. Prompt template for test prompt enhancement.
with AdamW [22], setting the learning rate to 1 × 10−4,
ϵ = 1 × 10−10, and a weight decay of 3 × 10−2.
During inference, videos are generated at a resolution of
1280 × 720 with 81 frames, following the standard output
format of Wan2.2-I2V-A14B based models.
13


Annotation Task Instructions
You will be presented with five sets of short, AI-generated videos (5 s, no audio).
Each set contains:
• Prompt – textual description of the intended video (scene, objects, motion).
• Reference Image – split into two halves:
– Left side: foreground objects that should appear in the video.
– Right side: background scene to be integrated with the objects.
• Generated Videos (4 total) – four model outputs attempting to fuse the objects with the background.
Your Task for Each Set
Step 1: Overall Ranking
• Watch all four videos carefully.
• Rank them from best to worst based on overall quality and faithfulness to the prompt.
• Assign unique ranks (1 = best, 4 = worst).
Step 2: Aspect Ratings
After ranking, rate each video on a 1–5 scale (1 = very poor, 5 = excellent):
• Object Identity – How well do objects retain their identity?
• Scene / Background Identity – How well is the background preserved?
• Video Quality – Overall realism and temporal coherence.
Notes
• Evaluate all four videos in every set before submitting answers.
• There are five sets in total (20 videos).
Figure 13. Recruitment post for our user study.
(a) Users rank the overall quality of the four candidate videos.
(b) Users rate three specific aspects with Likert scale 1-5.
Figure 14. Web-based annotation interface used in our user study. Part (a) collects a global quality ranking, while part (b) gathers detailed
aspect-wise ratings for each video.
14
