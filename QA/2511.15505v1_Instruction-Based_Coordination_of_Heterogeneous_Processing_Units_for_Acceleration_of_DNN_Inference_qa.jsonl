{"paper_id": "2511.15505v1_Instruction-Based_Coordination_of_Heterogeneous_Processing_Units_for_Acceleration_of_DNN_Inference", "citation_index": 18, "question": "Who is the first author of the cited paper that is revelant to \"Recent unified accelerators target diverse models, such as architectures that support both attention and convolutional Conv operations or versatile designs that handle Convolutional Neural Networks CNNs, Graph Neural Networks GNNs, and Vision Transformers ViTs\"?", "answer": "B. Zhang, R. Kannan, C. Busart,", "reference_entry": "B. Zhang, R. Kannan, C. Busart, and V. K. Prasanna, “VisionAGILE:"}
{"paper_id": "2511.15505v1_Instruction-Based_Coordination_of_Heterogeneous_Processing_Units_for_Acceleration_of_DNN_Inference", "citation_index": 31, "question": "Who is the first author of the cited paper that is revelant to \"DNN Model Processing and Profiling The proposed framework starts with quantized DNN models in ONNX format , utilizing bit quantization with poweroftwo scaling factors for compatibility with the PU architecture, as shown in Fig\"?", "answer": "ONNX Community", "reference_entry": "ONNX Community, “Open Neural Network Exchange (ONNX),” 2024."}
{"paper_id": "2511.15505v1_Instruction-Based_Coordination_of_Heterogeneous_Processing_Units_for_Acceleration_of_DNN_Inference", "citation_index": 11, "question": "Who is the first author of the cited paper that is revelant to \"Numerous automated design frameworks have emerged for DNNtoFPGA acceleration , typically following two architectural strategies unified Processing Unit PU architectures and heterogeneous PU architectures, as described in ,\"?", "answer": "J. Li, W. Wang,", "reference_entry": "J. Li, W. Wang, and W.-J. Li, “Hardware Computation Graph for DNN"}
